<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/home/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=home/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Intermediate Machine Learning | Kean Teng Blog</title>
<meta name="keywords" content="Python, Data Analysis, Data Science, Machine Learning, Kaggle">
<meta name="description" content="My Kaggle Learning Note 3">
<meta name="author" content="Kean Teng Blog">
<link rel="canonical" href="http://localhost:1313/home/docs/2023/2023-03-02-intermediate-machine-learning/">
<link crossorigin="anonymous" href="/home/assets/css/stylesheet.1f5296aba415da9f4c144e5f6d371e1ec7b408a5c439cae7bfd53f8ab906e529.css" integrity="sha256-H1KWq6QV2p9MFE5fbTceHse0CKXEOcrnv9U/irkG5Sk=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://i.postimg.cc/0Qq5g2fX/favicon.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/home/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/home/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/home/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/home/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/home/docs/2023/2023-03-02-intermediate-machine-learning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/home/docs/2023/2023-03-02-intermediate-machine-learning/">
  <meta property="og:site_name" content="Kean Teng Blog">
  <meta property="og:title" content="Intermediate Machine Learning">
  <meta property="og:description" content="My Kaggle Learning Note 3">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
    <meta property="article:published_time" content="2023-03-02T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-03-02T00:00:00+00:00">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="Data Analysis">
    <meta property="article:tag" content="Data Science">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Kaggle">
      <meta property="og:image" content="http://localhost:1313/home/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/home/papermod-cover.png">
<meta name="twitter:title" content="Intermediate Machine Learning">
<meta name="twitter:description" content="My Kaggle Learning Note 3">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Docs",
      "item": "http://localhost:1313/home/docs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Intermediate Machine Learning",
      "item": "http://localhost:1313/home/docs/2023/2023-03-02-intermediate-machine-learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Intermediate Machine Learning",
  "name": "Intermediate Machine Learning",
  "description": "My Kaggle Learning Note 3",
  "keywords": [
    "Python", "Data Analysis", "Data Science", "Machine Learning", "Kaggle"
  ],
  "articleBody": "\rImages from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nUsing machine learning in data analysis is a rather procedural approach. As we can notice from the approach, we will start by Preparing data » Defining a model » Model diagnostic checking » Model prediction to complete our workflow. This workflow is often and commonly practiced when we are doing data analysis work. In this course, we will learn some important and useful technique that can be used in our work to achieve a better model.\nMain learnings:\nApproaches towards missing data values and categorical variables (non-numeric) Construct pipeline to improve our workflow and code flow. Cross-validation technique Build state-of-the-art model such as XGBoost Approaches to avoid data leakage. 1. Missing Values \u0026 Categorial Variables 1.1 Missing values We can deal with missing values with the following three ways:\nDrop the columns containing missing values (not recommended, might loss access to important information) Imputation to fill the empty cells with some number. Imputation, then add a new column that shows the missing entries. Visualize the methods\nImputation will perform better than dropping the entire columns. This is how we can do that in code:\n## 1. drop columns with missing values # get the columns name cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()] # perform drop reduced_X_train = X_train.drop(cols_with_missing, axis = 1) ## 2. imputation from sklearn.impute import SimpleImputer my_imputer = SimpleImputer() imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train)) # imputation removed column names, put back imputed_X_train.columns = X_train.columns ## 3. extended imputation X_train_plus = X_train.copy() for col in cols_with_missing: X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull() my_imputer = SimpleImputer() imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus)) imputed_X_train_plus.columns = X_train_plus.columns 1.2 Categorical variables There are three ways to deal with categorical variables (non-numeric data):\nDropping the categorical variables (if the columns does not provide any useful information) Ordinal encoding — assign a unique value in the dataset to a different integer. One-hot encoding — create new columns to indicate the presence of each possible value in the original data One hot encoding example\nFor One-hot encoding, it means that if a column with 100 rows contains 100 unique values, it will create an extra (100 rows *100 unique values –100 original rows) new entries.\nWe can apply the 3 approaches with the following code:\n## 1. droppping categorical variables dorp_X_train = X_train.select_dtypes(exclude= ['object']) ## 2. ordinal encoding from sklearn.preprocessing import OrdinalEncoder label_X_train = X_train.copy() ordinal_encoder = OrdinalEncoder() label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols]) ## 3. one hot encoding from sklearn.preprocessing import OneHotEncoder # one hot encode categorical data OH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False) OH_cols_train = pd.DataFrame(OH_encoder.fit_transform[object_cols]) # add back removed index OH_cols_train.index = X_train.index # remove categorical column and add back the one hot encoded columns num_X_train = X_train.drop(object_cols, axis = 1) OH_X_train = pd.concat([num_X_train, OH_cols_train], axis = 1) 2. Pipelines Pipeline is a way to bundle our preprocessing and modelling steps to keep our code organized. The benefits of using pipeline are it gives a cleaner code, reduces bugs and make our model easier to be implemented.\nHere’s how we can apply pipeline to impute missing numerical entries and one-hot encode missing categorical entries:\nfrom sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder # preprocess numerical data numerical_transformer = SimpleImputer(strategy = 'constant') # preprocess categorical data categorical_transformer = Pipeline(steps = [ ('imputer', SimpleImputer(strategy = 'most frequent')), ('onehot', OneHotEncoder(handle_unknown = 'ignore')) ]) # bundle the two preprocesses preprocessor = ColumnTransformer( transformers = [ ('num', numerical_transformer, numerical_cols), ('cat', categorical_transformer, categorical_cols) ]) # bundle preprocessing and modelling my_pipeline = Pipeline(steps = [ ('preprocessor', preprocessor), ('model', model) ]) # model evaluation my_pipeline.fit(X)train, y_train) preds = my_pipeline.predict(X_valid) mean_absolute_error(y_valid, preds) 3. Cross-validation Cross-validation means we run our modelling process on different subsets of the data to get several measures of our model quality. Although this technique gives a more accurate measure of model quality, it can take some time to run as it need to estimate multiple models as we can see from the below images.\nIt is recommended to run cross-validation for smaller datasets while for larger datasets, a single validation if often suffice.\nCross-Validation\nHere’s how we can apply cross-validation in Python together with pipeline which we learned earlier on:\nfrom sklearn.model_selection import cross_val_score # split the data to 5 sets for validation scores = -1 * cross_val_score(my_pipeline, X, y, cv = 5, scoring = 'neg_mean_absolute_error') print(scores.mean()) It is surprising to see that negative mean absolute error is used in the code. This is because “sklearn” has a convention where all metrics are defined so a high number is better. Thus, the use of negatives allows convention consistency.\n4. XGBoost — Gradient Boosting In the random forest method, we improve a model prediction by averaging the prediction of many decision trees. Random forest method is one of an ensemble method where we combine the prediction of several models. A state-of-the-art method would be to apply gradient boosting where we perform iterative cycles to add models into an ensemble to result in better prediction.\nConcepts:\nUse the current ensemble to generate predictions for each observation in the dataset. Use the prediction to calculate a loss function. Use the loss function to fit a new model that will be added to the ensemble which will reduce the loss. Add this new model to the ensemble. Repeat Here’s how to do it in code:\nfrom xgboost import XGBRegressor from sklearn.metrics import mean_absolute_error my_model = XGBRegressor() my_model.fit(X_train, y_train) predictions = my_model.predict(X_valid) mean_absolute_error(predictions, y_valid) There are a few parameters in the xgregressor function that might affect the accuracy of our result:\nn_estimators which means how many times to go through the modelling cycle (concepts above), value too high or too low might result in overfitting or underfitting respectively early_stopping_rounds which means stopping the model when the validation score stops improving with imposed criteria learning_rate which means multiply the predictions from each model by a small number before adding up the prediction from each component model n_jobs which aims to improve model’s runtime. We can set the number equal to the cores of our machine model = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, n_jobs = 4) model.fit(X_train,y_train, early_stopping_rounds = 5, eval_set = [(X_valid, y_valid)], verbose = False) 5. Data Leakage Data leakage happens when training data contains information about the target, but similar data will not be available when we used the model to perform prediction. The two main types of data leakage are target leakage and train-test contamination.\n5.1 Target leakage Target leakage occurs when predictors include data that will not be available at the time when predictions is made. A way to overcome this issue is to think about timing or chronological order that data becomes available rather than whether a feature will help to make good predictions.\n5.2 Train-test contamination Train-test contamination happens when validation data affects the preprocessing behavior as the validation process is corrupted.\nFor example, imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions. In the courses, there are several interesting case studies on data leakage that worth looking to improve our acumen when interpreting results from work.\nCase 1\nGuide:\nThis is tricky, and it depends on details of how data is collected (which is common when thinking about leakage). Would you at the beginning of the month decide how much leather will be used that month? If so, this is ok. But if that is determined during the month, you would not have access to it when you make the prediction. If you have a guess at the beginning of the month, and it is subsequently changed during the month, the actual amount used during the month cannot be used as a feature (because it causes leakage).\n",
  "wordCount" : "1313",
  "inLanguage": "en",
  "image": "http://localhost:1313/home/papermod-cover.png","datePublished": "2023-03-02T00:00:00Z",
  "dateModified": "2023-03-02T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Kean Teng Blog"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/home/docs/2023/2023-03-02-intermediate-machine-learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Kean Teng Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://i.postimg.cc/0Qq5g2fX/favicon.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/home/" accesskey="h" title="Kean Teng Blog (Alt + H)">Kean Teng Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/home/ja/" title="日本語"
                            aria-label="日本語">日本語</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/home/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/home/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/home/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/home/faqs/" title="FAQs">
                    <span>FAQs</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/home/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/home/docs/">Docs</a></div>
    <h1 class="post-title entry-hint-parent">
      Intermediate Machine Learning
    </h1>
    <div class="post-description">
      My Kaggle Learning Note 3
    </div>
    <div class="post-meta"><span title='2023-03-02 00:00:00 +0000 UTC'>March 2, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Kean Teng Blog&nbsp;|&nbsp;<a href="https://github.com/keanteng/home/tree/main/content/docs/2023/2023-03-02-intermediate-machine-learning/index.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-missing-values--categorial-variables" aria-label="1. Missing Values &amp; Categorial Variables">1. Missing Values &amp; Categorial Variables</a><ul>
                        
                <li>
                    <a href="#11-missing-values" aria-label="1.1 Missing values">1.1 Missing values</a></li>
                <li>
                    <a href="#12-categorical-variables" aria-label="1.2 Categorical variables">1.2 Categorical variables</a></li></ul>
                </li>
                <li>
                    <a href="#2-pipelines" aria-label="2. Pipelines">2. Pipelines</a></li>
                <li>
                    <a href="#3-cross-validation" aria-label="3. Cross-validation">3. Cross-validation</a></li>
                <li>
                    <a href="#4-xgboost--gradient-boosting" aria-label="4. XGBoost — Gradient Boosting">4. XGBoost — Gradient Boosting</a></li>
                <li>
                    <a href="#5-data-leakage" aria-label="5. Data Leakage">5. Data Leakage</a><ul>
                        
                <li>
                    <a href="#51-target-leakage" aria-label="5.1 Target leakage">5.1 Target leakage</a></li>
                <li>
                    <a href="#52-train-test-contamination" aria-label="5.2 Train-test contamination">5.2 Train-test contamination</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><center><img src="https://images.unsplash.com/photo-1580927752452-89d86da3fa0a?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1170&q=80"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i>Images from Unsplash</i></p>
<blockquote>
<p><em>Disclaimer: This article is my learning note from the courses I took from Kaggle.</em></p></blockquote>
<p>Using machine learning in data analysis is a rather procedural approach. As we can notice from the approach, we will start by <em>Preparing data &raquo; Defining a model &raquo; Model diagnostic checking &raquo; Model prediction</em> to complete our workflow. This workflow is often and commonly practiced when we are doing data analysis work. In this course, we will learn some important and useful technique that can be used in our work to achieve a better model.</p>
<p><strong>Main learnings:</strong></p>
<ul>
<li>Approaches towards missing data values and categorical variables (non-numeric)</li>
<li>Construct pipeline to improve our workflow and code flow.</li>
<li>Cross-validation technique</li>
<li>Build state-of-the-art model such as <code>XGBoost</code></li>
<li>Approaches to avoid data leakage.</li>
</ul>
<h2 id="1-missing-values--categorial-variables">1. Missing Values &amp; Categorial Variables<a hidden class="anchor" aria-hidden="true" href="#1-missing-values--categorial-variables">#</a></h2>
<h3 id="11-missing-values">1.1 Missing values<a hidden class="anchor" aria-hidden="true" href="#11-missing-values">#</a></h3>
<p>We can deal with missing values with the following three ways:</p>
<ul>
<li>Drop the columns containing missing values (not recommended, might loss access to important information)</li>
<li>Imputation to fill the empty cells with some number.</li>
<li>Imputation, then add a new column that shows the missing entries.</li>
</ul>
<center><img src="images/img1.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i>Visualize the methods</i></p>
<p>Imputation will perform better than dropping the entire columns. This is how we can do that in code:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="c1">## 1. drop columns with missing values</span>
</span></span><span class="line"><span class="cl"><span class="c1"># get the columns name</span>
</span></span><span class="line"><span class="cl"><span class="n">cols_with_missing</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span>
</span></span><span class="line"><span class="cl">                      <span class="k">if</span> <span class="n">X_train</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># perform drop</span>
</span></span><span class="line"><span class="cl"><span class="n">reduced_X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">cols_with_missing</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## 2. imputation</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
</span></span><span class="line"><span class="cl"><span class="n">my_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">imputed_X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">my_imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># imputation removed column names, put back</span>
</span></span><span class="line"><span class="cl"><span class="n">imputed_X_train</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## 3. extended imputation</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train_plus</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cols_with_missing</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">X_train_plus</span><span class="p">[</span><span class="n">col</span> <span class="o">+</span> <span class="s1">&#39;_was_missing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train_plus</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">my_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">imputed_X_train_plus</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">my_imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_plus</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">imputed_X_train_plus</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X_train_plus</span><span class="o">.</span><span class="n">columns</span>
</span></span></code></pre></div><h3 id="12-categorical-variables">1.2 Categorical variables<a hidden class="anchor" aria-hidden="true" href="#12-categorical-variables">#</a></h3>
<p>There are three ways to deal with categorical variables (non-numeric data):</p>
<ul>
<li>Dropping the categorical variables (if the columns does not provide any useful information)</li>
<li>Ordinal encoding — assign a unique value in the dataset to a different integer.</li>
<li>One-hot encoding — create new columns to indicate the presence of each possible value in the original data</li>
</ul>
<center><img src="images/img2.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i>One hot encoding example</i></p>
<p>For One-hot encoding, it means that if a column with 100 rows contains 100 unique values, it will create an extra <code>(100 rows *100 unique values –100 original rows)</code> new entries.</p>
<p>We can apply the 3 approaches with the following code:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="c1">## 1. droppping categorical variables</span>
</span></span><span class="line"><span class="cl"><span class="n">dorp_X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span> <span class="p">[</span><span class="s1">&#39;object&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## 2. ordinal encoding</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
</span></span><span class="line"><span class="cl"><span class="n">label_X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">ordinal_encoder</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">label_X_train</span><span class="p">[</span><span class="n">object_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">ordinal_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">object_cols</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## 3. one hot encoding</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># one hot encode categorical data</span>
</span></span><span class="line"><span class="cl"><span class="n">OH_encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span> <span class="o">=</span> <span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">sparse</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">OH_cols_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">OH_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">[</span><span class="n">object_cols</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># add back removed index</span>
</span></span><span class="line"><span class="cl"><span class="n">OH_cols_train</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">index</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># remove categorical column and add back the one hot encoded columns</span>
</span></span><span class="line"><span class="cl"><span class="n">num_X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">object_cols</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">OH_X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">num_X_train</span><span class="p">,</span> <span class="n">OH_cols_train</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="2-pipelines">2. Pipelines<a hidden class="anchor" aria-hidden="true" href="#2-pipelines">#</a></h2>
<p>Pipeline is a way to bundle our preprocessing and modelling steps to keep our code organized. The benefits of using pipeline are it gives a cleaner code, reduces bugs and make our model easier to be implemented.</p>
<p>Here’s how we can apply pipeline to impute missing numerical entries and one-hot encode missing categorical entries:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># preprocess numerical data</span>
</span></span><span class="line"><span class="cl"><span class="n">numerical_transformer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span> <span class="o">=</span> <span class="s1">&#39;constant&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># preprocess categorical data</span>
</span></span><span class="line"><span class="cl"><span class="n">categorical_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">  <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span> <span class="o">=</span> <span class="s1">&#39;most frequent&#39;</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">  <span class="p">(</span><span class="s1">&#39;onehot&#39;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span> <span class="o">=</span> <span class="s1">&#39;ignore&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># bundle the two preprocesses</span>
</span></span><span class="line"><span class="cl"><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">transformers</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="n">numerical_transformer</span><span class="p">,</span> <span class="n">numerical_cols</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">categorical_transformer</span><span class="p">,</span> <span class="n">categorical_cols</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># bundle preprocessing and modelling</span>
</span></span><span class="line"><span class="cl"><span class="n">my_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">  <span class="p">(</span><span class="s1">&#39;preprocessor&#39;</span><span class="p">,</span> <span class="n">preprocessor</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">  <span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># model evaluation</span>
</span></span><span class="line"><span class="cl"><span class="n">my_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">preds</span> <span class="o">=</span> <span class="n">my_pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_valid</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="3-cross-validation">3. Cross-validation<a hidden class="anchor" aria-hidden="true" href="#3-cross-validation">#</a></h2>
<p>Cross-validation means we run our modelling process on different subsets of the data to get several measures of our model quality. Although this technique gives a more accurate measure of model quality, it can take some time to run as it need to estimate multiple models as we can see from the below images.</p>
<p>It is recommended to run cross-validation for smaller datasets while for larger datasets, a single validation if often suffice.</p>
<center><img src="images/img3.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i>Cross-Validation</i></p>
<p>Here’s how we can apply cross-validation in Python together with pipeline which we learned earlier on:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># split the data to 5 sets for validation</span>
</span></span><span class="line"><span class="cl"><span class="n">scores</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">my_pipeline</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;neg_mean_absolute_error&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</span></span></code></pre></div><p>It is surprising to see that negative mean absolute error is used in the code. This is because “sklearn” has a convention where all metrics are defined so a high number is better. Thus, the use of negatives allows convention consistency.</p>
<h2 id="4-xgboost--gradient-boosting">4. XGBoost — Gradient Boosting<a hidden class="anchor" aria-hidden="true" href="#4-xgboost--gradient-boosting">#</a></h2>
<p>In the random forest method, we improve a model prediction by averaging the prediction of many decision trees. Random forest method is one of an ensemble method where we combine the prediction of several models. A state-of-the-art method would be to apply gradient boosting where we perform iterative cycles to add models into an ensemble to result in better prediction.</p>
<p><strong>Concepts:</strong></p>
<ul>
<li>Use the current ensemble to generate predictions for each observation in the dataset.</li>
<li>Use the prediction to calculate a loss function.</li>
<li>Use the loss function to fit a new model that will be added to the ensemble which will reduce the loss.</li>
<li>Add this new model to the ensemble.</li>
<li>Repeat</li>
</ul>
<p>Here’s how to do it in code:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBRegressor</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">my_model</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">my_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">predictions</span> <span class="o">=</span> <span class="n">my_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
</span></span></code></pre></div><p>There are a few parameters in the <code>xgregressor</code> function that might affect the accuracy of our result:</p>
<ul>
<li><code>n_estimators</code> which means how many times to go through the modelling cycle (concepts above), value too high or too low might result in overfitting or underfitting respectively</li>
<li><code>early_stopping_rounds</code> which means stopping the model when the validation score stops improving with imposed criteria</li>
<li><code>learning_rate</code> which means multiply the predictions from each model by a small number before adding up the prediction from each component model</li>
<li><code>n_jobs</code> which aims to improve model’s runtime. We can set the number equal to the cores of our machine</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">early_stopping_rounds</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">eval_set</span> <span class="o">=</span> <span class="p">[(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">  <span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="5-data-leakage">5. Data Leakage<a hidden class="anchor" aria-hidden="true" href="#5-data-leakage">#</a></h2>
<p>Data leakage happens when training data contains information about the target, but similar data will not be available when we used the model to perform prediction. The two main types of data leakage are target leakage and train-test contamination.</p>
<h3 id="51-target-leakage">5.1 Target leakage<a hidden class="anchor" aria-hidden="true" href="#51-target-leakage">#</a></h3>
<p>Target leakage occurs when predictors include data that will not be available at the time when predictions is made. A way to overcome this issue is to think about timing or chronological order that data becomes available rather than whether a feature will help to make good predictions.</p>
<h3 id="52-train-test-contamination">5.2 Train-test contamination<a hidden class="anchor" aria-hidden="true" href="#52-train-test-contamination">#</a></h3>
<p>Train-test contamination happens when validation data affects the preprocessing behavior as the validation process is corrupted.</p>
<pre tabindex="0"><code>For example, imagine you run preprocessing (like fitting an imputer for 
missing values) before calling train_test_split(). The end result? 
Your model may get good validation scores, giving you great confidence 
in it, but perform poorly when you deploy it to make decisions.
</code></pre><p>In the courses, there are several interesting case studies on data leakage that worth looking to improve our acumen when interpreting results from work.</p>
<center><img src="images/img4.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i>Case 1</i></p>
<p><strong>Guide:</strong></p>
<p>This is tricky, and it depends on details of how data is collected (which is common when thinking about leakage). Would you at the beginning of the month decide how much leather will be used that month? If so, this is ok. But if that is determined during the month, you would not have access to it when you make the prediction. If you have a guess at the beginning of the month, and it is subsequently changed during the month, the actual amount used during the month cannot be used as a feature (because it causes leakage).</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/home/tags/python/">Python</a></li>
      <li><a href="http://localhost:1313/home/tags/data-analysis/">Data Analysis</a></li>
      <li><a href="http://localhost:1313/home/tags/data-science/">Data Science</a></li>
      <li><a href="http://localhost:1313/home/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="http://localhost:1313/home/tags/kaggle/">Kaggle</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/home/docs/2023/2023-03-15-intro-to-sql/">
    <span class="title">« Prev</span>
    <br>
    <span>Intro to SQL</span>
  </a>
  <a class="next" href="http://localhost:1313/home/docs/2023/2023-02-25-stable-diffusion-webui-with-civitai-loras/">
    <span class="title">Next »</span>
    <br>
    <span>Stable Diffusion WebUI with Civitai LORAs</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on x"
            href="https://x.com/intent/tweet/?text=Intermediate%20Machine%20Learning&amp;url=http%3a%2f%2flocalhost%3a1313%2fhome%2fdocs%2f2023%2f2023-03-02-intermediate-machine-learning%2f&amp;hashtags=Python%2cDataAnalysis%2cDataScience%2cMachineLearning%2cKaggle">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fhome%2fdocs%2f2023%2f2023-03-02-intermediate-machine-learning%2f&amp;title=Intermediate%20Machine%20Learning&amp;summary=Intermediate%20Machine%20Learning&amp;source=http%3a%2f%2flocalhost%3a1313%2fhome%2fdocs%2f2023%2f2023-03-02-intermediate-machine-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fhome%2fdocs%2f2023%2f2023-03-02-intermediate-machine-learning%2f&title=Intermediate%20Machine%20Learning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fhome%2fdocs%2f2023%2f2023-03-02-intermediate-machine-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on whatsapp"
            href="https://api.whatsapp.com/send?text=Intermediate%20Machine%20Learning%20-%20http%3a%2f%2flocalhost%3a1313%2fhome%2fdocs%2f2023%2f2023-03-02-intermediate-machine-learning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on telegram"
            href="https://telegram.me/share/url?text=Intermediate%20Machine%20Learning&amp;url=http%3a%2f%2flocalhost%3a1313%2fhome%2fdocs%2f2023%2f2023-03-02-intermediate-machine-learning%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Intermediate%20Machine%20Learning&u=http%3a%2f%2flocalhost%3a1313%2fhome%2fdocs%2f2023%2f2023-03-02-intermediate-machine-learning%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
        data-repo="keanteng/home"
        data-repo-id="R_kgDOJUcFvg"
        data-category="General"
        data-category-id="DIC_kwDOJUcFvs4CeIVI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/home/">Kean Teng Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
