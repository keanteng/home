[{"content":" Disclaimer: This article is my learning note from the courses I took from Kaggle.\n{{ partial \u0026ldquo;disqus.html\u0026rdquo; . }}\n","permalink":"https://keanteng.github.io/home/docs/2023-04-13-geospatial-analysis/","summary":"Disclaimer: This article is my learning note from the courses I took from Kaggle.\n{{ partial \u0026ldquo;disqus.html\u0026rdquo; . }}","title":"Geospatial Analysis"},{"content":" Images from Unsplash\nThis article aims to provide a comprehensive overview of how this site is set up and run.\nPre-requisite There are a few things to prepare before your own site can be created, as follows:\nA GitHub account Installed Microsoft Visual Studio Code Installed Git After you create a GitHub account and installing the necessary software, we are ready to begin!\nWebsite Foundations Setup The site that we are going to create will be based on Hugo, an open-source site generators where it provides a framework for us to deploy a site with speed and ease.\nTo build our website, we will need to install Hugo into our local machine. The installation process will be done on Windows Terminal with the support of Go language. Of course, you can also use other language such as chocolatey, scoop and winget.\nAfter installing go, here\u0026rsquo;s how to install Hugo on terminal:\ngo install -tags extended github.com/gohugoio/hugo@latest hugo version # check if you are using the latest version With Hugo installed in our local system, we would like to now create the foundation or framework for our website. Here I encourage you to change your directory first, preferably to \\Desktop so that you can access all your files easily:\ncd C:\\Users\\Username\\Desktop hugo new site \u0026lt;your_site_name\u0026gt; -f yml Now change your directory again and create a page on your website:\ncd \u0026lt;your_site_name\u0026gt; hugo new docs/page.md Website Theme Setup With the site foundation ready, we now add a theme or a specific design to our site for better functionality and appearance. You can check out different theme here, but in my case we will proceed with the PaperMode theme.\nHere we will use git to install the website theme:\ngit init git clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod After that, navigate to GitHub to create an empty repository and head back to terminal where we need to link up our files with the repository created. Since we are using an empty repository, we need to create a first file, usually README.md to avoid causing any error:\necho \u0026#34;# Test\u0026#34; \u0026gt;\u0026gt; README.md git add README.md git commit -m \u0026#34;Initial commit\u0026#34; git branch -M main git remote add origin https://github.com/YOUR-GIT-NAME/REPOSITORY-NAME.git git push -u origin main If you want to see how your website look like, you can deploy your site locally using Hugo. This is a good practice to check for error and website update before deploy your site publicly. Simply click the link or type 127.0.0.1 on your web browser:\nhugo server Website Deployment Workflow Just now your were shown to deploy the site locally, now to do it publicly - meaning on the web, you need the support from GitHub workflow.\nHere we will need to create an additional directory and put in some codes into it:\nmkdir -p .github/workflows After creating this directory, create a file with name deploy.yml in the workflow folder. Then navigate these two files (your can use file explorer) and put in these codes:\nconfig.yml After copying, change the first line baseurl: to the following format \u0026quot;https://YOUR-NAME-ON-GITHUB.github.io/REPOSITORY-NAME/\u0026quot; copy the code from here (https://github.com/adityatelange/hugo-PaperMod/blob/exampleSite/config.yml) deploy.yml name: Publish to GH Pages on: push: branches: - main pull_request: jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout source uses: actions/checkout@v3 with: submodules: true - name: Checkout destination uses: actions/checkout@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: ref: gh-pages path: built-site - name: Setup Hugo run: | curl -L -o /tmp/hugo.tar.gz \u0026#39;https://github.com/gohugoio/hugo/releases/download/v0.110.0/hugo_extended_0.110.0_linux-amd64.tar.gz\u0026#39; tar -C ${RUNNER_TEMP} -zxvf /tmp/hugo.tar.gz hugo - name: Build run: ${RUNNER_TEMP}/hugo - name: Deploy if: github.ref == \u0026#39;refs/heads/main\u0026#39; run: | cp -R public/* ${GITHUB_WORKSPACE}/built-site/ cd ${GITHUB_WORKSPACE}/built-site git add . git config user.name \u0026#39;keanteng\u0026#39; # change to your username git config user.email \u0026#39;u2004763@siswa.um.edu.my\u0026#39; # change to your email git commit -m \u0026#39;Updated site\u0026#39; git push Before we link up our local files with the repository, you need to create a new branch on your repository called git-pages and you need to change the setting for GitHub actions.\nChange the setting here\nFinal Step For the last part of the website setup, we will link up our local files with the repository created so that we can view our site online:\ngit status git add . git commit -m \u0026#34;site update\u0026#34; git push Now, just head to GitHub actions and click on pages build and deployment, and click on your website link on the web!\nClick on the link to view your site\nAcknowledgement ✨ I would like to thank Hugo and PaperMode for empowering me with the tool and foundations to build such a beautiful and impressive site. I want to thank dhij for the wonderful tutorial on YouTube for the site set-up, I failed on numerous attempt to set up the site by looking at some blogs on Medium until I look up his video. I am inspired to make this blog as comprehensive as possible so anyone that reads it can follow successfully. Thanks to Abdur Rahman for his own site set-up as well where I can reference to his code on GitHub such as markdown pages set-up. Finally, thanks to Lil\u0026rsquo;Log where I am inspired to kick-start my own site to share my learnings and some codes on her GitHub blog repository like embedding images and equations on pages, otherwise I might still struggle to add caption to all the images attached.\n","permalink":"https://keanteng.github.io/home/docs/2023_04_09-creating-a-website-with-hugo--papermode/","summary":"There are a few things to prepare before your own site can be created, as follows: A GitHub account Installed Microsoft Visual Studio Code Installed Git. After you create a GitHub account and installing the necessary software, we are ready to begin!","title":"Creating a Website With Hugo \u0026 PaperMode"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nComputer vision literally means computer able to see and recognize stuff. Applications such as Google Lens and Google Image Search are some good examples of where computer vision is being used in our daily life.\nIn this course, we will explore some technique used to empower computer with the power of seeing:\nBuilding an image classifier with Keras Concepts of visual feature extraction Custom covnet Apply data augmentation to extend dataset. 1. Convolutional Classifier Convolutional neural networks or “covnet” is a neural network specializes in computer vision. A covnet used for image classification has two parts: a convolutional base and a dense head. The base is used to extract the features of an image while the head is used to determine the class of the image. So, the aims of training a neural network are simply to know which feature to extract from a particular image and to know which class the image will belong from the features.\nThe process\nHere is how we can train a covnet on Python to recognize a car and a truck:\n# 1. load pretrained base pretrained_base = tf.keras.models.load_model( # file path \u0026#39;../input/cv-course-models/cv-course-models/vgg16-pretrained-base\u0026#39;, ) pretrained_base.trainable = False # 2. attach classfier head from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ pretrained_base, layers.Flatten(), # transform 2d output to 1d layers.Dense(6, activation = \u0026#39;relu\u0026#39;), layers.Dense(1, activation = \u0026#39;sigmoid\u0026#39;), # transform output to class probability (truck) ]) # 3. model fitting model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;binary_crossentropy\u0026#39;, metrics = [\u0026#39;binary_accurary\u0026#39;], ) history = model.fit( ds_train, validation_data = ds_valid, epochs = 30, verbose = 0, ) # 4. visualize model loss import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_frame.loc[:, [\u0026#39;binary_accurary\u0026#39;, \u0026#39;val_binary_accurary\u0026#39;]].plot(); 2. Features Extraction 2.1 Convolution and ReLU The process of feature extraction does three things. It filters an image for a certain feature; it detects the feature within the filtered image, and it condenses the image to enhance the features.\nFeatur extraction overview\nIn training, the covnet will learn weights from image features and the weights are contained in the convolutional layers. The weights are known as kernels which can be presented as an array of number. A kernel will scan over an image and produce a weighted sum of pixel values (finding the best kernel values) — emphasizing and de-emphasizing certain image patterns and information.\nThe process\nActivations in the network is called feature maps. Feature maps is the result when filter is applied to image — it contains features a kernel extract.\nApplying filter\nAfter filtering, the feature maps will be passed to an activation function which can be though as scoring pixel values according to some measure of importance. For example, ReLU activation assumes negative values are not important, so they are set to zero. In fact, these images are how the head of a network is able to solve the classification problem — looking for a particular characteristic of images that we want to classify.\nAfter passing through activation function\nHere’s how to can perform feature extraction in Python:\n# define kernel import tensorflow as tf kernel =tf.constant([ [-1, -1, -1], [-1, 8, -1], [-1, -1, -1], ]) plt.figure(figsize = (3,3)) show_kernel(kernel) # applying kernel image_filter = tf.nn.conv2d( input = image, fitlers = kernel, strides = 1, # section 3 padding = \u0026#39;SAME\u0026#39;, ) plt.figure(figsize = (6,6)) plt.imshow(tf.squeeze(image_filter)) plt.axis(\u0026#39;off\u0026#39;) plt.show(); # applying activation function image_detect = tf.nn.relu(image_filter) plt.figure(figsize = (6,6)) plt.imshow(tf.squeeze(image_filter)) plt.axis(\u0026#39;off\u0026#39;) plt.show(); 2.2 Maximum Pooling from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Conv2D(filters=64, kernel_size=3), # activation is None layers.MaxPool2D(pool_size=2), # More layers follow ]) Notice that after the Conv2D layer, we will apply a MaxPool2D layer for the condensation step. This layer will not contain any trainable weights as of the previous layer, but it will condense the feature maps to only retain important feature. This is what maximum pooling does. It takes patches of activations in the original feature maps and replaces them with maximum activation in those patches. The pooling steps will increase the proportions of active pixels to zero pixels — intensifying the feature after ReLU activation.\nMaximum pooling\nIs zero pixels unimportant? In fact, zero pixels carries positional information and MaxPool2D function will remove them (positional information of the feature maps) and this will lead to a property in covnet known as translation invariance.\nThis means that a covnet with maximum pooling tend to not distinguish features by their location in image. Notice from the first row of images below, after repeated pooling, the positional information is destroyed and no longer distinguishable. But pooling only causes translation invariance in network over small distance. The second row of the image features two dots far apart and this feature remains distinct after repeated pooling.\nIn fact, such invariance is good for an image classifier as it reduces data for training since we do not need to teach the network differences in perspective and framing when same features are positioned on different part of an original image.\n3. Sliding Window Both convolution and pooling steps are performed over a sliding window with parameters “kernel_size” for convolution and “pool_size” for pooling.\nNotice in section 2.1, when we perform pooling, there are two extra parameters: strides and padding. strides means how far the window will move each step and padding describes how the pixels at the edge are being handle.\nPooling layer will almost always have stride values greater than 1, like (2, 2) or (3, 3), but not larger than the window itself.\nConsidering the sliding window process, is it necessary to always stay within the boundary? In fact, there is a trade-off between staying within and out of bound by changing the parameter padding in our code:\npadding = ‘valid’: Convolution window stay entirely inside input. Output will shrink. It will shrink more for larger kernels. This will limit the number of layers contained in a network, notably small size input. padding = ‘same’: Pad the input with 0’s around the border to make size of output and input the same. However, this will dilute the influence of pixels at the borders. from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\u0026#39;same\u0026#39;, activation=\u0026#39;relu\u0026#39;), layers.MaxPool2D(pool_size=2, strides=1, padding=\u0026#39;same\u0026#39;) # More layers follow ]) Example for visualization:\nshow_extraction( image, kernel, # Window parameters conv_stride=3, pool_size=2, pool_stride=2, subplot_shape=(1, 4), figsize=(14, 6), ) Since the circle is just 1 pixel wide, using stride = 3 is too coarse to produce a decent feature maps. We should then reduce the number of strides for a better feature map.\nThe more horizontal parts of the input end up with the greatest activation as the kernel is designed to detect horizontal lines.\n4. Custom Covnet Through feature extraction, we learned how to extract simple features from an image through filter, detect and pooling. By repeating the extraction process, we can extract more complex and refined features as the process travel deeper into the network.\nThis can be done through convolution blocks with stacks of Conv2D and MaxPool2D layers as below:\nHere’s how we can design a covnet that can extract complex features:\n# 1. define model from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ # First Convolutional Block layers.Conv2D(filters=32, kernel_size=5, activation=\u0026#34;relu\u0026#34;, padding=\u0026#39;same\u0026#39;, input_shape=[128, 128, 3]), layers.MaxPool2D(), # Second Convolutional Block layers.Conv2D(filters=64, kernel_size=3, activation=\u0026#34;relu\u0026#34;, padding=\u0026#39;same\u0026#39;), layers.MaxPool2D(), # Third Convolutional Block layers.Conv2D(filters=128, kernel_size=3, activation=\u0026#34;relu\u0026#34;, padding=\u0026#39;same\u0026#39;), layers.MaxPool2D(), # Classifier Head layers.Flatten(), layers.Dense(units=6, activation=\u0026#34;relu\u0026#34;), layers.Dense(units=1, activation=\u0026#34;sigmoid\u0026#34;), ]) model.summary() # 2. model training model.compile( optimizer=tf.keras.optimizers.Adam(epsilon=0.01), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;binary_accuracy\u0026#39;] ) history = model.fit( ds_train, validation_data=ds_valid, epochs=40, verbose=0, ) # 3. model loss evaluation import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_frame.loc[:, [\u0026#39;binary_accuracy\u0026#39;, \u0026#39;val_binary_accuracy\u0026#39;]].plot(); 5. Data Augmentation More data will generally help a model performs better — to better differentiate image. In this section, we will learn to augment our data by applying transformation to our datasets such as rotation, flipping, warping and changing of contrast and color tone. Here’s how to perform data augmentation in Python:\nData augmentation example\n# 1. define model - with augmentation from tensorflow import keras from tensorflow.keras import layers pretrained_base = tf.keras.models.load_model( \u0026#39;../input/cv-course-models/cv-course-models/vgg16-pretrained-base\u0026#39;, ) pretrained_base.trainable = False model = keras.Sequential([ # Preprocessing layers.RandomFlip(\u0026#39;horizontal\u0026#39;), # flip left-to-right layers.RandomContrast(0.5), # contrast change by up to 50% # Base pretrained_base, # Head layers.Flatten(), layers.Dense(6, activation=\u0026#39;relu\u0026#39;), layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;), ]) # 2. model training model.compile( optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;binary_accuracy\u0026#39;], ) history = model.fit( ds_train, validation_data=ds_valid, epochs=30, verbose=0, ) # 3. model loss evaluation import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_frame.loc[:, [\u0026#39;binary_accuracy\u0026#39;, \u0026#39;val_binary_accuracy\u0026#39;]].plot(); ","permalink":"https://keanteng.github.io/home/docs/2023-03-20-computer-vision/","summary":"Computer vision literally means computer able to see and recognize stuff. Applications such as Google Lens and Google Image Search are some good examples of where computer vision is being used in our daily life. In this course, we will explore some technique used to empower computer with the power of seeing:","title":"Computer Vision"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nFor examining and assessing huge datasets and databases, SQL or structured programming language skills plays a vital role to enable us to design and manage data.\nSome common keywords used in SQL as follows:\nSELECT, WHERE, FROM GROUP BY, HAVING, COUNT ORDER BY AS, WITH JOIN 1. Data Preparation Since we will be using Python to apply SQL, there are some steps required for data preparation. We will first start by establishing a connection to the BigQuery service to create a client object that hold projects. We will then select a project from the biquery-public-data which holds a collection of datasets for each project. The datasets will contain the tables that we want for our analysis. Here’s how we can do that in Python:\nfrom google.cloud import bigquery # 1. create a client object client = bigquery.Client() dataset_ref = client.dataset(\u0026#34;hacker-news\u0026#34;, project = \u0026#34;bigquery-public-data\u0026#34;) # 2. get the hacker news dataset from a project dataset = client.get_dataset(dataset_ref) # 3. list the tables in the datasets tables = list(client.list_tables(dataset)) # 4. print the all the table names for table in table: print(table.table_id) Now from the code we learn about all the table names in the datasets from the printed outputs. Here’s how we can fetch the table with code:\n# create a reference to the table name full table_ref = dataset_ref.table(\u0026#34;full\u0026#34;) # get the table table = client.get_table(table_ref) Oftentimes, when we want to preview at the table before our analysis, but it contains a huge number of rows and might consume resources to load, we can choose to display only a certain number of rows of the table.\n# Restrict rows and convert the result to dataframe client.list_rows(table, max_results = 5).to_dataframe() 1.1 Table Schema The structure of a table is known as table schema, and it will tell us information about each specific columns in our table. The information is:\nName of the column Datatype of the column Mode of the column (by default it will allow null values — nullable) Description of the data in the particular column # code command table.schema # Output: [SchemaField(\u0026#39;title\u0026#39;, \u0026#39;STRING\u0026#39;, \u0026#39;NULLABLE\u0026#39;, \u0026#39;Story title\u0026#39;, (), None)] 2. Queries Foundations We will start by three keywords which represents the foundational components of a SQL query, namely SELECT, FROM and WHERE.\nBy SELECT, it refers to the columns that we want to select, FROM refers to the table that we are after and finally WHERE refers to the return condition that we want to impose on the output. Below is how we can select the cat owner’s name from an imaginary dataset:\nAn imaginary dataset\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT Name FROM `bigquery-public-data.pet_records.pets` WHERE Animal = \u0026#39;Dog\u0026#39; \u0026#34;\u0026#34;\u0026#34; 2.1 Big Datasets Some BigQuery datasets can be huge and costly to compute. If you are running them on cloud services with limited computing hours, your limit might drain easily with huge datasets. We can avoid this issue by imposing a function to limit the amount of data that we are capable to scan to avoid running over our computing limit.\nHere’s how we can check how much data a query will scan, alternatively we can also impose a limit with a parameter on how much data that will be scanned:\n# query query = \u0026#34;\u0026#34;\u0026#34;\u0026#34; SELECT score, title FROM `bigquery-public-data.hacker_news.full` WHERE type = \u0026#34;job\u0026#34; \u0026#34;\u0026#34;\u0026#34; ## 1. Cost estimation # create object for cost estimation without running it dry_run_config = bigquery.QueryJobConfig(dry_run = True) # estimate cost dry_run_query_job = client.query(query, job_config = dry_run_config) print(\u0026#34;This query will process {} bytes.\u0026#34;.format(dry_run_query_job.total_bytes_processed)) ## 2. Setting parameter ONE_GB = 1000*1000*1000 # set up the query safe_config = bigquery.QueryJobConfig(maximum_bytes_billed = ONE_GB) # run the query safe_query_job = client.query(query, job_config = safe_config) job_posts_scores = safe_query_job.to_dataframe() job_posts_scores.mean() 3. More Queries 1 This section will add three new techniques: GROUP BY, HAVING and COUNT() to assist us in getting more interesting insights from our queries.\nGROUP BY means grouping together rows with the same value in that column as a single group, COUNT() will return a count of things such as the number of entries of a column while HAVING is used in combination with GROUP BY to ignore groups that do not meet a certain criterion.\nHere is how we can apply these techniques to find the most discussed new comments on the Hacker News project:\n# select comment with \u0026gt; 10 replies query_popular = \u0026#34;\u0026#34;\u0026#34; SELECT parent, COUNT(1) AS NumPosts FROM `bigquery-public-data.hacker_news.comments` GROUP BY parent HAVING COUNT(1) \u0026gt; 10 \u0026#34;\u0026#34;\u0026#34; # limit how much data will be scan safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10) query_job = client.query(query_popular, job_config=safe_config) # run query and convert to data frame popular_comments = query_job.to_dataframe() # print the first five rows popular_comments.head() 4. More Queries 2 In this section, we will learn about ORDER BY clause where it is used to sort the returned results by the rest of the query. When we use ORDER BY the returned results will be sorted in ascending order whether it is string or numeric. If we would like to have the order reverse, we can add in the DESC argument.\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT Name FROM `bigquery-public-data.pet_records.pets` WHERE Animal = \u0026#39;Dog\u0026#39; ORDER BY Name DESC \u0026#34;\u0026#34;\u0026#34; Now considering we have a column that tell us about the date that a pet owner getting a pet, we can use the EXTRACT clause to get the day, month, week and year information from the column.\n5. More Queries 3 In this section, we will work on two keywords that helps to organize our query for better readability — if we happen we have a complex query. Starting with AS, it is used to rename columns generated by our queries. WITH is used together with AS to create a temporary table so that we can write query against them. This helps us to split queries into readable chunks and makes the data cleaning work easier.\nConsider that we want to know the pet IDs that are owned for more than five years:\nquery = \u0026#34;\u0026#34;\u0026#34; WITH Seniors AS ( SELECT Name, ID FROM `bigquery-public-data.pet_records.pets` WHERE Years_old \u0026gt; 5 ) SELECT ID FROM Seniors \u0026#34;\u0026#34;\u0026#34; Another example is to find how many bitcoins transactions being made in a month:\nquery = \u0026#34;\u0026#34;\u0026#34;\u0026#34; WITH Time AS ( SELECT DATE(block_timestamp) AS trans_date FROM `bigquery-public-data.crypto_bitcoin.transactions` ) SELECT COUNT(1) AS transactions, trans_date FROM Time GROUP BY trans_date ORDER BY trans_date \u0026#34;\u0026#34;\u0026#34; # limit how much data will be scanned safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10) query_job = client.query(query, job_config=safe_config) # run query and convert to data frame transactions_by_date = query_job.to_dataframe() # Print the first five rows transactions_by_date.head() # Extra: visualize the result transactions_by_date.set_index(\u0026#39;trans_date\u0026#39;).plot() 6. Combining Data Sources For the last clause in this course, we will learn about JOIN — INNER JOIN in this section. For INNERJOIN, a row will only be selected if the column that we used to combine in one table also matches with the table that we used for joining. JOIN clause allows us to query and combine the information from different tables. For example, we have two tables; one contains the pet name and the other one contains the pet’s owner name. Here how we can associate the pet with its owner:\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT p.Name AS Pet_Name, 0.Name AS Owner_Name FROM `bigquery-public-data.pet_records.pets` AS P INNER JOIN bigquery-public-data.pet_records.owners` AS O ON p.ID = o.Pet_ID \u0026#34;\u0026#34;\u0026#34; Here’s another example where we want to find how many files are covered by each type of software license on GitHub.\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT L.license, COUNT(1) AS number_of_files FROM `bigquery-public-data.github_repos.sample_files` AS sf INNER JOIN `bigquery-public-data.github_repos.licenses` AS L ON sf.repo_name = L.repo_name GROUP BY L.license ORDER BY number_of_files DESC \u0026#34;\u0026#34;\u0026#34; # limit how much data will be scanned safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10) query_job = client.query(query, job_config=safe_config) # run query and convert to data frame file_count_by_license = query_job.to_dataframe() file_count_by_license ","permalink":"https://keanteng.github.io/home/docs/2023-03-15-intro-to-sql/","summary":"For examining and assessing huge datasets and databases, SQL or structured programming language skills plays a vital role to enable us to design and manage data. Some common keywords used in SQL as follows: SELECT, WHERE FROM GROUP BY, HAVING, COUNT ORDER BY AS, WITH JOIN. Date Preparation Since we will be","title":"Intro to SQL"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nUsing machine learning in data analysis is a rather procedural approach. As we can notice from the approach, we will start by Preparing data \u0026raquo; Defining a model \u0026raquo; Model diagnostic checking \u0026raquo; Model prediction to complete our workflow. This workflow is often and commonly practiced when we are doing data analysis work. In this course, we will learn some important and useful technique that can be used in our work to achieve a better model.\nMain learnings:\nApproaches towards missing data values and categorical variables (non-numeric) Construct pipeline to improve our workflow and code flow. Cross-validation technique Build state-of-the-art model such as XGBoost Approaches to avoid data leakage. 1. Missing Values \u0026amp; Categorial Variables 1.1 Missing values We can deal with missing values with the following three ways:\nDrop the columns containing missing values (not recommended, might loss access to important information) Imputation to fill the empty cells with some number. Imputation, then add a new column that shows the missing entries. Visualize the methods\nImputation will perform better than dropping the entire columns. This is how we can do that in code:\n## 1. drop columns with missing values # get the columns name cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()] # perform drop reduced_X_train = X_train.drop(cols_with_missing, axis = 1) ## 2. imputation from sklearn.impute import SimpleImputer my_imputer = SimpleImputer() imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train)) # imputation removed column names, put back imputed_X_train.columns = X_train.columns ## 3. extended imputation X_train_plus = X_train.copy() for col in cols_with_missing: X_train_plus[col + \u0026#39;_was_missing\u0026#39;] = X_train_plus[col].isnull() my_imputer = SimpleImputer() imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus)) imputed_X_train_plus.columns = X_train_plus.columns 1.2 Categorical variables There are three ways to deal with categorical variables (non-numeric data):\nDropping the categorical variables (if the columns does not provide any useful information) Ordinal encoding — assign a unique value in the dataset to a different integer. One-hot encoding — create new columns to indicate the presence of each possible value in the original data One hot encoding example\nFor One-hot encoding, it means that if a column with 100 rows contains 100 unique values, it will create an extra (100 rows *100 unique values –100 original rows) new entries.\nWe can apply the 3 approaches with the following code:\n## 1. droppping categorical variables dorp_X_train = X_train.select_dtypes(exclude= [\u0026#39;object\u0026#39;]) ## 2. ordinal encoding from sklearn.preprocessing import OrdinalEncoder label_X_train = X_train.copy() ordinal_encoder = OrdinalEncoder() label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols]) ## 3. one hot encoding from sklearn.preprocessing import OneHotEncoder # one hot encode categorical data OH_encoder = OneHotEncoder(handle_unknown = \u0026#39;ignore\u0026#39;, sparse = False) OH_cols_train = pd.DataFrame(OH_encoder.fit_transform[object_cols]) # add back removed index OH_cols_train.index = X_train.index # remove categorical column and add back the one hot encoded columns num_X_train = X_train.drop(object_cols, axis = 1) OH_X_train = pd.concat([num_X_train, OH_cols_train], axis = 1) 2. Pipelines Pipeline is a way to bundle our preprocessing and modelling steps to keep our code organized. The benefits of using pipeline are it gives a cleaner code, reduces bugs and make our model easier to be implemented.\nHere’s how we can apply pipeline to impute missing numerical entries and one-hot encode missing categorical entries:\nfrom sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder # preprocess numerical data numerical_transformer = SimpleImputer(strategy = \u0026#39;constant\u0026#39;) # preprocess categorical data categorical_transformer = Pipeline(steps = [ (\u0026#39;imputer\u0026#39;, SimpleImputer(strategy = \u0026#39;most frequent\u0026#39;)), (\u0026#39;onehot\u0026#39;, OneHotEncoder(handle_unknown = \u0026#39;ignore\u0026#39;)) ]) # bundle the two preprocesses preprocessor = ColumnTransformer( transformers = [ (\u0026#39;num\u0026#39;, numerical_transformer, numerical_cols), (\u0026#39;cat\u0026#39;, categorical_transformer, categorical_cols) ]) # bundle preprocessing and modelling my_pipeline = Pipeline(steps = [ (\u0026#39;preprocessor\u0026#39;, preprocessor), (\u0026#39;model\u0026#39;, model) ]) # model evaluation my_pipeline.fit(X)train, y_train) preds = my_pipeline.predict(X_valid) mean_absolute_error(y_valid, preds) 3. Cross-validation Cross-validation means we run our modelling process on different subsets of the data to get several measures of our model quality. Although this technique gives a more accurate measure of model quality, it can take some time to run as it need to estimate multiple models as we can see from the below images.\nIt is recommended to run cross-validation for smaller datasets while for larger datasets, a single validation if often suffice.\nCross-Validation\nHere’s how we can apply cross-validation in Python together with pipeline which we learned earlier on:\nfrom sklearn.model_selection import cross_val_score # split the data to 5 sets for validation scores = -1 * cross_val_score(my_pipeline, X, y, cv = 5, scoring = \u0026#39;neg_mean_absolute_error\u0026#39;) print(scores.mean()) It is surprising to see that negative mean absolute error is used in the code. This is because “sklearn” has a convention where all metrics are defined so a high number is better. Thus, the use of negatives allows convention consistency.\n4. XGBoost — Gradient Boosting In the random forest method, we improve a model prediction by averaging the prediction of many decision trees. Random forest method is one of an ensemble method where we combine the prediction of several models. A state-of-the-art method would be to apply gradient boosting where we perform iterative cycles to add models into an ensemble to result in better prediction.\nConcepts:\nUse the current ensemble to generate predictions for each observation in the dataset. Use the prediction to calculate a loss function. Use the loss function to fit a new model that will be added to the ensemble which will reduce the loss. Add this new model to the ensemble. Repeat Here’s how to do it in code:\nfrom xgboost import XGBRegressor from sklearn.metrics import mean_absolute_error my_model = XGBRegressor() my_model.fit(X_train, y_train) predictions = my_model.predict(X_valid) mean_absolute_error(predictions, y_valid) There are a few parameters in the xgregressor function that might affect the accuracy of our result:\nn_estimators which means how many times to go through the modelling cycle (concepts above), value too high or too low might result in overfitting or underfitting respectively early_stopping_rounds which means stopping the model when the validation score stops improving with imposed criteria learning_rate which means multiply the predictions from each model by a small number before adding up the prediction from each component model n_jobs which aims to improve model’s runtime. We can set the number equal to the cores of our machine model = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, n_jobs = 4) model.fit(X_train,y_train, early_stopping_rounds = 5, eval_set = [(X_valid, y_valid)], verbose = False) 5. Data Leakage Data leakage happens when training data contains information about the target, but similar data will not be available when we used the model to perform prediction. The two main types of data leakage are target leakage and train-test contamination.\n5.1 Target leakage Target leakage occurs when predictors include data that will not be available at the time when predictions is made. A way to overcome this issue is to think about timing or chronological order that data becomes available rather than whether a feature will help to make good predictions.\n5.2 Train-test contamination Train-test contamination happens when validation data affects the preprocessing behavior as the validation process is corrupted.\nFor example, imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions. In the courses, there are several interesting case studies on data leakage that worth looking to improve our acumen when interpreting results from work.\nCase 1\nGuide:\nThis is tricky, and it depends on details of how data is collected (which is common when thinking about leakage). Would you at the beginning of the month decide how much leather will be used that month? If so, this is ok. But if that is determined during the month, you would not have access to it when you make the prediction. If you have a guess at the beginning of the month, and it is subsequently changed during the month, the actual amount used during the month cannot be used as a feature (because it causes leakage).\n","permalink":"https://keanteng.github.io/home/docs/2023-03-02-intermediate-machine-learning/","summary":"Using machine learning in data analysis is a rather procedural approach. As we can notice from the approach, we will start by \u003cem\u003ePreparing data \u0026raquo; Defining a model \u0026raquo; Model diagnostic checking \u0026raquo; Model prediction\u003c/em\u003e to complete our workflow. This workflow is often and commonly practiced when we are doing data analysis work.","title":"Intermediate Machine Learning"},{"content":" Images from Unsplash\nDisclaimer: This article is for educational purpose only and the author does not suggest any illegal usage of this technology (generative AI). Please be responsible for your generations and creations. Do not use them for any malicious intent or harm in any form and be respectful.\nThere are two ways to use Stable Diffusion — either locally on your own PC or through a cloud computing services such as Google Collab and Kaggle.\nIt has been a frustration for many without a decent GPU and sufficient VRAM. This has caused long computation time and a lot of friction for you to tune your model parameter/ prompt, and worse, you are not even able to load the Stable Diffusion model.\nSources: Stable Diffusion WebUI https://cdn.changelog.com/uploads/news_items/RdAG/large.png?v=63829970528\nI will break this article in 3 parts according to the resources I gathered, and you are free to discover the one that you see fit.\n1. Google Collab — Realistic \u0026amp; Animated Image Generation Terms: google collab, civitai, account\nGoogle Collab offers a decent and fast GPU and long-running hours for any Google users with a Google account. You can check out the YouTube video here by Nolan Aatama where you can learn about some quick installation steps to run Stable Diffusion in your own account. The account provides installation guide on various models such as Dreamshaper and more.\nYou can check out the models offered on Civitai to look for prompt examples and user review of the model as well as other models offered on the platform.\n# prompts example positive: space, rocket, stallite, earth, milky ways, space dust, high res, 8k negative: lowres, bad hands, bad fingers, sketeches, paintings, If you want to check the prompts for any of the generated images you like can use this Stable Diffusion Decomposer to get the details of each generated images.\nStable Diffusion Decomposer\nYou need to create a Civitai account to post your review and submit comment.\nDo note that it takes around 10 minutes to run the code. It took around 20 seconds to generate a 512x1024 pixels image which is decent. For Google Collab, I think there is no computing quota being set and you can run the program for a few hours. But do avoid leaving the tabs idle as Google will reconnect your program which caused you to re-run all the code again.\n2. Kaggle — Animated Image Generation Terms: kaggle, webui, gpu, telegram\nI only manage to find scripts that provide animated image generation on Kaggle, you can look at it here. You just need to copy and run all the codes in the notebook to access the WebUI. Do note that on Kaggle, you only have 30 hours of GPU resources being allocated, so use it wisely and shut down the connection when you are not running the program to conserve the quota allocated to you.\nTurn off GPU if it is not in use\nIf you like animated image and you don’t want to always re-run the scripts to access WebUI, you can also check out this site, a site that host the WebUI for animated image generation. You can get around 100 quotas by signing in daily (which allows you to generate about 90+ images). You also need to acquire the sign-in token with a Telegram account.\nSome of the model offered on the site\n3. Your Own PC Terms: local, google translation, extension, apply and restart\nIf you want to run Stable Diffusion locally, you can have a read on this article on the installation guide. You can use Google Translate for the article as it is in Chinese.\nBasically, this is what you need to do:\nInstallation of Stable Diffusion WebUI Load models, put in models/Stable-diffusion/ files after the installation step above (can get from Civitai website) After opening the local URL, go to the Extensions tab and install https://github.com/civitai/sd_civitai_extension. Go to Installed tab and click Apply and Restarts UI Done! ","permalink":"https://keanteng.github.io/home/docs/2023-02-25-stable-diffusion-webui-with-civitai-loras/","summary":"There are two ways to use Stable Diffusion — either locally on your own PC or through a cloud computing services such as Google Collab and Kaggle. It has been a frustration for many without a decent GPU and sufficient VRAM. This","title":"Stable Diffusion WebUI with Civitai LORAs"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\n0. Concepts Creation and evaluation of a deep learning model is a procedural work with steps. Data preparation \u0026raquo; Model optimization\u0026raquo; Model fitting \u0026raquo; Model evaluation \u0026raquo; Model prediction\nNote:\nModel optimization (adam optimizer) Model fitting (batch, epoch) Model evaluation(dropout, batch normalization) 1. Modelling \u0026amp; Neural Network Terms: layer, activation function, neural network, dense, rectified linear unit\nSimple Neural Network (linear)\nThe above neural network can be generated with code using Python. The network above organized neurons into layers. Collecting linear units with common set of inputs result in a dense layer.\nfrom tensorflow import keras from tensorflow.keras import layers # 1 linear unit network model = keras.Sequential([ layers.Dense(units = 1, input_shape = [1]) ]) model = keras.Sequential([ layers.Dense(units = 1, input_shape = [3]) ]) If we want to fit a curve (non-linear), a feature called activation function is needed, otherwise the neural network can only learn linear relationships. A common example is the rectifier function max(0,x), where we get a rectified linear unit when we attach the function to a linear unit.\nWe can also stack layers to achieve a complex data transformation.\nVisualizing the Network\nUsing code to define the network:\nfrom tensorflow import keras from tensorflow.keras import keras # [layer1, layer2, layer3,...] model = keras.Sequential([ layers.Dense(units = 4, activation = \u0026#39;relu\u0026#39;, input_shape = [2], layers.Dense(units = 3, activation = \u0026#39;relu\u0026#39;), layers.Dense(units = 1) ]) 2. Model Optimization \u0026amp; Fitting Terms: loss function, stochastic gradient descent, batch, epoch, error/loss\nSimilar to regression, we need to know how well our model fits the data. We use loss function to measures the difference between observed and predicted values. The common measure would be the mean absolute error where it computed the average length between the fitted curve and data points. Other errors examples are mean-squared error or Huber loss.\nHow can we minimize the error? We can use optimization algorithms — the stochastic gradient descent. The concepts as follows:\nGet random sample (batch) from original dataset and run through the network for prediction. Measure the error (loss) Adjust the weight in a direction that makes the loss smaller (an epoch for each complete round a training) We can use a built-in “adam” optimizer to optimize our model. This allows self-tuning to minimize loss. Below is code to fit 256 rows of training data for 10 times:\n# get training data dimension (row, column) print(X_train.shape) # define model from tensorflow import keras from tensorflow.keras import layers mode = keras.Sequential([ layers.Dense(512, activation = \u0026#39;relu\u0026#39;, input_shape = [11]), layers.Dense(512, activation=\u0026#39;relu\u0026#39;), layers.Dense(512, activation=\u0026#39;relu\u0026#39;), layers.Dense(1), ]) # add in optimizer and loss function model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;mae\u0026#39;, ) # model fitting history = model.fit( X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 256, epochs = 10, ) # visualize outcome import pandas as pd # training history to data frame history_df = pd.DataFrame(history.history) # plot the loss history_df[\u0026#39;loss\u0026#39;].plot(); 3. Model Evaluation 1 Terms: underfitting, overfitting, capacity, training, callback, stopping criteria\nTraining data normally contains signal and noise where signal helps our model make prediction from new data and noise represent the random fluctuation coming from data in the real world. To see if our model fits the data well, we will compare learning curve between training set and validation set.\nUnderfitting the training set is when the loss is not as low as it could be because the model hasn’t learned enough signal. Overfitting the training set is when the loss is not as low as it could be because the model learned too much noise.\nA model’s capacity is the size and complexity of the patterns it is able to learn. We can adjust a model’s capacity if we detect overfitting or underfitting scenarios.\n# sample model model = keras.Sequential([ layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.Dense(1), ]) # wider model = keras.Sequential([ layers.Dense(32, activation = \u0026#39;relu\u0026#39;), layers.Dense(1), ]) # deeper model = keras.Sequential([ layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.Dense(1), ]) Sometimes the validation increase during training after a certain point although it kept decreasing early on. This can due to a model is learning noise from the datasets. We can overcome this issue by imposing an early stopping criterion.\nA callback function is used where we detect when the validation loss starts to rise again, and we reset the weights back the where the minimum occurred. An example “if there is not at least 0.001 improvement in the validation loss over 20 epochs, then stop training and keep the best model you found”.\nfrom tensorflow.keras.callbacks import EarlyStopping early_stopping = EarlyStopping( min_delta = 0. 001, # min change to count as improvement patience = 20, # how many epochs to wait before stopping restore_best_weights = True, ) 4. Model Evaluation 2 Terms: dropout, batch normalization\nTo correct overfitting in our model, we can implement the idea of dropout where we randomly drop out some fraction of a layer’s input units every step of training. This avoids the network to learn spurious patterns in the training data which leads to overfitting.\nmodel = keras.Sequential([ ... layers.Dropout(rate = 0.3) # 30% dropout to the next layer layers.Dense(16), ... ]) To correct slow or unstable training, we can apply batch normalization to put all data on a common scale. SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.\nmodel = keras.Sequential([ layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.BatchNormalization(), ]) # or model = keras.Sequential([ layers.Dense(16), layers.BatchNormalization(), layers.Activation(\u0026#39;relu\u0026#39;), ]) Example for a full model fitting process:\nfrom tensorflow import keras from tensorflow.keras import layers # define network model = keras.Sequential([ layers.Dense(1024, activation=\u0026#39;relu\u0026#39;, input_shape=[11]), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1024, activation=\u0026#39;relu\u0026#39;), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1024, activation=\u0026#39;relu\u0026#39;), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1), ]) # add optimier model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;mae\u0026#39;, ) # model fitting history = model.fit( X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 256, epochs = 100, verbose = 0, ) # visualize history_df = pd.DataFrame(history.history) history_df.loc[:,[\u0026#39;loss\u0026#39;,\u0026#39;val_loss\u0026#39;]].plot(); 5. Binary Classification Terms: sigmoid, binary, accuracy\nWe cannot use accuracy to measure model performance as it does not change smoothly — it changes in jumps as it represents ratio counts. Thus, a replacement would be the cross-entropy function where it measures the distance between probabilities. To convert outputs from dense layer into probabilities, we will use the sigmoid activation function.\nCode example:\nfrom tensorflow import keras from tensorflow.keras import layers # define network model = keras.Sequential([ layers.Dense(4, activation=\u0026#39;relu\u0026#39;, input_shape=[33]), layers.Dense(4, activation=\u0026#39;relu\u0026#39;), # final layer need sigmoid function to produce class probabilities layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;), ]) # add optimizer model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;binary_crossentropy\u0026#39;, metrics = [\u0026#39;binary_accuracy\u0026#39;] ) # add stopping criteria early_stopping = keras.callbacks.EarlyStopping( patience = 10, min_delta = 0.001, restore_best_weights = True, ) # model fitting history = model.fit( X_train, y_train, validation_data=(X_valid, y_valid), batch_size=512, epochs=1000, callbacks = [early_stopping], verbose = 0 # hide output since too many to display ) # model levaluation history_df = pd.DataFrame(history.history) # starts at epoch 5 history_df.loc[5:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_df.loc[5:, [\u0026#39;binary_accuracy\u0026#39;, \u0026#39;val_binary_accuracy\u0026#39;]].plot() print((\u0026#34;Best Validation Loss: {:0.4f}\u0026#34; +\\ \u0026#34;\\nBest Validation Accuracy: {:0.4f}\u0026#34;)\\ .format(history_df[\u0026#39;val_loss\u0026#39;].min(), history_df[\u0026#39;val_binary_accuracy\u0026#39;].max())) 6. Reference Learn Intro to Deep Learning Tutorials - Kaggle ","permalink":"https://keanteng.github.io/home/docs/2023-02-24-intro-to-deep-learning/","summary":"Creation and evaluation of a deep learning model is a procedural work with steps. *Data preparation \u0026raquo; Model optimization\u0026raquo; Model fitting \u0026raquo; Model evaluation \u0026raquo; Model prediction Note Model optimization (adam optimizer) Model fitting (batch, epoch)","title":"Intro to Deep Learning"},{"content":" Images from Unsplash\nIn this article, we will learn how to image scraping on this National Geographic article: Asteroids vs. comets: How do they differ, and do they pose a threat to Earth? We will learn how to download in bulk the high-quality images in the webpage with Python code.\nThe process:\nFind the webpage data structure for the URL specified. Search for the images link and save in a list. Download all the images. Find the webpage data structure for the URL specified We will be using Visual Studio Code with Python and Jupyter notebook configured. However, a Python code file will also do the job. Here are the three packages to be installed using Window PowerShell:\n# Lauch Power Shell by pressing Start and then type Power Shell in the search bar # installation procedure py -m pip install requests py -m pip install bs4 py -m pip install urllib First, we start by importing the modules needed. Then, we need to make HTTP request to view the webpage data structure we want. This can be done as follows:\nimport requests from bs4 import BeautifulSoup import urllib.request def getdata(url): r = requests.get(url) return r.text htmldata = getdata(\u0026#34;https://www.nationalgeographic.co.uk/space/2023/01/asteroids-vs-comets-how-do-they-differ-and-do-they-pose-a-threat-to-earth\u0026#34;) soup = BeautifulSoup(htmldata, \u0026#39;html.parser\u0026#39;) You can check the output by typing the variable name:\nOutput representing webpage structure.\nSearch for the images link and save in a list Now, we want to search for all the images links that embedded images in the webpage. We will use a test list to store each of the link as list of elements for download later. The print function is used to print out all the links that are being searched.\ntest = list() for item in soup.find_all(\u0026#39;img\u0026#39;): print(item[\u0026#39;src\u0026#39;]) test.append(item.get(\u0026#39;src\u0026#39;)) All the searched links in list\nDownload all the images Now simply using a for loop, we can save all the images inside our current folder. The number of iterations, n will be the number of images in the list. In our code, we will also rename the images with number 0,1,2,…,n to avoid duplicate file name.\nfor i in range(len(test)): urllib.request.urlretrieve(test[i], str(i)+\u0026#34;.jpg\u0026#34;) All the images being downloaded\nExtra: Some website will have several images urls mixed up including .svg, .png., .gif and .jpg. We can use if statement to counter this issue:\ntemp = list() for i in range(len(test)): if test[i].__contains__(\u0026#39;.png\u0026#39; or \u0026#39;.jpg\u0026#39; or \u0026#39;.jpeg\u0026#39;): temp.append(test[i]) for i in range(len(temp)): urllib.request.urlretrieve(temp[i], str(i)+\u0026#34;.jpg\u0026#34;) That said, job well done. Do note that this approach has restriction on the type of web page for scraping. It works best for articles pages with images embedded. If you encounter connection time out error with your request, do have a check on the site accessibility (Internet Service Provider). Otherwise, it should work perfectly!\nReferences: Beautiful Soup Documentation — Beautiful Soup 4.4.0 documentation (beautiful-soup-4.readthedocs.io) urllib.request — Extensible library for opening URLs — Python 3.11.2 documentation ","permalink":"https://keanteng.github.io/home/docs/2023-02-19-images-scraping-from-web-pages/","summary":"In this article, we will learn how to image scraping on this National Geographic article: Asteroids vs. comets: How do they differ, and do they pose a threat to Earth? We will learn how to download in bulk the high-quality images in the webpage with Python code.","title":"Images Scraping from Web Pages"},{"content":"In this article, I will be showing the process of scraping some listed companies market capitalization in Malaysia data using Google Sheet. We will perform web scraping on the i3 Investor site.\nImages from Unsplash\nGenerate Webpage for Scraping First of all, open a new google sheet and create a table like this:\nCreate a table like this\nInside the table, we have a few companies name and their listed code. Notice that in cell C3, we put a link — this link will serve as a “prefix”. If the stock code is put at the back of the link, it will direct to the webpage of the particular page.\nprefix link: https://klse.i3investor.com/web/stock/overview/ link to webpage: https://klse.i3investor.com/web/stock/overview/1023 Now, we use the concatenate function to append the code to the prefix links for all the companies in the table:\nUse CONCAT() function to create the links\nWeb Scraping in Action Before we start web scraping, we need to learn about this function — IMPORTXML().\n=importxml(\u0026#34;url\u0026#34;, \u0026#34;query\u0026#34;) Notice that we already have all the URLs needed as we have created the links on the table. Now, the missing piece is called “query” — simply means what do we want to know and where can it be found?\nThe query is called the XPath where it is used in web browser, now let’s hope into Maybank stock page and get the XPath query.\nHighlight the market capitalization amount\nAfter clicking inspect, a window will pop out highlighting a code segment. Here, we need to right click and select Copy full XPath.\nClick copy full XPath\nLet’s hope back to Google Sheet, you will be pasting the query as follow:\n=importxml(\u0026#34;url\u0026#34;,\u0026#34;/html/body/div[3]/div/div[2]/div[8]/div[1]/div[2]/div[1]/div[2]/p/strong\u0026#34;) Note: The URL will be the URLs in the Reference column After applying the formula to the Market Cap column, you will manage to scrape all the Market Capitalization data on the sheet — the data will be updated live.\nData will be loaded once your apply the formula\n","permalink":"https://keanteng.github.io/home/docs/2022-11-05-google-sheet-simple-web-scraping/","summary":"In this article, I will be showing the process of scraping some listed companies market capitalization in Malaysia data using Google Sheet. We will perform web scraping on the i3 Investor site. First of all, open a new google sheet and","title":"Google Sheet Simple Web Scraping"}]