[{"content":" Images from Unsplash\nBlueAutomata, MIT Licensed Â© Kean Teng 2023\n0. Introduction Reporting is a crucial aspect of various sectors across industries to ensure transparency, compliance, decision-making, and accountability. It helps in decision-making to build trust, manage risk and informed decision. Generally, workflows for reporting involves gathering raw data, cleaning of raw data and compiling raw data into their respective template using Microsoft Excel. This can be a repetitive and routine tasks in large organization which is time-consuming and mistake-prone when dealing with large number of raw data during these processes due to human errors.\nblueautomata is a Python package built to provide a framework to automate the workflow for reporting in a company. Such framework allows you to automate the data cleaning, data compilation and data preparation in reportable format on Excel by auto-calling written macros processes.\n1. A Case Study A New Manager In the year 2055, a guy named Travis Umbrella become the Manager of LightSpeed Investment under the Department of Data Management. Previously, he works as the Manager under the Department of Investment. He is an adventurous person and dare to try something new, so when opportunities come, he decided to advance in the field of managing data to get some new perspective in his work.\nIn fact, Travis works in a weird company that might not exist on this planet because the company makes investment in a weird way but with a high return. The company gives stocks to the employee so that they can sell in stock markets, around the world.\nFor each employee, they can only receive one type of stock like Google GOOG, but they are allowed to sell in a few markets such as New York Stock Exchange NYSE and Hong Kong Stock Exchange HKSE. Every month, Travis needs to submit a report for auditing where he needs to track down the employees stock holding type and the market that they sell their stock. LightSpeed Investment is a fast-moving company, where employees come and go staying less than 3 months. So at the end of the month when Travis submits his report he has to make sure that employees that resigned should not be included in his report.\nTime To Act For Travis to complete his report he would first need to get hold of the stocks\u0026rsquo; data that are traded by the firms\u0026rsquo; employees in worldwide stock markets. He then stored the transactions record for each stock market in an individual file for reference. Since there is no well-build system for the data collection, each of the file looks different, some contains more columns than the other, some with missing information such as employees\u0026rsquo; name and some even contains wrong employee IDs, wrong employee names and outdated information.\nTravis realizes that he must find some other data to validate his collected data, so he reached out to his friend, James Claude Diamond from the Human Resources Department to get the latest copy of the staff data where it contains the staff\u0026rsquo;s assigned stocks and stock market that they can sell their stock. He also compiles a copy of checklist data by himself to match up the stock code and stock market code to their respective stock name and stock market name.\nThink Different From an investment background, Travis is good at math and logical thinking. Of course, he knows for resigned employees their names will not be in the staff dataset. In fact, what he will do is he will match the staff ID from the stock market data he collected with the staff data to obtain information as follows:\nThe staff\u0026rsquo;s name The staff\u0026rsquo;s assigned stock The staff\u0026rsquo;s status (resigned or not) The staff\u0026rsquo;s cube (type of options) That approach, allows him to work quickly to compile the data he collects from the stock markets, what he will do then, is to create files for each stock category to see where they are traded around the world - of course with a nice looking template before submitting for audit review after the quarter ends!\nPattern Finding Travis is quick to find patterns in his collected raw data, he noticed that although there are empty cells about the staff name, department and department code, the staff ID is always recorded whenever transaction is performed. Of course, with the staff data on his hand, he can then learn about the stock that the staff used.\nMoreover, although staff data does not tell him about the department code, but it does tell him the staff department name where he can then make reference to his checklist data for the department code. Well, he can also reference the cube code on his checklist for the name of the code.\n2. A Notorious Case Travis\u0026rsquo;s idea is able processed most of his datasets. But there are some notorious dataset that puzzled him. He noticed that on one of the data he collected from HKSE the name of the staff does not match with the company staff directory, for example \u0026ldquo;Muhamad Ali bin Abu\u0026rdquo; is named as \u0026ldquo;Muhamad Ali Abu\u0026rdquo;; \u0026ldquo;Lucas Den Shi Ki\u0026rdquo; is named as \u0026ldquo;Lucas Shi Ki Den\u0026rdquo; and \u0026ldquo;Jeff Adrian\u0026rdquo; is named as \u0026ldquo;Jef aA_Drian\u0026rdquo;. And in the dataset, there is no IDs column and only name column.\nTo resolve this, he has to rely on a metric, called the Levinshtein ratio that computes the difference between two sequences. Generally if two sequences are exactly the same, the Levinshtein ratio will be 100 and when they gradually differ more in terms of arrangement and alphabets the ratio will decrease.\n3. Towards The End What he will do then is he will match all the names in the staff data against the similar names, if the Levinshtein ratio is higher than 90, then the similar names will be replaced with the names in the staff data.\nWith all the data being cleaned and labelled, Travis can now start to prepare for his reports. Since all the data are stored as one dataset, he would need to export it as individual Excel files for each stock category:\nNow Travis gets all the files he wants, but these files are raw, he\u0026rsquo;ll use VBA to add some formatting to them so that they look nice before submitting to his boss. The good news is he don\u0026rsquo;t really need to do a loop to apply the macros he wrote because the macro can run on the entire specify folder:\nHere is the output that Travis will get for each of his stock category, the picture showing the file before using macros and after using macros:\n4. Real Applications BlueAutomata BlueAutomata is a class that is build for data compilation workflow, the class takes in 5 parameters in the form of path and list. Let\u0026rsquo;s look at an example of processing raw data in folders into a masterlist:\nfrom blueautomata.data_compilation import BlueAutomata test = BlueAutomata( folder_path= \u0026#39;PATH\u0026#39;, checklist = \u0026#39;PATH\u0026#39;, staff_data = \u0026#39;PATH\u0026#39;, name_key= [\u0026#39;BSE\u0026#39;, \u0026#39;HKEX\u0026#39;, \u0026#39;KLSE\u0026#39;, \u0026#39;LSE1\u0026#39;, \u0026#39;NASDAQ\u0026#39;, \u0026#39;NYSE\u0026#39;, \u0026#39;SGX\u0026#39;, \u0026#39;SSE\u0026#39;, \u0026#39;TSE\u0026#39;], name_code= [1, 1, 1, 1, 1, 1, 1, 1, 1], ) df = test.automata_execution() # print the dataframe df.head() Output, first five rows\nAutomataReport This package also provides function for you to take a preview at the output with a summary report based on your input parameter, for example:\nfrom blueautomata.automation_report import AutomataReport test = AutomataReport( folder_path=r\u0026#34;C:\\Users\\Khor Kean Teng\\Downloads\\AUP Automata\\data\\fakesystem\u0026#34;, checklist = \u0026#39;data/checklist.xlsx\u0026#39;, staff_data = \u0026#39;data/fake_hr_data.xlsx\u0026#39;, name_key= [\u0026#39;BSE\u0026#39;, \u0026#39;HKEX\u0026#39;, \u0026#39;KLSE\u0026#39;, \u0026#39;LSE1\u0026#39;, \u0026#39;NASDAQ\u0026#39;, \u0026#39;NYSE\u0026#39;, \u0026#39;SGX\u0026#39;, \u0026#39;SSE\u0026#39;, \u0026#39;TSE\u0026#39;], name_code= [1, 1, 1, 1, 1, 1, 1, 1, 1], ) df = test.automata_report_summary() # print the dataframe df.head(n = len(df)) Output, all possible matches and unmatches\nInconsistency A special class named Inconsistency is created to deal with Excel file that:\nDoes not have a User ID column Have a Name column, but the names are written in format different from the staff data. Even for files with only Name column and all the names are correct but no User ID, this class is still able to compile the data. A wrong dataset example with only two columns:\nWrong name column\nLet\u0026rsquo;s apply this package to fix:\nfrom blueautomata.inconsistency import Inconsistency test = Inconsistency( filepath=\u0026#34;data/fakesystem/experiment/sample1.xlsx\u0026#34;, staff_data=\u0026#34;data/fake_hr_data.xlsx\u0026#34;, checklist=\u0026#34;data/checklist.xlsx\u0026#34;, sheet_number=0, ) df = test.fix_inconsistency() df.head() Output, fixed name and granular\nSystemCubeChecker This package also contains a class called SystemCubeChecker to update the wrongly assigned system and cube. In other words, it will update the entries where the cube names are put as system names by moving the cube names to the cube column and re-assigning a system name to them.\nData with wrong system columns\nLet\u0026rsquo;s fix it:\nfrom blueautomata.system_cube import SystemCubeChecker test = SystemCubeChecker( masterlistpath = \u0026#39;data/fakesystem/experiment/sample2.xlsx\u0026#39;, system_to_check= [\u0026#39;Avatar\u0026#39;, \u0026#39;Ironman\u0026#39;, \u0026#39;Thor\u0026#39;, \u0026#39;Spiderman\u0026#39;, \u0026#39;Eternals\u0026#39;], cube_to_assign=\u0026#39;Marvel\u0026#39; ) df = test.system_cube_update() df.head() Output, fixed system and cube columns\nOthers A Function to export large datasets into individual files based on column category of Department:\nfrom blueautomata.batch_export import BatchExport df = pd.read_excel(\u0026#39;data/output.xlsx\u0026#39;) temp = BatchExport( destination=\u0026#39;data/dept\u0026#39;, masterlist= df, ) temp.batch_export() A function to call VBA macros within Excel:\nfrom blueautomata.to_vba import automate_vba temp = automate_vba( filepath=r\u0026#39;C:\\\\Users\\\\user\\\\Downloads\\\\Automata\\\\vbanew.xlsm\u0026#39;, macro = \u0026#39;vbanew.xlsm!Module1.template\u0026#39; ) temp.templatetize() 5. References Modified from https://keanteng.github.io/blueautomata/ 6. Disclaimer The case study is purely a creative endeavor to illustrate how this package is inspired and should not be interpreted as a representation of reality. Any opinions, beliefs, or ideologies expressed within the story do not reflect those of the author or any other entity mentioned herein.\nIt is a work of fiction. Names, characters, places, and incidents are the products of the author\u0026rsquo;s imagination or are used fictitiously. Any resemblance to actual events, locales, or persons, living or dead, is entirely coincidental.\n","permalink":"https://keanteng.github.io/home/docs/2023-09-07-introducing-blue-automata/","summary":"Images from Unsplash\nBlueAutomata, MIT Licensed Â© Kean Teng 2023\n0. Introduction Reporting is a crucial aspect of various sectors across industries to ensure transparency, compliance, decision-making, and accountability. It helps in decision-making to build trust, manage risk and informed decision. Generally, workflows for reporting involves gathering raw data, cleaning of raw data and compiling raw data into their respective template using Microsoft Excel. This can be a repetitive and routine tasks in large organization which is time-consuming and mistake-prone when dealing with large number of raw data during these processes due to human errors.","title":"Introducing BlueAutomata"},{"content":" Images from Unsplash\nMkDocs is a fast, simple and downright gorgeous static site generator that\u0026rsquo;s geared towards building project documentation. With source files in Markdown configured with a YAML configuration, site can be easily generated and deployed. Material for Mkdocs is build on top of Mkdocs, and it is targeted for project documentation with a set of powerful and beautiful documentation framework. Let\u0026rsquo;s go through how the site below is built:\nMy created site\nGetting Started Let\u0026rsquo;s create a folder and open it using VS Code (File \u0026gt; Open Folder). Then using pip we can install mkdocs-material to our machine:\npip install mkdocs-material Before we are ready to start creating content for our site, we need to load some files from Material for Mkdocs:\npy -m mkdocs new folderpath Below files will be created:\nyourfolder - docs - index.md - mkdocs.yml Configure Your Site To configure your site, you can refer to some references here, here\u0026rsquo;s some code for you to start quickly and you can then make changes whenever you see fit:\n# site info site_name: WEBSITE_NAME site_url: \u0026#34;\u0026#34; site_author: \u0026#39;YOUR NAME\u0026#39; site_description: \u0026#39;YOUR DESCRIPTION\u0026#39; # site theme theme: name: material palette: primary: lime accent: black plugins: - search: lang: en features: - navigation.footer - content.code.copy logo: assets/logo.png favicon: assets/favicon.png # add repository repo_url: YOUR GIT REPO LINK repo_name: GITUSERNAME/GITREPONAME # copyright copyright: PACKAGE NAME \u0026amp;copy; 2023 YOUR NAME # social links extra: social: - icon: fontawesome/brands/github link: - icon: fontawesome/brands/medium link: - icon: fontawesome/brands/linkedin link: - icon: fontawesome/brands/instagram link: - icon: fontawesome/brands/twitter link: You can also type something inside the index.md file, for example:\n# Lorem Ipsum \u0026gt; Vitae suscipit tellus mauris a. Euismod nisi porta lorem mollis. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. - Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. - Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. To see how the content looks like on your site, type in terminal:\npy -m mkdocs serve A link will pop up and direct you to your site you have just created.\nPublish As GitHub Page If you want to put this site online, you can use GitHub to do that. Let\u0026rsquo;s start by creating an empty repository on GitHub and upload your files there:\n# first commit for empty repo echo \u0026#34;#reponame\u0026#34; \u0026gt;\u0026gt; README.md git init git add README.md git commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin https://github.com/username/reponame.git git push -u origin main # second commit and upload your package git add . git commit -m \u0026#34;second commit\u0026#34; git push Then go to the repository page and create a new branch called gh-pages:\nThen go to Settings. You need to do two things here:\nGo to Page and add the gh-pages branch that you created just now to Build and Deployment Go to Actions \u0026gt; General and change the Workflow permissions to Read and write perission Then let\u0026rsquo;s get back to Visual Studio Code where you work with your website, you need to create a file that look like this:\n- docs - mkdocs.yml - .github - workflows - ci.yml Then in ci.yml paste this into the file:\nname: ci on: push: branches: - master - main permissions: contents: write jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - run: echo \u0026#34;cache_id=$(date --utc \u0026#39;+%V\u0026#39;)\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - uses: actions/cache@v3 with: key: mkdocs-material-${{ env.cache_id }} path: .cache restore-keys: | mkdocs-material- - run: pip install mkdocs-material - run: mkdocs gh-deploy --force Once you push the changes to your repository, you can go to the Actions tab, and you can see your website link is being created under pages build and deployment!\n","permalink":"https://keanteng.github.io/home/docs/2023-09-04-creating-a-site-using-materials-mkdocs/","summary":"Images from Unsplash\nMkDocs is a fast, simple and downright gorgeous static site generator that\u0026rsquo;s geared towards building project documentation. With source files in Markdown configured with a YAML configuration, site can be easily generated and deployed. Material for Mkdocs is build on top of Mkdocs, and it is targeted for project documentation with a set of powerful and beautiful documentation framework. Let\u0026rsquo;s go through how the site below is built:","title":"Creating a Site Using Materials Mkdocs"},{"content":" Images from Unsplash\nLet\u0026rsquo;s talk about Python package publishing. You have spent hours creating a Python package with some wonderful tools in it, and you want to share to others so that they can use it to simplify their work, speed up their workflows, this article will share about the ways that how you can release your package to the world.\nCreating Package For Python package publishing, we will use Pypi which is a repository of software for the Python programming language. On the platform, it allows you to find and install software developed by the Python community, it is also a platform where package authors distribute their software.\nGenerally, you can use Python to create a package that looks like this:\nCURRENTFOLDER - package_name - class_1.py - class_2.py In the above example, we are creating a class for each Python script. Python classes provide a mean of bundling data and functions together. And the classes are stored in a folder named package_name and this package is stored inside a folder (make sure you use File\u0026gt; Open Folder to read the folder content on VS Code). It is good to not put any print statements in class, but you can write functions with return output for your class.\nNow, let\u0026rsquo;s add a __init__.py script and import the classes, and you are ready to publish this package:\npackage_name - __init__.py - class_1.py - class_2.py # __init__.py from package_name.class_1.py import class_A from package_name.class_2.py import class_B Package Publishing To publish your created package, you first have to create an account at Pypi. You might need to perform two-factor authentication for your account creation where you need to install Google Authenticator on your mobile phone to generate OTP to verify your account.\nUsing GitHub Then, create a GitHub repository for your package and upload the package there, here\u0026rsquo;s some guide:\n# first commit for empty repo echo \u0026#34;#reponame\u0026#34; \u0026gt;\u0026gt; README.md git init git add README.md git commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin https://github.com/username/reponame.git git push -u origin main # second commit and upload your package git add . git commit -m \u0026#34;second commit\u0026#34; git push Once your package is on GitHub, you need to create 3 additional files before publishing your package (make sure you also push these files to GitHub):\nsetup.py setup.cfg LICENSE.txt The files are stored outside your package:\nCURRENTFOLDER - package_name - class_1.py - class_2.py - setup.py - setup.cfg - LICENSE.txt For the three files you have created just now, let\u0026rsquo;s see what should we write in the file:\nsetup.py from distutils.core import setup setup( name = \u0026#39;YOURPACKAGENAME\u0026#39;, # How you named your package folder (MyLib) packages = [\u0026#39;YOURPACKAGENAME\u0026#39;], # Chose the same as \u0026#34;name\u0026#34; version = \u0026#39;0.1\u0026#39;, # Start with a small number and increase it with every change you make license=\u0026#39;MIT\u0026#39;, # Chose a license from here: https://help.github.com/articles/licensing-a-repository description = \u0026#39;TYPE YOUR DESCRIPTION HERE\u0026#39;, # Give a short description about your library author = \u0026#39;YOUR NAME\u0026#39;, # Type in your name author_email = \u0026#39;your.email@domain.com\u0026#39;, # Type in your E-Mail url = \u0026#39;https://github.com/user/reponame\u0026#39;, # Provide either the link to your github or to your website download_url = \u0026#39;\u0026#39;, # put empty first keywords = [\u0026#39;SOME\u0026#39;, \u0026#39;MEANINGFULL\u0026#39;, \u0026#39;KEYWORDS\u0026#39;], # Keywords that define your package best install_requires=[ \u0026#34;\u0026#34;, ], # put empty first classifiers=[ \u0026#39;Development Status :: 3 - Alpha\u0026#39;, # Chose either \u0026#34;3 - Alpha\u0026#34;, \u0026#34;4 - Beta\u0026#34; or \u0026#34;5 - Production/Stable\u0026#34; as the current state of your package \u0026#39;Intended Audience :: Developers\u0026#39;, # Define that your audience are developers \u0026#39;Topic :: Software Development :: Build Tools\u0026#39;, \u0026#39;License :: OSI Approved :: MIT License\u0026#39;, # Again, pick a license \u0026#39;Programming Language :: Python :: 3.11\u0026#39;, #Specify which pyhton versions that you want to support ], ) In the above code, you can see that the download_url and the install_requires parameters are still empty. For download_url, you can get it from GitHub the release section, then click create new release:\nYou will be asked to give a tag and release title, you can write tag such as v_0.1 and a title that you want\nThen, once your published the release, you can view your release. As we can see below, two zipped files are created. Now, you can copy the link for the Source code(tar.gz) and paste it on the download_url parameter in the setup.py file. Whenever you have a new version update for your package, do remember to publish a new release on GitHub so that you can update the link on this file.\nFor the install_requires parameter, it means the dependencies your package has or the packages that you are importing when you build your package. You only need to add package that you download through pip, for standard libraries such as os random you can ignore putting them.\n# example install_requires = [ \u0026#39;geopandas\u0026#39;, \u0026#39;rancoord\u0026#39;, ] setup.cfg For this file, you can put in your description file like this:\n[metadata] description-file = README.md LICENSE.txt For License file, you just need to copy and paste the license content where you can refer here.\nTime to Upload It\u0026rsquo;s time to publish your package. On Windows terminal type:\npip install --user --upgrade setuptools wheel python3 setup.py sdist bdist_wheel Then you need to install twine, a Python package for you to upload the package:\npy -m pip install twine After installing, type:\npy -m twine upload dist/* You will be asked to type your username and password. But remember just now you might register your account with two-factor authentication, and you might face error such as insufficient authentication scope. I do recommend using API token in this case for the upload. Go to your Pypi account the Account Setting section and search for API tokens and create a new API token:\nThen you can type your username and password like this and hit Enter:\nusername: __token__ password: PASTETOKENHERE If you face error, you can also try this:\npy -m twine upload -u USERNAME -p PASSWORD dist/* Congrats, and now your package is uploaded to Pypi and you can view it there.\n","permalink":"https://keanteng.github.io/home/docs/2023-09-04-how-to-publish-on-pypi/","summary":"Let\u0026rsquo;s talk about Python package publishing. You have spent hours creating a Python package with some wonderful tools in it","title":"Publish a Package on Pypi"},{"content":" Images from Unsplash\nGoogle Earth Engine is used by scientists, engineers and developers to detect change, map trends and quantify differences on the Earthâs surface.\nIn this article, I will share about the ways that we can obtain information such as elevation, slope and land cover from Earth Engine by telling the services the geo coordinates that we are interested in. All in simple Python code.\nLand Cover, Slope and Elevation\nBefore we get started, make sure you have already installed the Earth Engine library on Python and you also have an Earth Engine account:\npy -m pip install ee To use the Earth Engine API on Python, you also need to perform some authentication, you can run the following command on a Jupyter notebook or Python script:\nimport ee ee.Authenticate() A pop-up box will appear and ask for an API key, after typing the API key you are ready to go.\n1. Elevation The elevation of a geographic location is its height above or below a fixed reference point, most commonly a reference geoid, a mathematical model of the Earthâs Sea level as an equipotential gravitational surface.\nTo obtain elevation data for any particular location, we can use the Shuttle Radar Topography Mission digital elevation data, we can get the data by calling:\nee.Initialize() DEM = ee.Image(\u0026#34;USGS/SRTMGL1_003\u0026#34;) Letâs say I have already prepared a dataset called regression_data containing all the latitude and longitude for a particular location as follows:\nlatitude longtitude 2.234 103.24242 3.435 101.5364 4.345 105.1454 ... ... I will then convert these coordinates into a point with the format (latitude, longtitude). Subsequently, all the points will then be converted into features in list order:\nimport pandas as pd df = pd.read_csv(\u0026#39;regression_data.csv\u0026#39;) nodes = [] for i in range(0, 4000): temp = [df[\u0026#39;longitude\u0026#39;][i], df[\u0026#39;latitude\u0026#39;][i]] nodes.append(temp) # make points from nodes points = [ee.Geometry.Point(coord) for coord in nodes] Notice that there are 4000 coordinates being converted to point.\n# make features from points (name by list order) feats = [ee.Feature(p, {\u0026#39;name\u0026#39;: \u0026#39;node{}\u0026#39;.format(i)}) for i, p in enumerate(points)] # make a featurecollection from points fc = ee.FeatureCollection(feats) Now, we have transformed all the points into a feature. The next step is to extract the points from the digital elevation map.\n# extract points from DEM reducer = ee.Reducer.first() data = DEM.reduceRegions(fc, reducer.setOutputs([\u0026#39;elevation\u0026#39;]), 30) Do note that the data is stored as an Earth Engine object, and we could not view it directly. But here we can use geemap to convert the output into a csv file that contains all the elevation for each of the coordinate:\nimport geemap geemap.ee_to_csv(data, filename=\u0026#39;altitude3.csv\u0026#39;) // output node1 23 node2 50 node3 0 2. Slope Well, how about slope. Slope can be defined as the angle of inclination where it describes the steepness of the groundâs surface, the slope data is also available in the Shuttle Radar Topography Mission digital elevation data that we used to find elevation data earlier.\nIn fact, by just changing the output from elevation to slope:\n# extract points from DEM reducer = ee.Reducer.first() data = DEM.reduceRegions(fc, reducer.setOutputs([\u0026#39;slope\u0026#39;]), 30) We are able to obtain the slope in degree for all our coordinate.\n3. Land Cover Land cover is the physical material at the surface of Earth. Land covers include grass, asphalt, trees, bare ground, water, build area and more.\nFor land cover data, we can use the Copernicus Global Land Cover Layers from Earth Engine:\nlandcover = ee.ImageCollection(\u0026#39;COPERNICUS/Landcover/100m/Proba-V-C3/Global\u0026#39;) What the code will do is to check for any particular coordinates in which type of land does it falls on. From the Copernicus dataset they are more than 20 land types that the coordinates can be classified. You can refer to it here.\nThe steps are similar to what we have done previously:\nlandcover_at_point = landcover.filterBounds(fc).first() reducer = ee.Reducer.first() # Get the land cover class at the given point data = landcover_at_point.reduceRegions(fc, reducer.setOutputs([\u0026#39;discrete_classification\u0026#39;]), 30) Using geemap again, we can export the classified output into a csv file:\ngeemap.ee_to_csv(data, filename=\u0026#39;landcover.csv\u0026#39;) Then we are done. We managed to obtain the elevation, slope and land cover information for 4000 coordinates, all in simple yet powerful code!\n","permalink":"https://keanteng.github.io/home/docs/2023-08-26-land-cover-elevation-and-slope/","summary":"Google Earth Engine is used by scientists, engineers and developers to detect change, map trends and quantify differences on the Earthâs surface.","title":"Land Cover, Elevation \u0026 Slope"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nMany people tend to say that machine learning models are \u0026ldquo;black boxes\u0026rdquo; because they can make good predictions but cannot understand the logic behind those predictions.\nIn this course, we will learn on methods to extract insights from model:\nWhat features in the data the model think as the most important? How each feature affect the prediction? So, why do we need model insights? Model insights can be useful in a couple of ways:\nDebugging\nGiven the frequency and potentially disastrous consequences of bugs, debugging is one of the most valuable skills in data science. Understanding the patterns a model is finding will help you identify when those are at odds with your knowledge of the real world, and this is typically the first step in tracking down bugs.\nInforming Feature Engineering\nFeature engineering is usually the most effective way to improve model accuracy. Feature engineering usually involves repeatedly creating new features using transformations of your raw data or features you have previously created.\nSometimes you can go through this process using nothing but intuition about the underlying topic. But you\u0026rsquo;ll need more direction when you have 100s of raw features or when you lack background knowledge about the topic you are working on.\nDirecting Future Data Collection\nYou have no control over datasets you download online. But many businesses and organizations using data science have opportunities to expand what types of data they collect. Collecting new types of data can be expensive or inconvenient, so they only want to do this if they know it will be worthwhile. Model-based insights give you a good understanding of the value of features you currently have, which will help you reason about what new values may be most helpful.\nInforming Human Decision-Making\nSome decisions are made automatically by models. Amazon doesn\u0026rsquo;t have humans (or elves) scurry to decide what to show you whenever you go to their website. But many important decisions are made by humans. For these decisions, insights can be more valuable than predictions.\nBuilding Trust\nMany people won\u0026rsquo;t assume they can trust your model for important decisions without verifying some basic facts. This is a smart precaution given the frequency of data errors. In practice, showing insights that fit their general understanding of the problem will help build trust, even among people with little deep knowledge of data science.\n1. Permutation Importance Permutation importance is calculated after a model has been fitted. Imagine that now, we want to predict a person\u0026rsquo;s height when they become 20 years old using only data that is available at age 10.\nNow, if we randomly shuffle a single column of the validation data but all the other columns remain in place, how would the accuracy of the prediction be affected?\nOf course, such an approach would reduce the model accuracy, since the data no longer corresponds to what we can observe in the real world. Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions. In this case, shuffling height at age 10 would cause terrible predictions. If we shuffled socks owned instead, the resulting predictions wouldn\u0026rsquo;t suffer nearly as much.\nSo, here is what we can do:\nGet a trained model Shuffle values in a single column and make prediction from it. Calculate the difference of prediction and target value with a loss function. The performance deterioration is the importance of the variable that we shuffled Return to step 2 until all the importance for each column is calculated 1.1 Example Here is how to calculate the importance with eli5 library:\nimport eli5 from eli5.sklearn import PermutationImportance perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y) eli5.show_weights(perm, feature_names = val_X.columns.tolist()) From the above figure, the values towards the top is the most important features. The first number in each row shows how much the model performance decreased with a random shuffling. There is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the Â± measures how performance varied from one-reshuffling to the next.\nYou\u0026rsquo;ll occasionally see negative values for permutation importance. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn\u0026rsquo;t matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance.\n2. Partial Plots Similar to permutation importance, partial dependence plots are calculated after a model has been fit. To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.\nWe will use the fitted model to predict our outcome (probability their player won \u0026ldquo;man of the match\u0026rdquo;). But we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).\n2.1 Example Let\u0026rsquo;s get a decision tree from the model:\nfrom sklearn import tree import graphviz tree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names) graphviz.Source(tree_graph) Produce a partial dependence plot:\nfrom matplotlib import pyplot as plt from sklearn.inspection import PartialDependenceDisplay # Create and plot the data disp1 = PartialDependenceDisplay.from_estimator(tree_model, val_X, [\u0026#39;Goal Scored\u0026#39;]) plt.show() The y-axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning \u0026ldquo;Man of The Match.\u0026rdquo; But extra goals beyond that appear to have little impact on predictions.\nfeature_to_plot = \u0026#39;Distance Covered (Kms)\u0026#39; disp2 = PartialDependenceDisplay.from_estimator(tree_model, val_X, [feature_to_plot]) plt.show() This graph seems too simple to represent reality. But that\u0026rsquo;s because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model\u0026rsquo;s structure.\n# Build Random Forest model rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) disp3 = PartialDependenceDisplay.from_estimator(rf_model, val_X, [feature_to_plot]) plt.show() This model thinks you are more likely to win Man of the Match if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model. Though this dataset is small enough that we would be careful in how we interpret any model.\n2.2 2D Partial Dependence Plots We will use the same datasets as of above:\nfig, ax = plt.subplots(figsize=(8, 6)) f_names = [(\u0026#39;Goal Scored\u0026#39;, \u0026#39;Distance Covered (Kms)\u0026#39;)] # Similar to previous PDP plot except we use tuple of features instead of single feature disp4 = PartialDependenceDisplay.from_estimator(tree_model, val_X, f_names, ax=ax) plt.show() From the plot above, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn\u0026rsquo;t matter. Can you see this by tracing through the decision tree with 0 goals?\nBut distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot.\n3. SHAP Value SHAP or SHapley Additive exPlanations is used to break down a prediction t0 show the impact of each feature. It interprets the impact of having a certain value for a given feature in comparison to the prediction we would make if that feature took some baseline value.\nFor example, consider the Man of the Match award example for previous section, we could ask questions like how much prediction driven by the fact that the team scored 3 goals?\nBut for each team, they are many features, so if we answer for the number of goals, we could repeat the process for other features too. SHAP values of all features sum up to explain why my prediction was different from the baseline.\nsum(SHAP values for all features) = pred_for_team - pred_for_baseline_values To interpret the graph:\nWe predicted 0.7, whereas the base_value is 0.4979. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature\u0026rsquo;s effect. Feature values decreasing the prediction are in blue. The biggest impact comes from Goal Scored being 2. Though the ball possession value has a meaningful effect decreasing the prediction.\nIf you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.\nHow to Do That in Code Let\u0026rsquo;s get the model ready:\nimport numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier data = pd.read_csv(\u0026#39;../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv\u0026#39;) y = (data[\u0026#39;Man of the Match\u0026#39;] == \u0026#34;Yes\u0026#34;) # Convert from string \u0026#34;Yes\u0026#34;/\u0026#34;No\u0026#34; to binary feature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]] X = data[feature_names] train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) We will look for SHAP value for a single row of the dataset. Let\u0026rsquo;s check on the raw prediction first:\nrow_to_show = 5 data_for_prediction = val_X.iloc[row_to_show] # use 1 row of data here. Could use multiple rows if desired data_for_prediction_array = data_for_prediction.values.reshape(1, -1) my_model.predict_proba(data_for_prediction_array) The output is array([[0.29, 0.71]]), the team is 70% likely to have a player win the award\nimport shap # package used to calculate Shap values # Create object that can calculate shap values explainer = shap.TreeExplainer(my_model) # Calculate Shap values shap_values = explainer.shap_values(data_for_prediction) shap.initjs() shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction) The shap_values object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don\u0026rsquo;t win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we\u0026rsquo;ll pull out SHAP values for positive outcomes (pulling out shap_values[1]).\nOf course, SHAP package also has explainers for every type of model:\nshap.DeepExplainer works with Deep Learning models. shap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values. 4. Advanced Uses of SHAP Value Shap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature).\nConsider the equation:\ny = 4 * x1 + 2 * x2 If x1 takes the value 2, instead of a baseline value of 0, then our SHAP value for x1 would be 8 (from 4 times 2).\nThese are harder to calculate with the sophisticated models we use in practice. But through some algorithmic cleverness, Shap values allow us to decompose any prediction into the sum of effects of each feature value, yielding a graph like this:\nIn addition to this nice breakdown for each prediction, the Shap library offers great visualizations of groups of Shap values. We will focus on two of these visualizations. These visualizations have conceptual similarities to permutation importance and partial dependence plots\nSHAP summary plots give us a birds-eye view of feature importance and what is driving it. We\u0026rsquo;ll walk through an example plot for the soccer data:\nThis plot is made of many dots. Each dot has three characteristics:\nVertical location shows what feature it is depicting Color shows whether that feature was high or low for that row of the dataset Horizontal location shows whether the effect of that value caused a higher or lower prediction. Some things you should be able to easily pick out:\nThe model ignored the Red and Yellow \u0026amp; Red features. Usually Yellow Card doesn\u0026rsquo;t affect the prediction, but there is an extreme case where a high value caused a much lower prediction. High values of Goal scored caused higher predictions, and low values caused low predictions How to Do That in Code Get the data and model ready:\nimport numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier data = pd.read_csv(\u0026#39;../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv\u0026#39;) y = (data[\u0026#39;Man of the Match\u0026#39;] == \u0026#34;Yes\u0026#34;) # Convert from string \u0026#34;Yes\u0026#34;/\u0026#34;No\u0026#34; to binary feature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]] X = data[feature_names] train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) Let\u0026rsquo;s get a SHAP summary plot:\nimport shap # package used to calculate Shap values # Create object that can calculate shap values explainer = shap.TreeExplainer(my_model) # calculate shap values. This is what we will plot. # Calculate shap_values for all of val_X rather than a single row, to have more data for plot. shap_values = explainer.shap_values(val_X) # Make plot. Index of [1] is explained in text below. shap.summary_plot(shap_values[1], val_X) The code isn\u0026rsquo;t too complex. But there are a few caveats.\nWhen plotting, we call shap_values[1]. For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, we index in to get the SHAP values for the prediction of \u0026ldquo;True\u0026rdquo;. Calculating SHAP values can be slow. It isn\u0026rsquo;t a problem here, because this dataset is small. But you\u0026rsquo;ll want to be careful when running these to plot with reasonably sized datasets. The exception is when using an xgboost model, which SHAP has some optimizations for and which is thus much faster. SHAP Dependence Contributions Plots For SHAP dependence contribution plots provide a similar insight to partial dependence plot\u0026rsquo;s, but they add a lot more detail.\nEach dot represents a row of the data. The horizontal location is the actual value from the dataset, and the vertical location shows what having that value did to the prediction. The fact this slopes upward says that the more you possess the ball, the higher the model\u0026rsquo;s prediction is for winning the Man of the Match award.\nThe spread suggests that other features must interact with Ball Possession %. For example, here we have highlighted two points with similar ball possession values. That value caused one prediction to increase, and it caused the other prediction to decrease.\nFor comparison, a simple linear regression would produce plots that are perfect lines, without this spread.\nThis suggests we delve into the interactions, and the plots include color coding to help do that. While the primary trend is upward, you can visually inspect whether that varies by dot color.\nThese two points stand out spatially as being far away from the upward trend. They are both colored purple, indicating the team scored one goal. You can interpret this to say In general, having the ball increases a team\u0026rsquo;s chance of having their player win the award. But if they only score one goal, that trend reverses and the award judges may penalize them for having the ball so much if they score that little.\nHow to Do That in Code import shap # package used to calculate Shap values # Create object that can calculate shap values explainer = shap.TreeExplainer(my_model) # calculate shap values. This is what we will plot. shap_values = explainer.shap_values(X) # make plot. shap.dependence_plot(\u0026#39;Ball Possession %\u0026#39;, shap_values[1], X, interaction_index=\u0026#34;Goal Scored\u0026#34;) If you don\u0026rsquo;t supply an argument for interaction_index, Shapley uses some logic to pick one that may be interesting!\n","permalink":"https://keanteng.github.io/home/docs/2023-08-26-ml-explainability/","summary":"Many people tend to say that machine learning models are black boxes because they can make good predictions but cannot understand the logic behind those predictions.","title":"Machine Learning Explainability"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nIn this course, we will learn on how to:\ndetermine which features are the most important with mutual information invent new features in several real-world problem domains encode high-cardinality categoricals with a target encoding create segmentation features with k-means clustering decompose a dataset\u0026rsquo;s variation into features with principal component analysis 1. Introduction The reason we perform feature engineering is we want to make our data more suited to the problem at hand. Consider \u0026ldquo;apparent temperature\u0026rdquo; measures like the heat index and the wind chill. These quantities attempt to measure the perceived temperature to humans based on air temperature, humidity, and wind speed, things which we can measure directly. You could think of an apparent temperature as the result of a kind of feature engineering, an attempt to make the observed data more relevant to what we actually care about: how it actually feels outside!\nFor a feature to be useful, it needs to have a relationship with the target that the model can learn. For example, linear model can only learn linear relationship. So, when using a linear model, your goal is to transform the features to make their relationship to the target linear.\nLet\u0026rsquo;s say we square the Length feature to get Area, however, we create a linear relationship. Adding Area to the feature set means this linear model can now fit a parabola. Squaring a feature, in other words, gave the linear model the ability to fit squared features.\nWe can see that the model fit the are and length feature well\n1.1 Example We will use the concrete dataset for this section. We\u0026rsquo;ll first establish a baseline by training the model on the un-augmented dataset. This will help us determine whether our new features are actually useful.\nEstablishing baselines like this is good practice at the start of the feature engineering process. A baseline score can help you decide whether your new features are worth keeping, or whether you should discard them and possibly try something else.\nX = df.copy() y = X.pop(\u0026#34;CompressiveStrength\u0026#34;) # Train and score baseline model baseline = RandomForestRegressor(criterion=\u0026#34;absolute_error\u0026#34;, random_state=0) baseline_score = cross_val_score( baseline, X, y, cv=5, scoring=\u0026#34;neg_mean_absolute_error\u0026#34; ) baseline_score = -1 * baseline_score.mean() print(f\u0026#34;MAE Baseline Score: {baseline_score:.4}\u0026#34;) If you ever cook at home, you might know that the ratio of ingredients in a recipe is usually a better predictor of how the recipe turns out than their absolute amounts. We might reason then that ratios of the features above would be a good predictor of CompressiveStrength.\nThe cell below adds three new ratio features to the dataset. The output in fact, shows the performance of our model improved:\nX = df.copy() y = X.pop(\u0026#34;CompressiveStrength\u0026#34;) # Create synthetic features X[\u0026#34;FCRatio\u0026#34;] = X[\u0026#34;FineAggregate\u0026#34;] / X[\u0026#34;CoarseAggregate\u0026#34;] X[\u0026#34;AggCmtRatio\u0026#34;] = (X[\u0026#34;CoarseAggregate\u0026#34;] + X[\u0026#34;FineAggregate\u0026#34;]) / X[\u0026#34;Cement\u0026#34;] X[\u0026#34;WtrCmtRatio\u0026#34;] = X[\u0026#34;Water\u0026#34;] / X[\u0026#34;Cement\u0026#34;] # Train and score model on dataset with additional ratio features model = RandomForestRegressor(criterion=\u0026#34;absolute_error\u0026#34;, random_state=0) score = cross_val_score( model, X, y, cv=5, scoring=\u0026#34;neg_mean_absolute_error\u0026#34; ) score = -1 * score.mean() print(f\u0026#34;MAE Score with Ratio Features: {score:.4}\u0026#34;) 2. Mutual Information Let\u0026rsquo;s say you encounter a dataset with hundreds or even thousands of features, it can be overwhelming to think of where should we choose to begin our study. A great option that we can choose is to construct a ranking with feature utility metric - to measure the associations between a feature and a target. Then, we can choose a smaller set of the most useful features to develop our initial model.\nSuch metric is known as mutual information - it is a lot like correlation that measures the relationship between two quantities. The good thing is mutual information can detect any kind of relationship, but for correlation, it is only for linear relationship.\nMutual information describes relationship in terms of uncertainty. For two quantities, it is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If we know the value of a feature, how much more confident can we get about the target.\nFrom the above image, it seems that knowing the value of ExterQual should make you more certain about the corresponding SalePrice \u0026ndash; each category of ExterQual tends to concentrate SalePrice to within a certain range. The mutual information that ExterQual has with SalePrice is the average reduction of uncertainty in SalePrice taken over the four values of ExterQual. Since Fair occurs less often than Typical, for instance, Fair gets less weight in the MI score.\nThe least possible mutual information between quantities is 0.0. When MI is zero, the quantities are independent: neither can tell you anything about the other. Conversely, in theory there\u0026rsquo;s no upper bound to what MI can be. In practice though values above 2.0 or so are uncommon\nNote:\nMI can help you to understand the relative potential of a feature as a predictor of the target, considered by itself. It\u0026rsquo;s possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can\u0026rsquo;t detect interactions between features. It is a univariate metric. The actual usefulness of a feature depends on the model you use it with. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn\u0026rsquo;t mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association. 2.1 Example Now we have an automobile dataset with a few features related to cars. Here\u0026rsquo;s how we can compute the MI score for the dataset:\nThe scikit-learn algorithm for MI treats discrete features differently from continuous features. Consequently, you need to tell it which are which. As a rule of thumb, anything that must have a float dtype is not discrete. Categoricals (object or categorial dtype) can be treated as discrete by giving them a label encoding.\nX = df.copy() y = X.pop(\u0026#34;price\u0026#34;) # Label encoding for categoricals for colname in X.select_dtypes(\u0026#34;object\u0026#34;): X[colname], _ = X[colname].factorize() # All discrete features should now have integer dtypes (double-check this before using MI!) discrete_features = X.dtypes == int from sklearn.feature_selection import mutual_info_regression def make_mi_scores(X, y, discrete_features): mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features) mi_scores = pd.Series(mi_scores, name=\u0026#34;MI Scores\u0026#34;, index=X.columns) mi_scores = mi_scores.sort_values(ascending=False) return mi_scores mi_scores = make_mi_scores(X, y, discrete_features) mi_scores[::3] # show a few features with their MI scores curb_weight 1.540126 highway_mpg 0.951700 length 0.621566 fuel_system 0.485085 stroke 0.389321 num_of_cylinders 0.330988 compression_ratio 0.133927 fuel_type 0.048139 Name: MI Scores, dtype: float64 Let\u0026rsquo;s visualize the above output with a bar plot:\ndef plot_mi_scores(scores): scores = scores.sort_values(ascending=True) width = np.arange(len(scores)) ticks = list(scores.index) plt.barh(width, scores) plt.yticks(width, ticks) plt.title(\u0026#34;Mutual Information Scores\u0026#34;) plt.figure(dpi=100, figsize=(8, 5)) plot_mi_scores(mi_scores) W\nAs we might expect, the high-scoring curb_weight feature exhibits a strong relationship with price, the target.\nsns.relplot(x=\u0026#34;curb_weight\u0026#34;, y=\u0026#34;price\u0026#34;, data=df); The fuel_type feature has a fairly low MI score, but as we can see from the figure, it clearly separates two price populations with different trends within the horsepower feature. This indicates that fuel_type contributes an interaction effect and might not be unimportant after all. Before deciding a feature is unimportant from its MI score, it\u0026rsquo;s good to investigate any possible interaction effects \u0026ndash; domain knowledge can offer a lot of guidance here.\nsns.lmplot(x=\u0026#34;horsepower\u0026#34;, y=\u0026#34;price\u0026#34;, hue=\u0026#34;fuel_type\u0026#34;, data=df); 3. Creating Features In the Automobile dataset are features describing a car\u0026rsquo;s engine. Research yields a variety of formulas for creating potentially useful new features. The \u0026ldquo;stroke ratio\u0026rdquo;, for instance, is a measure of how efficient an engine is versus how performant:\nautos[\u0026#34;stroke_ratio\u0026#34;] = autos.stroke / autos.bore autos[[\u0026#34;stroke\u0026#34;, \u0026#34;bore\u0026#34;, \u0026#34;stroke_ratio\u0026#34;]].head() The more complicated a combination is, the more difficult it will be for a model to learn, like this formula for an engine\u0026rsquo;s \u0026ldquo;displacement\u0026rdquo;, a measure of its power:\nautos[\u0026#34;displacement\u0026#34;] = ( np.pi * ((0.5 * autos.bore) ** 2) * autos.stroke * autos.num_of_cylinders ) We can use data visualization to get an idea on how to perform transformation (using powers or logarithms). For example, the distribution of WindSpeed in US Accidents is highly skewed, we can perform a log-transformation:\n# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log accidents[\u0026#34;LogWindSpeed\u0026#34;] = accidents.WindSpeed.apply(np.log1p) # Plot a comparison fig, axs = plt.subplots(1, 2, figsize=(8, 4)) sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0]) sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]); 3.1 Counts When the have features describing the presence or absence of something, they often come in sets, we can aggregate them by creating a count:\nIn Traffic Accidents are several features indicating whether some roadway object was near the accident. This will create a count of the total number of roadway features nearby using the sum method:\nroadway_features = [\u0026#34;Amenity\u0026#34;, \u0026#34;Bump\u0026#34;, \u0026#34;Crossing\u0026#34;, \u0026#34;GiveWay\u0026#34;, \u0026#34;Junction\u0026#34;, \u0026#34;NoExit\u0026#34;, \u0026#34;Railway\u0026#34;, \u0026#34;Roundabout\u0026#34;, \u0026#34;Station\u0026#34;, \u0026#34;Stop\u0026#34;, \u0026#34;TrafficCalming\u0026#34;, \u0026#34;TrafficSignal\u0026#34;] accidents[\u0026#34;RoadwayFeatures\u0026#34;] = accidents[roadway_features].sum(axis=1) accidents[roadway_features + [\u0026#34;RoadwayFeatures\u0026#34;]].head(10) In the Concrete dataset are the amounts of components in a concrete formulation. Many formulations lack one or more components (that is, the component has a value of 0). This will count how many components are in a formulation with the dataframe\u0026rsquo;s built-in greater-than gt method:\ncomponents = [ \u0026#34;Cement\u0026#34;, \u0026#34;BlastFurnaceSlag\u0026#34;, \u0026#34;FlyAsh\u0026#34;, \u0026#34;Water\u0026#34;, \u0026#34;Superplasticizer\u0026#34;, \u0026#34;CoarseAggregate\u0026#34;, \u0026#34;FineAggregate\u0026#34;] concrete[\u0026#34;Components\u0026#34;] = concrete[components].gt(0).sum(axis=1) concrete[components + [\u0026#34;Components\u0026#34;]].head(10) 3.2 Building-Up \u0026amp; Breaking Down Features Often you\u0026rsquo;ll have complex strings that can usefully be broken into simpler pieces. Some common examples:\nID numbers: \u0026#39;123-45-6789\u0026#39; Phone numbers: \u0026#39;(999) 555-0123\u0026#39; Features like these will often have some kind of structure that you can make use of. US phone numbers, for instance, have an area code (the \u0026lsquo;(999)\u0026rsquo; part) that tells you the location of the caller\nstr accessor lets you apply string methods like split directly to columns. The Customer Lifetime Value dataset contains features describing customers of an insurance company. From the Policy feature, we could separate the Type from the Level of coverage:\ncustomer[[\u0026#34;Type\u0026#34;, \u0026#34;Level\u0026#34;]] = ( # Create two new features customer[\u0026#34;Policy\u0026#34;] # from the Policy feature .str # through the string accessor .split(\u0026#34; \u0026#34;, expand=True) # by splitting on \u0026#34; \u0026#34; # and expanding the result into separate columns ) customer[[\u0026#34;Policy\u0026#34;, \u0026#34;Type\u0026#34;, \u0026#34;Level\u0026#34;]].head(10) Of course, we can join simple features into a composed feature if there was some interaction in the combination:\nautos[\u0026#34;make_and_style\u0026#34;] = autos[\u0026#34;make\u0026#34;] + \u0026#34;_\u0026#34; + autos[\u0026#34;body_style\u0026#34;] autos[[\u0026#34;make\u0026#34;, \u0026#34;body_style\u0026#34;, \u0026#34;make_and_style\u0026#34;]].head() 3.3 Group Transform Group transform aggregate infromation across multiple rows grouped by some category. We can create features like \u0026ldquo;the average income of a person\u0026rsquo;s state of residence,\u0026rdquo; or \u0026ldquo;the proportion of movies released on a weekday, by genre.\u0026rdquo;.\nUsing an aggregation function, a group transform combines two features: a categorical feature that provides the grouping and another feature whose values you wish to aggregate. For an \u0026ldquo;average income by state\u0026rdquo;, you would choose State for the grouping feature, mean for the aggregation function, and Income for the aggregated feature. To compute this in Pandas, we use the groupby and transform methods:\ncustomer[\u0026#34;AverageIncome\u0026#34;] = ( customer.groupby(\u0026#34;State\u0026#34;) # for each state [\u0026#34;Income\u0026#34;] # select the income .transform(\u0026#34;mean\u0026#34;) # and compute its mean ) customer[[\u0026#34;State\u0026#34;, \u0026#34;Income\u0026#34;, \u0026#34;AverageIncome\u0026#34;]].head(10) Here\u0026rsquo;s a function to create a DataFrame that calculate the frequency with which each state occurs in the dataset:\ncustomer[\u0026#34;StateFreq\u0026#34;] = ( customer.groupby(\u0026#34;State\u0026#34;) [\u0026#34;State\u0026#34;] .transform(\u0026#34;count\u0026#34;) / customer.State.count() ) customer[[\u0026#34;State\u0026#34;, \u0026#34;StateFreq\u0026#34;]].head(10) If you\u0026rsquo;re using training and validation splits, to preserve their independence, it\u0026rsquo;s best to create a grouped feature using only the training set and then join it to the validation set. We can use the validation set\u0026rsquo;s merge method after creating a unique set of values with drop_duplicates on the training set:\n# Create splits df_train = customer.sample(frac=0.5) df_valid = customer.drop(df_train.index) # Create the average claim amount by coverage type, on the training set df_train[\u0026#34;AverageClaim\u0026#34;] = df_train.groupby(\u0026#34;Coverage\u0026#34;)[\u0026#34;ClaimAmount\u0026#34;].transform(\u0026#34;mean\u0026#34;) # Merge the values into the validation set df_valid = df_valid.merge( df_train[[\u0026#34;Coverage\u0026#34;, \u0026#34;AverageClaim\u0026#34;]].drop_duplicates(), on=\u0026#34;Coverage\u0026#34;, how=\u0026#34;left\u0026#34;, ) df_valid[[\u0026#34;Coverage\u0026#34;, \u0026#34;AverageClaim\u0026#34;]].head(10) 4. Clustering with K-means Clustering means the assigning of data points to group based on how similar the points are to each other. In feature engineering, we attempt to discover groups of customers representing a market segment or geographic area that share similar weather patterns. By adding a feature of cluster labels, it helps machine learning models untangle complicated relationships of space and proximity.\nCluster is a categorical variable. The motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It\u0026rsquo;s a \u0026ldquo;divide and conquer\u0026rdquo; strategy.\nClustering the YearBuilt feature helps this linear model learn its relationship to SalePrice.\nThe figure shows how clustering can improve a simple linear model. The curved relationship between the YearBuilt and SalePrice is too complicated for this kind of model \u0026ndash; it underfits. On smaller chunks however the relationship is almost linear, and that the model can learn easily.\n4.1 k-Means Clustering K-means clustering measures similarity using ordinary straight-line distance (Euclidean distance, in other words). It creates clusters by placing a number of points, called centroids, inside the feature-space. Each point in the dataset is assigned to the cluster of whichever centroid it\u0026rsquo;s closest to. The \u0026ldquo;k\u0026rdquo; in \u0026ldquo;k-means\u0026rdquo; is how many centroids (that is, clusters) it creates. You define the k yourself.\nYou could imagine each centroid capturing points through a sequence of radiating circles. When sets of circles from competing centroids overlap they form a line. The result is what\u0026rsquo;s called a Voronoi tessallation. The tessallation shows you to what clusters future data will be assigned; the tessallation is essentially what k-means learns from its training data.\nThe k-means algorithm:\nRandomly initialize some predefined number of centroids Assign points to the nearest cluster centroid Move each centroid to minimize the distance to its point Iterate from second step until centroid not moving anymore 4.2 Example As spatial features, California Housing\u0026rsquo;s \u0026lsquo;Latitude\u0026rsquo; and \u0026lsquo;Longitude\u0026rsquo; make natural candidates for k-means clustering. Since k-means clustering is sensitive to scale, it can be a good idea rescale or normalize data with extreme values. Our features are already roughly on the same scale, so we\u0026rsquo;ll leave them as-is.\n# Create cluster feature kmeans = KMeans(n_clusters=6) X[\u0026#34;Cluster\u0026#34;] = kmeans.fit_predict(X) X[\u0026#34;Cluster\u0026#34;] = X[\u0026#34;Cluster\u0026#34;].astype(\u0026#34;category\u0026#34;) X.head() Let\u0026rsquo;s see the cluster on a plot:\nsns.relplot( x=\u0026#34;Longitude\u0026#34;, y=\u0026#34;Latitude\u0026#34;, hue=\u0026#34;Cluster\u0026#34;, data=X, height=6, ); Let\u0026rsquo;s compare the distributions of the target within each cluster using box-plot.\nIf the clustering is informative, these distributions should, for the most part, separate across MedHouseVal, which is indeed what we see.\nX[\u0026#34;MedHouseVal\u0026#34;] = df[\u0026#34;MedHouseVal\u0026#34;] sns.catplot(x=\u0026#34;MedHouseVal\u0026#34;, y=\u0026#34;Cluster\u0026#34;, data=X, kind=\u0026#34;boxen\u0026#34;, height=6); 5. Principal Component Analysis (PCA) PCA is typically applied to standardized data. With standardized data \u0026ldquo;variation\u0026rdquo; means \u0026ldquo;correlation\u0026rdquo;. With unstandardized data \u0026ldquo;variation\u0026rdquo; means \u0026ldquo;covariance\u0026rdquo;. All data in this section will be standardized before applying PCA.\nIn the Abalone dataset are physical measurements taken from several thousand Tasmanian abalone. (An abalone is a sea creature much like a clam or an oyster.) We\u0026rsquo;ll just look at a couple features for now: the \u0026lsquo;Height\u0026rsquo; and \u0026lsquo;Diameter\u0026rsquo; of their shells.\nWe can think that in this data, there are axes of variation that describe the ways the abalone tend to differ from one to another. We can give names to these axes of variation. The longer axis we might call the \u0026ldquo;Size\u0026rdquo; component: small height and small diameter (lower left) contrasted with large height and large diameter (upper right). The shorter axis we might call the \u0026ldquo;Shape\u0026rdquo; component: small height and large diameter (flat shape) contrasted with large height and small diameter (round shape).\nOf course, we can also describe the abalone with size and shape. The whole idea of PCA: instead of describing the data with the original features, we describe it with its axes of variation. The axes of variation become the new features.\nIn fact these new features are actually just linear combinations of the original features:\ndf[\u0026#34;Size\u0026#34;] = 0.707 * X[\u0026#34;Height\u0026#34;] + 0.707 * X[\u0026#34;Diameter\u0026#34;] df[\u0026#34;Shape\u0026#34;] = 0.707 * X[\u0026#34;Height\u0026#34;] - 0.707 * X[\u0026#34;Diameter\u0026#34;] The size and shape features are known as the principal components of the data. The weights are called loadings. There will be as many principal components as there are features in the original dataset: if we had used ten features instead of two, we would have ended up with ten components.\nMoreover, PCA also tells us the amount of variation in each component:\nThe Size component captures the majority of the variation between Height and Diameter. It\u0026rsquo;s important to remember, however, that the amount of variance in a component doesn\u0026rsquo;t necessarily correspond to how good it is as a predictor: it depends on what you\u0026rsquo;re trying to predict.\n5.1 PCA for Feature Engineering We can use PCA for feature engineering in two ways. First, we can use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create \u0026ndash; a product of \u0026lsquo;Height\u0026rsquo; and \u0026lsquo;Diameter\u0026rsquo; if \u0026lsquo;Size\u0026rsquo; is important, say, or a ratio of \u0026lsquo;Height\u0026rsquo; and \u0026lsquo;Diameter\u0026rsquo; if Shape is important. You could even try clustering on one or more of the high-scoring components.\nOn the other hand, we can use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:\nDimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information. Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task. Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio. Decorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with. 5.2 Example We will use the Automobile dataset from previous study and apply PCA to discover some features from the dataset:\nfeatures = [\u0026#34;highway_mpg\u0026#34;, \u0026#34;engine_size\u0026#34;, \u0026#34;horsepower\u0026#34;, \u0026#34;curb_weight\u0026#34;] X = df.copy() y = X.pop(\u0026#39;price\u0026#39;) X = X.loc[:, features] # Standardize X_scaled = (X - X.mean(axis=0)) / X.std(axis=0) Now we can fit scikit-learn\u0026rsquo;s PCA estimator and create the principal components. You can see here the first few rows of the transformed dataset.\nfrom sklearn.decomposition import PCA # Create principal components pca = PCA() X_pca = pca.fit_transform(X_scaled) # Convert to dataframe component_names = [f\u0026#34;PC{i+1}\u0026#34; for i in range(X_pca.shape[1])] X_pca = pd.DataFrame(X_pca, columns=component_names) X_pca.head() After fitting, the PCA instance contains the loadings in its components_ attribute. (Terminology for PCA is inconsistent, unfortunately. We\u0026rsquo;re following the convention that calls the transformed columns in X_pca the components, which otherwise don\u0026rsquo;t have a name.) We\u0026rsquo;ll wrap the loadings up in a dataframe.\nloadings = pd.DataFrame( pca.components_.T, # transpose the matrix of loadings columns=component_names, # so the columns are the principal components index=X.columns, # and the rows are the original features ) loadings PC1\tPC2\tPC3\tPC4 highway_mpg\t-0.492347\t0.770892\t0.070142\t-0.397996 engine_size\t0.503859\t0.626709\t0.019960\t0.594107 horsepower\t0.500448\t0.013788\t0.731093\t-0.463534 curb_weight\t0.503262\t0.113008\t-0.678369\t-0.523232 Recall that the signs and magnitudes of a component\u0026rsquo;s loadings tell us what kind of variation it\u0026rsquo;s captured. The first component (PC1) shows a contrast between large, powerful vehicles with poor gas milage, and smaller, more economical vehicles with good gas milage. We might call this the \u0026ldquo;Luxury/Economy\u0026rdquo; axis. The next figure shows that our four chosen features mostly vary along the Luxury/Economy axis.\n# Look at explained variance plot_variance(pca); Let\u0026rsquo;s check the MI score of the components.\nmi_scores = make_mi_scores(X_pca, y, discrete_features=False) mi_scores PC1 1.013264 PC2 0.379156 PC3 0.306703 PC4 0.203329 Name: MI Scores, dtype: float64 PC1 is highly informative, though the remaining components, despite their small variance, still have a significant relationship with price. Examining those components could be worthwhile to find relationships not captured by the main Luxury/Economy axis.\nPC3 shows a contrast between horsepower and curb_weight \u0026ndash; sports cars vs. wagons, it seems.\n# Show dataframe sorted by PC3 idx = X_pca[\u0026#34;PC3\u0026#34;].sort_values(ascending=False).index cols = [\u0026#34;make\u0026#34;, \u0026#34;body_style\u0026#34;, \u0026#34;horsepower\u0026#34;, \u0026#34;curb_weight\u0026#34;] df.loc[idx, cols] make\tbody_style\thorsepower\tcurb_weight 118\tporsche\thardtop\t207\t2756 117\tporsche\thardtop\t207\t2756 119\tporsche\tconvertible\t207\t2800 45\tjaguar\tsedan\t262\t3950 96\tnissan\thatchback\t200\t3139 We will create a new ratio features from this:\ndf[\u0026#34;sports_or_wagon\u0026#34;] = X.curb_weight / X.horsepower sns.regplot(x=\u0026#34;sports_or_wagon\u0026#34;, y=\u0026#39;price\u0026#39;, data=df, order=2); 6. Target Encoding A target encoding is any kind of encoding that replaces a feature\u0026rsquo;s categories with some number derived from the target.\nautos[\u0026#34;make_encoded\u0026#34;] = autos.groupby(\u0026#34;make\u0026#34;)[\u0026#34;price\u0026#34;].transform(\u0026#34;mean\u0026#34;) autos[[\u0026#34;make\u0026#34;, \u0026#34;price\u0026#34;, \u0026#34;make_encoded\u0026#34;]].head(10) From the above code, we are performing a mean encoding to the dataset. We can do this to a binary dataset as well - and we call it as bin counting.\nTarget encoding for the above example presents a couple of problems, however. First are unknown categories. Target encodings create a special risk of overfitting, which means they need to be trained on an independent \u0026ldquo;encoding\u0026rdquo; split. When you join the encoding to future splits, Pandas will fill in missing values for any categories not present in the encoding split. These missing values you would have to impute somehow.\nMoreover, for rare categories. When a category only occurs a few times in the dataset, any statistics calculated on its group are unlikely to be very accurate. In the Automobiles dataset, the mercury make only occurs once. The \u0026ldquo;mean\u0026rdquo; price we calculated is just the price of that one vehicle, which might not be very representative of any Mercuries we might see in the future. Target encoding rare categories can make overfitting more likely.\nTo avoid the above issues, we need to apply smoothing. That is, to blend in-category average with the overall average.\nencoding = weight * in_category + (1 - weight) * overall So how do we compute for the weight? We can do it by computing the m-estimate.\nweight = n / (n + m) n is the total number of times that category occurs in the data. The parameter m determines the \u0026ldquo;smoothing factor\u0026rdquo;. Larger values of m put more weight on the overall estimate.\nLet\u0026rsquo;s say in the Automobile dataset and there are 3 cars with the make of chevrolet. For m = 2, chevrolet category would be encoded with 60% of the average Chevrelot price and 40% of the overall average price.\nWhen choosing a value for m, consider how noisy you expect the categories to be. Does the price of a vehicle vary a great deal within each make? Would you need a lot of data to get good estimates? If so, it could be better to choose a larger value for m; if the average price for each make were relatively stable, a smaller value could be okay.\nBenefits of Target Encoding:\nHigh-cardinality features: A feature with many categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature\u0026rsquo;s most important property: its relationship with the target. Domain-motivated features: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature\u0026rsquo;s true informativeness. 6.1 Example In this final example, we will be using MovieLens1M dataset with one-million movie rating by users of the MovieLens website. With over 3000 categories, the Zipcode feature makes a good candidate for target encoding, and the size of this dataset (over one-million rows) means we can spare some data to create the encoding.\n# 25% split to train the encoder X = df.copy() y = X.pop(\u0026#39;Rating\u0026#39;) X_encode = X.sample(frac=0.25) y_encode = y[X_encode.index] X_pretrain = X.drop(X_encode.index) y_train = y[X_pretrain.index] from category_encoders import MEstimateEncoder # Create the encoder instance. Choose m to control noise. encoder = MEstimateEncoder(cols=[\u0026#34;Zipcode\u0026#34;], m=5.0) # Fit the encoder on the encoding split. encoder.fit(X_encode, y_encode) # Encode the Zipcode column to create the final training data X_train = encoder.transform(X_pretrain) Now we want to compare the encoded values to the target to see how informative it is:\nplt.figure(dpi=90) ax = sns.distplot(y, kde=False, norm_hist=True) ax = sns.kdeplot(X_train.Zipcode, color=\u0026#39;r\u0026#39;, ax=ax) ax.set_xlabel(\u0026#34;Rating\u0026#34;) ax.legend(labels=[\u0026#39;Zipcode\u0026#39;, \u0026#39;Rating\u0026#39;]); We can see that the distribution of the encoded Zipcode feature roughly follows the distribution of the actual ratings, meaning that movie-watchers differed enough in their ratings from zipcode to zipcode that our target encoding was able to capture useful information.\n","permalink":"https://keanteng.github.io/home/docs/2023-08-20-feature-engineering/","summary":"Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nIn this course, we will learn on how to:\ndetermine which features are the most important with mutual information invent new features in several real-world problem domains encode high-cardinality categoricals with a target encoding create segmentation features with k-means clustering decompose a dataset\u0026rsquo;s variation into features with principal component analysis 1. Introduction The reason we perform feature engineering is we want to make our data more suited to the problem at hand.","title":"Feature Engineering"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nData cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. It is a key part of data science, and it can be deeply frustrating. What should we do to the missing values? Why the dates are not in the correct format? How to clean up inconsistent data entry? These are some of the problems that we will learn to tackle in this course.\n1. Handling Missing Values Let\u0026rsquo;s start by taking a look at the data and check how many missing values are there:\ndf.head() missing = df.isnull().sum() print(missing) Now let\u0026rsquo;s look at the percentage of values in our dataset that were missing:\ntotal_cells = np.product(df.shape) total_missing = missing.sum() percent_missing = (total_missing/total_cells)*100 print(percent_missing) The next step is to figure out why the data is missing. It is important to ask ourselves, is this value missing because it wasn\u0026rsquo;t recorded, or it doesn\u0026rsquo;t exist? If it is the latter, it wouldn\u0026rsquo;t make sense to guess those values, we could just leave them empty. But if it is the other way round, we probably would need to guess what it might have been based on some other values in the column and row.\nWe called this method as imputation. In statistics, imputation is the process of replacing missing data with substituted values.\n# look at the # of missing points in the first ten columns missing_values_count[0:10] Date 0 GameID 0 Drive 0 qtr 0 down 61154 time 224 TimeUnder 0 TimeSecs 224 PlayTimeDiff 444 SideofField 528 dtype: int64 We can see that the columns has information on the number of seconds left in the game when the play was made. These values are probably missing because they were not recorded, it would be better for us to try and have a guess on it.\nConversely, other fields such as PenalizedTeam contains a lot of missing fields. This is because if there was no penalty, then the field will be missed. We could just leave it empty as it is.\nOf course, if we wouldn\u0026rsquo;t want to guess for the missing value, we can remove the rows of columns with missing values:\ndf.dropna() # remove rows with missing value df.dropna(axis = 1) # remove all columns with at least one missing value On the other hand, if we would like to try to fill in the missing value, we can use the Pandas\u0026rsquo; fillna function:\n# get a small subset of the NFL dataset subset_nfl_data = nfl_data.loc[:, \u0026#39;EPA\u0026#39;:\u0026#39;Season\u0026#39;].head() subset_nfl_data # replace all NA\u0026#39;s with 0 subset_nfl_data.fillna(0) If we want to replace the missing value with value comes directly after it in the same column:\n# replace all NA\u0026#39;s the value that comes directly after it in the same column, # then replace all the remaining na\u0026#39;s with 0 subset_nfl_data.fillna(method=\u0026#39;bfill\u0026#39;, axis=0).fillna(0) 2. Scaling \u0026amp; Normalization Let\u0026rsquo;s start by importing some libraries:\n# modules we\u0026#39;ll use import pandas as pd import numpy as np # for Box-Cox Transformation from scipy import stats # for min_max scaling from mlxtend.preprocessing import minmax_scaling # plotting modules import seaborn as sns import matplotlib.pyplot as plt The difference between scaling and normalization:\nScaling: changing the range of data Normalization: changing the shape of the distribution of the data When we perform scaling, we are transforming the data so that it fits within a specific scale like 0 to 1 or 1 to 100. We will scale data if we are using methods based on measures of how far apart data points are such as SVM (support vector machines) and KNN (k-nearest neighbors).\nFor example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don\u0026rsquo;t scale your prices, methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn\u0026rsquo;t fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you\u0026rsquo;re looking at something like height and weight? It\u0026rsquo;s not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\nScaling variables help us to compare different variables on equal footing. The shape of the data will not change with scaling, but the range will change.\n# generate 1000 data points randomly drawn from an exponential distribution original_data = np.random.exponential(size=1000) # mix-max scale the data between 0 and 1 scaled_data = minmax_scaling(original_data, columns=[0]) # plot both together to compare fig, ax = plt.subplots(1, 2, figsize=(15, 3)) sns.histplot(original_data, ax=ax[0], kde=True, legend=False) ax[0].set_title(\u0026#34;Original Data\u0026#34;) sns.histplot(scaled_data, ax=ax[1], kde=True, legend=False) ax[1].set_title(\u0026#34;Scaled data\u0026#34;) plt.show() Scaling Example\nFurthermore, for normalization it represents a more radical transformation. We perform normalization because our observations cannot be described as a normal distribution. Normally, we perform normalization when the data will be used for machine learning or statistical technique that assumes the data is normally distributed.\n# normalize the exponential data with boxcox normalized_data = stats.boxcox(original_data) # plot both together to compare fig, ax=plt.subplots(1, 2, figsize=(15, 3)) sns.histplot(original_data, ax=ax[0], kde=True, legend=False) ax[0].set_title(\u0026#34;Original Data\u0026#34;) sns.histplot(normalized_data[0], ax=ax[1], kde=True, legend=False) ax[1].set_title(\u0026#34;Normalized data\u0026#34;) plt.show() Scaling Example\n3. Parsing Date Importing library:\n# modules we\u0026#39;ll use import pandas as pd import numpy as np import seaborn as sns import datetime Let\u0026rsquo;s output the data column from the DataFrame to see if it really contains date:\n# print the first few rows of the date column print(landslides[\u0026#39;date\u0026#39;].head()) 0 3/2/07 1 3/22/07 2 4/6/07 3 4/14/07 4 4/15/07 Name: date, dtype: object By observation, these outputs look like date. But the data type is being set to object. That means this date column is not being recognized as a date, and we need to convert it so that it can be recognized as date.\nlandslide[\u0026#39;data_parsed\u0026#39;] = pd.to_datetime(landslide[\u0026#39;date\u0026#39;], format = \u0026#34;%m/%d/%y\u0026#34;) If we check back again:\n# print the first few rows landslides[\u0026#39;date_parsed\u0026#39;].head() 0 2007-03-02 1 2007-03-22 2 2007-04-06 3 2007-04-14 4 2007-04-15 Name: date_parsed, dtype: datetime64[ns] We can see that now the date is being correctly recognized. Here\u0026rsquo;s some tip, if we run into an error with multiple date formats, we could run: landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True).\n3,.1 Select The Day of The Month dom = landslide[\u0026#39;date_parsed\u0026#39;].dt.day dom.head() One of the biggest dangers in parsing dates is mixing up the months and days. The to_datetime() function does have very helpful error messages, but it doesn\u0026rsquo;t hurt to double-check that the days of the month we\u0026rsquo;ve extracted make sense.\n# remove na\u0026#39;s day_of_month_landslides = day_of_month_landslides.dropna() # plot the day of the month sns.distplot(day_of_month_landslides, kde=False, bins=31) Scaling Example\nLooks like the date is parsed correctly.\n4. Character Encoding Import libraries:\n# modules we\u0026#39;ll use import pandas as pd import numpy as np # helpful character encoding module import charset_normalizer # set seed for reproducibility np.random.seed(0) Character encoding are specific sets of rules for mapping from raw binary byte strings to character that make-up human-readable text. If you tried to read in text with a different encoding than the one it was originally written in, you ended up with scrambled text called mojibake (said like mo-gee-bah-kay). Here\u0026rsquo;s an example of mojibake:\nÃ¦ââ¡Ã¥âÃ¥ÅâÃ£?? In fact, character encoding mismatches are less common today. There are a lot of different character encoding out there, the main one for you to know is UTF-8.\nUTF-8 is the standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It\u0026rsquo;s when things aren\u0026rsquo;t in UTF-8 that you run into trouble.\n# start with a string before = \u0026#34;This is the euro symbol: â¬\u0026#34; # check to see what datatype it is type(before) # output # str # encode it to a different encoding, replacing characters that raise errors after = before.encode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) # check the type type(after) # output # bytes # take a look at what the bytes look like after # output # b\u0026#39;This is the euro symbol: \\xe2\\x82\\xac\u0026#39; # convert it back to utf-8 print(after.decode(\u0026#34;utf-8\u0026#34;)) # output # This is the euro symbol: â¬ However, when we try to use a different encoding to map our bytes into a string, we get an error. This is because the encoding we\u0026rsquo;re trying to use doesn\u0026rsquo;t know what to do with the bytes we\u0026rsquo;re trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in.\n5. Inconsistent Data Entry Import libraries:\n# modules we\u0026#39;ll use import pandas as pd import numpy as np # helpful modules import fuzzywuzzy from fuzzywuzzy import process import charset_normalizer In this section, we would like to check the \u0026lsquo;country\u0026rsquo; column to make sure that there is no data entry inconsistencies in it.\n# get all the unique values in the \u0026#39;Country\u0026#39; column countries = professors[\u0026#39;Country\u0026#39;].unique() # sort them alphabetically and then take a closer look countries.sort() countries array([\u0026#39; Germany\u0026#39;, \u0026#39; New Zealand\u0026#39;, \u0026#39; Sweden\u0026#39;, \u0026#39; USA\u0026#39;, \u0026#39;Australia\u0026#39;, \u0026#39;Austria\u0026#39;, \u0026#39;Canada\u0026#39;, \u0026#39;China\u0026#39;, \u0026#39;Finland\u0026#39;, \u0026#39;France\u0026#39;, \u0026#39;Greece\u0026#39;, \u0026#39;HongKong\u0026#39;, \u0026#39;Ireland\u0026#39;, \u0026#39;Italy\u0026#39;, \u0026#39;Japan\u0026#39;, \u0026#39;Macau\u0026#39;, \u0026#39;Malaysia\u0026#39;, \u0026#39;Mauritius\u0026#39;, \u0026#39;Netherland\u0026#39;, \u0026#39;New Zealand\u0026#39;, \u0026#39;Norway\u0026#39;, \u0026#39;Pakistan\u0026#39;, \u0026#39;Portugal\u0026#39;, \u0026#39;Russian Federation\u0026#39;, \u0026#39;Saudi Arabia\u0026#39;, \u0026#39;Scotland\u0026#39;, \u0026#39;Singapore\u0026#39;, \u0026#39;South Korea\u0026#39;, \u0026#39;SouthKorea\u0026#39;, \u0026#39;Spain\u0026#39;, \u0026#39;Sweden\u0026#39;, \u0026#39;Thailand\u0026#39;, \u0026#39;Turkey\u0026#39;, \u0026#39;UK\u0026#39;, \u0026#39;USA\u0026#39;, \u0026#39;USofA\u0026#39;, \u0026#39;Urbana\u0026#39;, \u0026#39;germany\u0026#39;], dtype=object) We notice that there are some problems with this array of data. For instance, \u0026lsquo;germany\u0026rsquo; and \u0026lsquo;Germany\u0026rsquo;; \u0026lsquo;New Zealand\u0026rsquo; and \u0026rsquo; New Zealand\u0026rsquo;. Firstly, we would convert everything to the lower case and remove the white spaces at the beginning and the end:\ndf[\u0026#39;Country\u0026#39;] = df[\u0026#39;Country\u0026#39;].str.lower() df[\u0026#39;Country\u0026#39;] = df[\u0026#39;Country\u0026#39;].str.strip() Next, we want to perform fuzzy matching to correct inconsistent data entry.\n# get all the unique values in the \u0026#39;Country\u0026#39; column countries = professors[\u0026#39;Country\u0026#39;].unique() # sort them alphabetically and then take a closer look countries.sort() countries array([\u0026#39;australia\u0026#39;, \u0026#39;austria\u0026#39;, \u0026#39;canada\u0026#39;, \u0026#39;china\u0026#39;, \u0026#39;finland\u0026#39;, \u0026#39;france\u0026#39;, \u0026#39;germany\u0026#39;, \u0026#39;greece\u0026#39;, \u0026#39;hongkong\u0026#39;, \u0026#39;ireland\u0026#39;, \u0026#39;italy\u0026#39;, \u0026#39;japan\u0026#39;, \u0026#39;macau\u0026#39;, \u0026#39;malaysia\u0026#39;, \u0026#39;mauritius\u0026#39;, \u0026#39;netherland\u0026#39;, \u0026#39;new zealand\u0026#39;, \u0026#39;norway\u0026#39;, \u0026#39;pakistan\u0026#39;, \u0026#39;portugal\u0026#39;, \u0026#39;russian federation\u0026#39;, \u0026#39;saudi arabia\u0026#39;, \u0026#39;scotland\u0026#39;, \u0026#39;singapore\u0026#39;, \u0026#39;south korea\u0026#39;, \u0026#39;southkorea\u0026#39;, \u0026#39;spain\u0026#39;, \u0026#39;sweden\u0026#39;, \u0026#39;thailand\u0026#39;, \u0026#39;turkey\u0026#39;, \u0026#39;uk\u0026#39;, \u0026#39;urbana\u0026#39;, \u0026#39;usa\u0026#39;, \u0026#39;usofa\u0026#39;], dtype=object) So what is it about fuzzy matching? Fuzzy matching is the process of automatically finding text strings that are very similar to the target string. In general, a string is considered \u0026ldquo;closer\u0026rdquo; to another one the fewer characters you\u0026rsquo;d need to change if you were transforming one string into another. So \u0026ldquo;apple\u0026rdquo; and \u0026ldquo;snapple\u0026rdquo; are two changes away from each other (add \u0026ldquo;s\u0026rdquo; and \u0026ldquo;n\u0026rdquo;) while \u0026ldquo;in\u0026rdquo; and \u0026ldquo;on\u0026rdquo; and one change away (replace \u0026ldquo;i\u0026rdquo; with \u0026ldquo;o\u0026rdquo;). You won\u0026rsquo;t always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\nFuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we\u0026rsquo;re going to get the ten strings from our list of cities that have the closest distance to \u0026ldquo;south korea\u0026rdquo;.\n# get the top 10 closest matches to \u0026#34;south korea\u0026#34; matches = fuzzywuzzy.process.extract(\u0026#34;south korea\u0026#34;, countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio) # take a look at them matches [(\u0026#39;south korea\u0026#39;, 100), (\u0026#39;southkorea\u0026#39;, 48), (\u0026#39;saudi arabia\u0026#39;, 43), (\u0026#39;norway\u0026#39;, 35), (\u0026#39;austria\u0026#39;, 33), (\u0026#39;ireland\u0026#39;, 33), (\u0026#39;pakistan\u0026#39;, 32), (\u0026#39;portugal\u0026#39;, 32), (\u0026#39;scotland\u0026#39;, 32), (\u0026#39;australia\u0026#39;, 30)] We can see that the first two items appear to be similar. Let\u0026rsquo;s replace all rows in the \u0026lsquo;Country\u0026rsquo; column that have a ratio \u0026gt; 47 with \u0026ldquo;south korea\u0026rdquo;.\n# function to replace rows in the provided column of the provided dataframe # that match the provided string above the provided ratio with the provided string def replace_matches_in_column(df, column, string_to_match, min_ratio = 47): # get a list of unique strings strings = df[column].unique() # get the top 10 closest matches to our input string matches = fuzzywuzzy.process.extract(string_to_match, strings, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio) # only get matches with a ratio \u0026gt; 90 close_matches = [matches[0] for matches in matches if matches[1] \u0026gt;= min_ratio] # get the rows of all the close matches in our dataframe rows_with_matches = df[column].isin(close_matches) # replace all rows with close matches with the input matches df.loc[rows_with_matches, column] = string_to_match # let us know the function\u0026#39;s done print(\u0026#34;All done!\u0026#34;) # use the function we just wrote to replace close matches to \u0026#34;south korea\u0026#34; with \u0026#34;south korea\u0026#34; replace_matches_in_column(df=professors, column=\u0026#39;Country\u0026#39;, string_to_match=\u0026#34;south korea\u0026#34;) Now, let\u0026rsquo;s do a double check:\n# get all the unique values in the \u0026#39;Country\u0026#39; column countries = professors[\u0026#39;Country\u0026#39;].unique() # sort them alphabetically and then take a closer look countries.sort() countries array([\u0026#39;australia\u0026#39;, \u0026#39;austria\u0026#39;, \u0026#39;canada\u0026#39;, \u0026#39;china\u0026#39;, \u0026#39;finland\u0026#39;, \u0026#39;france\u0026#39;, \u0026#39;germany\u0026#39;, \u0026#39;greece\u0026#39;, \u0026#39;hongkong\u0026#39;, \u0026#39;ireland\u0026#39;, \u0026#39;italy\u0026#39;, \u0026#39;japan\u0026#39;, \u0026#39;macau\u0026#39;, \u0026#39;malaysia\u0026#39;, \u0026#39;mauritius\u0026#39;, \u0026#39;netherland\u0026#39;, \u0026#39;new zealand\u0026#39;, \u0026#39;norway\u0026#39;, \u0026#39;pakistan\u0026#39;, \u0026#39;portugal\u0026#39;, \u0026#39;russian federation\u0026#39;, \u0026#39;saudi arabia\u0026#39;, \u0026#39;scotland\u0026#39;, \u0026#39;singapore\u0026#39;, \u0026#39;south korea\u0026#39;, \u0026#39;spain\u0026#39;, \u0026#39;sweden\u0026#39;, \u0026#39;thailand\u0026#39;, \u0026#39;turkey\u0026#39;, \u0026#39;uk\u0026#39;, \u0026#39;urbana\u0026#39;, \u0026#39;usa\u0026#39;, \u0026#39;usofa\u0026#39;], dtype=object) Now we can see that we only have \u0026ldquo;south korea\u0026rdquo; in the DataFrame!\n","permalink":"https://keanteng.github.io/home/docs/2023-08-19-data-cleaning/","summary":"Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. It is a key part of data science, and it can be deeply frustrating. What should we do to the missing values? Why the dates are not in the correct format? How to clean up inconsistent data entry? These are some of the problems that we will learn to tackle in this course.","title":"Data Cleaning"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nThe most common application of machine learning in the real world is forecasting. For example, businesses forecasting product demand, governments forecasting economic growth and meteorologists forecasting the weather. The understanding of things to come has become a pressing need across the science, government and industry, and machine learning is increasingly being applied to address this need.\nIn this course, we will learn about time series forecasting. We will also learn about:\nEngineering features to model major time series components such as trends, seasons and cycles Visualize time series with plot Create forecasting hybrids (combine the strength of complementary models) Adapt machine learning methods to various forecasting tasks 1. Introduction Basically, a time series is a set of observations recorded over time. The observations are recorded with a regular frequency such as day, week, month, quarter or year. We can use linear regression to build forecasting model for time series data:\ntarget = weight_1 * feature_1 + weight_2 * feature_2 + bias Parameters such as weight_1, weight_2 and bias are learned by the regression algorithm during training.\nThere are two unique features to time series:\nTime-step feature Lag feature Time-step feature can be derived from the time index. The most basic time-step feature is the time dummy that counts time steps in series from the beginning to the end:\n| | Hardcover | Time | |----------|-----------|------| | 1/4/2000 | 139 | 1 | | 2/4/2000 | 219 | 2 | | 3/4/2000 | 135 | 3 | Linear regression with the time dummy produces the model:\ntarget = weight * time + bias In a time plot:\nimport matplotlib.pyplot as plt import seaborn as sns plt.style.use(\u0026#34;seaborn-whitegrid\u0026#34;) plt.rc( \u0026#34;figure\u0026#34;, autolayout=True, figsize=(11, 4), titlesize=18, titleweight=\u0026#39;bold\u0026#39;, ) plt.rc( \u0026#34;axes\u0026#34;, labelweight=\u0026#34;bold\u0026#34;, labelsize=\u0026#34;large\u0026#34;, titleweight=\u0026#34;bold\u0026#34;, titlesize=16, titlepad=10, ) %config InlineBackend.figure_format = \u0026#39;retina\u0026#39; fig, ax = plt.subplots() ax.plot(\u0026#39;Time\u0026#39;, \u0026#39;Hardcover\u0026#39;, data=df, color=\u0026#39;0.75\u0026#39;) ax = sns.regplot(x=\u0026#39;Time\u0026#39;, y=\u0026#39;Hardcover\u0026#39;, data=df, ci=None, scatter_kws=dict(color=\u0026#39;0.25\u0026#39;)) ax.set_title(\u0026#39;Time Plot of Hardcover Sales\u0026#39;); Time Plot\nTime-step features let us model time dependence. A series is time dependent if its value can be predicted from the time they occurred in.\nMoreover, to make a lag feature from the dataset we will shift the observation of the target series so that they appear to have occurred later in time. Here\u0026rsquo;s a 1-step lag feature:\n| |Hardcover|Lag_1| |------------|---------|-----| |Date | | | |2000-04-01 |139 |NaN | |2000-04-02 |128 |139.0| |2000-04-03\t|172 |128.0| |2000-04-04 |139 |172.0| |2000-04-05 |191 |139.0| Linear regression with the time dummy produces the model:\ntarget = weight * lag + bias In a time plot:\nfig, ax = plt.subplots() ax = sns.regplot(x=\u0026#39;Lag_1\u0026#39;, y=\u0026#39;Hardcover\u0026#39;, data=df, ci=None, scatter_kws=dict(color=\u0026#39;0.25\u0026#39;)) ax.set_aspect(\u0026#39;equal\u0026#39;) ax.set_title(\u0026#39;Lag Plot of Hardcover Sales\u0026#39;); Lag Plot\nFrom the plot, it seems that sales on one day are correlated with sales from the previous day. In general, lag features let us model serial dependence which means an observation can be predicted from previous observation.\n1.1 Example In this section, we will look at an example using the tunnel traffic dataset from November 2003 to November 2005\nAdd time features to the data:\ndf = tunnel.copy() df[\u0026#39;Time\u0026#39;] = np.arange(len(tunnel.index)) df.head() To produce a linear regression plot:\nfrom sklearn.linear_model import LinearRegression # Training data X = df.loc[:, [\u0026#39;Time\u0026#39;]] # features y = df.loc[:, \u0026#39;NumVehicles\u0026#39;] # target # Train the model model = LinearRegression() model.fit(X, y) # Store the fitted values as a time series with the same time index as # the training data y_pred = pd.Series(model.predict(X), index=X.index) ax = y.plot(**plot_params) ax = y_pred.plot(ax=ax, linewidth=3) ax.set_title(\u0026#39;Time Plot of Tunnel Traffic\u0026#39;); Time Plot of Tunnel Traffic\nNow let\u0026rsquo;s add a lag column and use it for a linear regression plot:\ndf[\u0026#39;Lag_1\u0026#39;] = df[\u0026#39;NumVehicles\u0026#39;].shift(1) from sklearn.linear_model import LinearRegression X = df.loc[:, [\u0026#39;Lag_1\u0026#39;]] X.dropna(inplace=True) # drop missing values in the feature set y = df.loc[:, \u0026#39;NumVehicles\u0026#39;] # create the target y, X = y.align(X, join=\u0026#39;inner\u0026#39;) # drop corresponding values in target model = LinearRegression() model.fit(X, y) y_pred = pd.Series(model.predict(X), index=X.index) fig, ax = plt.subplots() ax.plot(X[\u0026#39;Lag_1\u0026#39;], y, \u0026#39;.\u0026#39;, color=\u0026#39;0.25\u0026#39;) ax.plot(X[\u0026#39;Lag_1\u0026#39;], y_pred) ax.set_aspect(\u0026#39;equal\u0026#39;) ax.set_ylabel(\u0026#39;NumVehicles\u0026#39;) ax.set_xlabel(\u0026#39;Lag_1\u0026#39;) ax.set_title(\u0026#39;Lag Plot of Tunnel Traffic\u0026#39;); Time Plot\nHere is how our forecast respond to the behavior of the series in the recent past:\nax = y.plot(**plot_params) ax = y_pred.plot() Observed vs Prediction Plot\n2. Trend Trend is a component of a time series that represents a persistent, long-term change in the mean of the series. It is the slowest-moving part of a series, the part representing the largest timescale of importance.\nExample of Trend\nTo see what kind of trend a series have, we can use the moving average plot. We compute the average values within a sliding window of some defined width.\nEach point on the graph represents the average of all the values in the series that fall within the window on either side. The idea is to smooth out any short-term fluctuations in the series so that only long-term changes remain.\nMoving Average\nFrom the above plot, we can see there is a repeating up and down movement yearly (seasonal change). For a change to be a part of the trend, it should occur over a longer period than the seasonal change. Thus, we take an average over a longer period than any seasonal period in the series (window size of 12) to smooth over the season within each year to visualize the trend.\nAfter identifying the trend, we can model it using a time-step feature. For example, a linear trend:\ntarget = a * time + b If we notice the trend to be quadratic, we can square the time dummy to the feature set:\ntarget = a * time ** 2 + b * time + c Quadaratic vs Linear Trend\n2.1 Example Let\u0026rsquo;s look back at the tunnel traffic dataset that we used for the previous section, in the series the observations are on a daily basis. We will use windows of 365 days to smooth over short-term changes within the year:\nmoving_average = tunnel.rolling( window=365, # 365-day window center=True, # puts the average at the center of the window min_periods=183, # choose about half the window size ).mean() # compute the mean (could also do median, std, min, max, ...) ax = tunnel.plot(style=\u0026#34;.\u0026#34;, color=\u0026#34;0.5\u0026#34;) moving_average.plot( ax=ax, linewidth=3, title=\u0026#34;Tunnel Traffic - 365-Day Moving Average\u0026#34;, legend=False, ); Moving Average Plot\nNow we will use the DeterministicProcess function from the statsmodels library to perform linear regression on the series:\nfrom statsmodels.tsa.deterministic import DeterministicProcess dp = DeterministicProcess( index=tunnel.index, # dates from the training data constant=True, # dummy feature for the bias (y_intercept) order=1, # the time dummy (trend) drop=True, # drop terms if necessary to avoid collinearity ) # `in_sample` creates features for the dates given in the `index` argument X = dp.in_sample() X.head() const\ttrend\tDay 2003-11-01\t1.0\t1.0 2003-11-02\t1.0\t2.0 2003-11-03\t1.0\t3.0 2003-11-04\t1.0\t4.0 2003-11-05\t1.0\t5.0 Model fitting:\nfrom sklearn.linear_model import LinearRegression y = tunnel[\u0026#34;NumVehicles\u0026#34;] # the target # The intercept is the same as the `const` feature from # DeterministicProcess. LinearRegression behaves badly with duplicated # features, so we need to be sure to exclude it here. model = LinearRegression(fit_intercept=False) model.fit(X, y) y_pred = pd.Series(model.predict(X), index=X.index) Plotting:\nax = tunnel.plot(style=\u0026#34;.\u0026#34;, color=\u0026#34;0.5\u0026#34;, title=\u0026#34;Tunnel Traffic - Linear Trend\u0026#34;) _ = y_pred.plot(ax=ax, linewidth=3, label=\u0026#34;Trend\u0026#34;) Time Plot\nNow let\u0026rsquo;s make a forecast using the fitted model:\nX = dp.out_of_sample(steps=30) y_fore = pd.Series(model.predict(X), index=X.index) y_fore.head() 2005-11-17 114981.801146 2005-11-18 115004.298595 2005-11-19 115026.796045 2005-11-20 115049.293494 2005-11-21 115071.790944 Freq: D, dtype: float64 ax = tunnel[\u0026#34;2005-05\u0026#34;:].plot(title=\u0026#34;Tunnel Traffic - Linear Trend Forecast\u0026#34;, **plot_params) ax = y_pred[\u0026#34;2005-05\u0026#34;:].plot(ax=ax, linewidth=3, label=\u0026#34;Trend\u0026#34;) ax = y_fore.plot(ax=ax, linewidth=3, label=\u0026#34;Trend Forecast\u0026#34;, color=\u0026#34;C3\u0026#34;) _ = ax.legend() Forecast Plot\n3. Seasonality A time series exhibits seasonality if there is a regular, periodic change in the mean of the series. Normally, such changes follow the clock and calendar. It can be repetitions over a day, week, or year.\nSeasonality Plot\nIn this section, we will explore two kinds of feature to model seasonality:\nIndicator: Best for season with few observations like weekly or daily observation Fourier Feature: Best for season with many observations (annual season of daily observations) Seasonal plot can be used to discover seasonal patterns where it shows segments of the time series plot against some common period (period you want to observe). Seasonal indicators are binary features that represent seasonal differences in the level of a time series. We can perform one-hot encoding to get weekly seasonal indicators or monthly seasonal indicator.\n3.1 Fourier Features \u0026amp; The Periodogram For long seasons over many observations, indicators seem to be impractical to capture the overall shape of the seasonal curve:\nSeasonal Plot\nFor the above plot, we can see the repetitions of various frequencies such as yearly and weekly. Of course, we want to capture these frequencies with Fourier features.\nFourier features are pairs of sine and cosine curves, one pair for each potential frequency in the season starting with the longest. Fourier pairs modeling annual seasonality would have frequencies: once per year, twice per year, three times per year, and so on.\nFourier Pairs\nIf we add a set of these sine / cosine curves to our training data, the linear regression algorithm will figure out the weights that will fit the seasonal component in the target series.\nFourier Pairs \u0026 Approx Seasonal Pattern\nIn fact, we only need eight features (4 sin and cosine pairs) to get a good estimate of the annual seasonality. The question remains is how do we get to choose the number of Fourier pairs? We can approach the problem with a periodogram where it tells us the frequencies in a time series.\nPeriodogram\nFrom the above plot, the periodogram drops off after the quarterly frequency, so we will choose four Fourier pairs to estimate the annual season. We ignore the weekly frequency as it is better to model with indicators.\nHere\u0026rsquo;s how we can derive a set of Fourier features from the index of time series:\nimport numpy as np def fourier_features(index, freq, order): time = np.arange(len(index), dtype=np.float32) k = 2 * np.pi * (1 / freq) * time features = {} for i in range(1, order + 1): features.update({ f\u0026#34;sin_{freq}_{i}\u0026#34;: np.sin(i * k), f\u0026#34;cos_{freq}_{i}\u0026#34;: np.cos(i * k), }) return pd.DataFrame(features, index=index) # Compute Fourier features to the 4th order (8 new features) for a # series y with daily observations and annual seasonality: # # fourier_features(y, freq=365.25, order=4) 3.2 Example Defining some functions. We are using the same tunnel traffic dataset as before:\n# annotations: https://stackoverflow.com/a/49238256/5769929 def seasonal_plot(X, y, period, freq, ax=None): if ax is None: _, ax = plt.subplots() palette = sns.color_palette(\u0026#34;husl\u0026#34;, n_colors=X[period].nunique(),) ax = sns.lineplot( x=freq, y=y, hue=period, data=X, ci=False, ax=ax, palette=palette, legend=False, ) ax.set_title(f\u0026#34;Seasonal Plot ({period}/{freq})\u0026#34;) for line, name in zip(ax.lines, X[period].unique()): y_ = line.get_ydata()[-1] ax.annotate( name, xy=(1, y_), xytext=(6, 0), color=line.get_color(), xycoords=ax.get_yaxis_transform(), textcoords=\u0026#34;offset points\u0026#34;, size=14, va=\u0026#34;center\u0026#34;, ) return ax def plot_periodogram(ts, detrend=\u0026#39;linear\u0026#39;, ax=None): from scipy.signal import periodogram fs = pd.Timedelta(\u0026#34;1Y\u0026#34;) / pd.Timedelta(\u0026#34;1D\u0026#34;) freqencies, spectrum = periodogram( ts, fs=fs, detrend=detrend, window=\u0026#34;boxcar\u0026#34;, scaling=\u0026#39;spectrum\u0026#39;, ) if ax is None: _, ax = plt.subplots() ax.step(freqencies, spectrum, color=\u0026#34;purple\u0026#34;) ax.set_xscale(\u0026#34;log\u0026#34;) ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104]) ax.set_xticklabels( [ \u0026#34;Annual (1)\u0026#34;, \u0026#34;Semiannual (2)\u0026#34;, \u0026#34;Quarterly (4)\u0026#34;, \u0026#34;Bimonthly (6)\u0026#34;, \u0026#34;Monthly (12)\u0026#34;, \u0026#34;Biweekly (26)\u0026#34;, \u0026#34;Weekly (52)\u0026#34;, \u0026#34;Semiweekly (104)\u0026#34;, ], rotation=30, ) ax.ticklabel_format(axis=\u0026#34;y\u0026#34;, style=\u0026#34;sci\u0026#34;, scilimits=(0, 0)) ax.set_ylabel(\u0026#34;Variance\u0026#34;) ax.set_title(\u0026#34;Periodogram\u0026#34;) return ax We will start with the seasonal plots over a week and a year:\nX = tunnel.copy() # days within a week X[\u0026#34;day\u0026#34;] = X.index.dayofweek # the x-axis (freq) X[\u0026#34;week\u0026#34;] = X.index.week # the seasonal period (period) # days within a year X[\u0026#34;dayofyear\u0026#34;] = X.index.dayofyear X[\u0026#34;year\u0026#34;] = X.index.year fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6)) seasonal_plot(X, y=\u0026#34;NumVehicles\u0026#34;, period=\u0026#34;week\u0026#34;, freq=\u0026#34;day\u0026#34;, ax=ax0) seasonal_plot(X, y=\u0026#34;NumVehicles\u0026#34;, period=\u0026#34;year\u0026#34;, freq=\u0026#34;dayofyear\u0026#34;, ax=ax1); Seasonal Plot\nFor the periodogram:\nplot_periodogram(tunnel.NumVehicles); Forecast Plot\nFrom the periodogram, there is a strong weekly season and a weaker annual season. We\u0026rsquo;ll model the weekly season with indicator and the yearly season with Fourier features. From right to left, the periodogram falls off between Bimonthly (6) and Monthly (12), so let\u0026rsquo;s use 10 Fourier pairs.\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess fourier = CalendarFourier(freq=\u0026#34;A\u0026#34;, order=10) # 10 sin/cos pairs for \u0026#34;A\u0026#34;nnual seasonality dp = DeterministicProcess( index=tunnel.index, constant=True, # dummy feature for bias (y-intercept) order=1, # trend (order 1 means linear) seasonal=True, # weekly seasonality (indicators) additional_terms=[fourier], # annual seasonality (fourier) drop=True, # drop terms to avoid collinearity ) X = dp.in_sample() # create features for dates in tunnel.index Model prediction:\ny = tunnel[\u0026#34;NumVehicles\u0026#34;] model = LinearRegression(fit_intercept=False) _ = model.fit(X, y) y_pred = pd.Series(model.predict(X), index=y.index) X_fore = dp.out_of_sample(steps=90) y_fore = pd.Series(model.predict(X_fore), index=X_fore.index) ax = y.plot(color=\u0026#39;0.25\u0026#39;, style=\u0026#39;.\u0026#39;, title=\u0026#34;Tunnel Traffic - Seasonal Forecast\u0026#34;) ax = y_pred.plot(ax=ax, label=\u0026#34;Seasonal\u0026#34;) ax = y_fore.plot(ax=ax, label=\u0026#34;Seasonal Forecast\u0026#34;, color=\u0026#39;C3\u0026#39;) _ = ax.legend() Forecast Plot\n4. Time Series as Features For some time series, they can only be modeled as a serially dependent properties, that is using as features past values of the target series. The goal in this lesson is to train models to fit curves to plots like those on the right \u0026ndash; we want them to learn serial dependence:\nTime vs Serial Dependence Plot\nOne common way for serial dependence to manifest is in cycle - patterns of growth and decay in a time series associated with how the value in a series at one time depends on values at previous times, but not necessarily on the time step itself. Cyclic behavior is a characteristic of systems that can effect themselves, economies, epidemics, animal populations and volcano eruptions often display cyclic behavior:\nCyclical Plot\nWhat distinguishes cyclic behavior from seasonality is that cycles are not necessarily time dependent, as seasons are. What happens in a cycle is less about the particular date of occurrence, and more about what has happened in the recent past\n4.1 Lagged Series \u0026amp; Lag Plots To investigate serial dependence, we need to create \u0026ldquo;lagged\u0026rdquo; copies of the series. When we say \u0026ldquo;lagging\u0026rdquo;, it means we are shifting the time series values forward by one or more time steps. By lagging a time series, we make past values appear contemporaneous with the values we are trying to predict.\ny\ty_lag_1\ty_lag_2\t1954-07\t5.8\tNaN\tNaN 1954-08\t6.0\t5.8\tNaN 1954-09\t6.1\t6.0\t5.8 1954-10\t5.7\t6.1\t6.0 1954-11\t5.3\t5.7\t6.1 A lag plot shows a time series values plotted against its lags. In the below images, there is a strong linear relationship between current unemployment rate and past rates.\nLag Plots\nIn order to measure serial dependence, we can use autocorrelation - the correlation a time series has with one of its lag. In general, it would not be useful to include every lag with a large autocorrelation. We can find the partial autocorrelation that tells us the correlation of a lag accounting for all the previous lags (amount of new correlation the lag contribute).\nIn the figure below, lag 1 through lag 6 fall outside the intervals of \u0026ldquo;no correlation\u0026rdquo; (in blue), so we might choose lags 1 through lag 6 as features for US Unemployment. (Lag 11 is likely a false positive.)\nPartial Autocorrelation Plot\nImportantly, we need to be mindful that autocorrelation and partial autocorrelation are measures of linear dependence. Real-world time series often have non-linear dependences, it\u0026rsquo;s good that a make a lag plot when choosing lag features.\nLag Plot\nSome non-linear relationship is the above image can be transformed to linear or learned by an appropriate algorithm.\n4.2 Example Let\u0026rsquo;s define some functions for it to be used for the flu trend dataset, containing records of doctor\u0026rsquo;s visits for the flu for weeks between 2009 and 2016:\nimport matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from scipy.signal import periodogram from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from statsmodels.graphics.tsaplots import plot_pacf def lagplot(x, y=None, lag=1, standardize=False, ax=None, **kwargs): from matplotlib.offsetbox import AnchoredText x_ = x.shift(lag) if standardize: x_ = (x_ - x_.mean()) / x_.std() if y is not None: y_ = (y - y.mean()) / y.std() if standardize else y else: y_ = x corr = y_.corr(x_) if ax is None: fig, ax = plt.subplots() scatter_kws = dict( alpha=0.75, s=3, ) line_kws = dict(color=\u0026#39;C3\u0026#39;, ) ax = sns.regplot(x=x_, y=y_, scatter_kws=scatter_kws, line_kws=line_kws, lowess=True, ax=ax, **kwargs) at = AnchoredText( f\u0026#34;{corr:.2f}\u0026#34;, prop=dict(size=\u0026#34;large\u0026#34;), frameon=True, loc=\u0026#34;upper left\u0026#34;, ) at.patch.set_boxstyle(\u0026#34;square, pad=0.0\u0026#34;) ax.add_artist(at) ax.set(title=f\u0026#34;Lag {lag}\u0026#34;, xlabel=x_.name, ylabel=y_.name) return ax def plot_lags(x, y=None, lags=6, nrows=1, lagplot_kwargs={}, **kwargs): import math kwargs.setdefault(\u0026#39;nrows\u0026#39;, nrows) kwargs.setdefault(\u0026#39;ncols\u0026#39;, math.ceil(lags / nrows)) kwargs.setdefault(\u0026#39;figsize\u0026#39;, (kwargs[\u0026#39;ncols\u0026#39;] * 2, nrows * 2 + 0.5)) fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs) for ax, k in zip(fig.get_axes(), range(kwargs[\u0026#39;nrows\u0026#39;] * kwargs[\u0026#39;ncols\u0026#39;])): if k + 1 \u0026lt;= lags: ax = lagplot(x, y, lag=k + 1, ax=ax, **lagplot_kwargs) ax.set_title(f\u0026#34;Lag {k + 1}\u0026#34;, fontdict=dict(fontsize=14)) ax.set(xlabel=\u0026#34;\u0026#34;, ylabel=\u0026#34;\u0026#34;) else: ax.axis(\u0026#39;off\u0026#39;) plt.setp(axs[-1, :], xlabel=x.name) plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name) fig.tight_layout(w_pad=0.1, h_pad=0.1) return fig data_dir = Path(\u0026#34;../input/ts-course-data\u0026#34;) flu_trends = pd.read_csv(data_dir / \u0026#34;flu-trends.csv\u0026#34;) flu_trends.set_index( pd.PeriodIndex(flu_trends.Week, freq=\u0026#34;W\u0026#34;), inplace=True, ) flu_trends.drop(\u0026#34;Week\u0026#34;, axis=1, inplace=True) ax = flu_trends.FluVisits.plot(title=\u0026#39;Flu Trends\u0026#39;, **plot_params) _ = ax.set(ylabel=\u0026#34;Office Visits\u0026#34;) Flu Trend Plot\nFlu Trends data shows irregular cycles instead of a regular seasonality: the peak tends to occur around the new year, but sometimes earlier or later, sometimes larger or smaller.\nLet\u0026rsquo;s look at the lag and autocorrelation plot:\n_ = plot_lags(flu_trends.FluVisits, lags=12, nrows=2) _ = plot_pacf(flu_trends.FluVisits, lags=12) Lag Plot\nPACF Plot\nFrom the lag plot, it seems that the relationship of the flu visits to its lags is mostly linear. For PACF plot, we can capture the serial dependence using lags 1, 2, 3 and 4. Here\u0026rsquo;s how to make lag and fill the NaN cells with 0:\ndef make_lags(ts, lags): return pd.concat( { f\u0026#39;y_lag_{i}\u0026#39;: ts.shift(i) for i in range(1, lags + 1) }, axis=1) X = make_lags(flu_trends.FluVisits, lags=4) X = X.fillna(0.0) Making forecast:\n# Create target series and data splits y = flu_trends.FluVisits.copy() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=60, shuffle=False) # Fit and predict model = LinearRegression() # `fit_intercept=True` since we didn\u0026#39;t use DeterministicProcess model.fit(X_train, y_train) y_pred = pd.Series(model.predict(X_train), index=y_train.index) y_fore = pd.Series(model.predict(X_test), index=y_test.index) ax = y_train.plot(**plot_params) ax = y_test.plot(**plot_params) ax = y_pred.plot(ax=ax) _ = y_fore.plot(ax=ax, color=\u0026#39;C3\u0026#39;) Forecast Plot\nTo improve the forecast we could try to find leading indicators, time series that could provide an \u0026ldquo;early warning\u0026rdquo; for changes in flu cases. For our second approach then we\u0026rsquo;ll add to our training data the popularity of some flu-related search terms as measured by Google Trends.\nPlotting the search phrase \u0026lsquo;FluCough\u0026rsquo; against the target \u0026lsquo;FluVisits\u0026rsquo; suggests such search terms could be useful as leading indicators: flu-related searches tend to become more popular in the weeks prior to office visits.\nax = flu_trends.plot( y=[\u0026#34;FluCough\u0026#34;, \u0026#34;FluVisits\u0026#34;], secondary_y=\u0026#34;FluCough\u0026#34;, ) Flu Trend Plot\nFiltering the search terms:\nsearch_terms = [\u0026#34;FluContagious\u0026#34;, \u0026#34;FluCough\u0026#34;, \u0026#34;FluFever\u0026#34;, \u0026#34;InfluenzaA\u0026#34;, \u0026#34;TreatFlu\u0026#34;, \u0026#34;IHaveTheFlu\u0026#34;, \u0026#34;OverTheCounterFlu\u0026#34;, \u0026#34;HowLongFlu\u0026#34;] # Create three lags for each search term X0 = make_lags(flu_trends[search_terms], lags=3) X0.columns = [\u0026#39; \u0026#39;.join(col).strip() for col in X0.columns.values] # Create four lags for the target, as before X1 = make_lags(flu_trends[\u0026#39;FluVisits\u0026#39;], lags=4) # Combine to create the training data X = pd.concat([X0, X1], axis=1).fillna(0.0) Forecast:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=60, shuffle=False) model = LinearRegression() model.fit(X_train, y_train) y_pred = pd.Series(model.predict(X_train), index=y_train.index) y_fore = pd.Series(model.predict(X_test), index=y_test.index) ax = y_test.plot(**plot_params) _ = y_fore.plot(ax=ax, color=\u0026#39;C3\u0026#39;) Forecast Plot\nOur forecasts are a bit rougher, but our model appears to be better able to anticipate sudden increases in flu visits, suggesting that the several time series of search popularity were indeed effective as leading indicators.\nThe time series illustrated in this lesson are what you might call \u0026ldquo;purely cyclic\u0026rdquo;: they have no obvious trend or seasonality. It\u0026rsquo;s not uncommon though for time series to possess trend, seasonality, and cycles \u0026ndash; all three components at once. You could model such series with linear regression by just adding the appropriate features for each component. You can even combine models trained to learn the components separately\n5. Hybrid Models To design an effective hybrid, we have to know how a time series is constructed. Previously, we learned about trend, season and cycles. Many time series can be described by an additive model of these three components plus some error term:\nseries = trend + seasons + cycles + error Residuals of a model are the difference between the model\u0026rsquo;s target and the prediction, as illustrated below:\nResiduals Plot\nNow imagine that we learn the time series components in an iterative manner: we start by learning the trend, then by subtracting it out, we learn the series seasonality, follow by cycles and then the error term:\nLearning TS Components\nOf course, it is possible for use to use one algorithm for some components and another algorithm for the rest. That means, we use one algorithm to fit the original series and another algorithm for the residuals series:\n# 1. Train and predict with first model model_1.fit(X_train_1, y_train) y_pred_1 = model_1.predict(X_train) # 2. Train and predict with second model on residuals model_2.fit(X_train_2, y_train - y_pred_1) y_pred_2 = model_2.predict(X_train_2) # 3. Add to get overall predictions y_pred = y_pred_1 + y_pred_2 While it\u0026rsquo;s possible to use more than two models, in practice it doesn\u0026rsquo;t seem to be especially helpful. In fact, the most common strategy for constructing hybrids is the one we\u0026rsquo;ve just described: a simple (usually linear) learning algorithm followed by a complex, non-linear learner like GBDTs or a deep neural net, the simple model typically designed as a \u0026ldquo;helper\u0026rdquo; for the powerful algorithm that follows.\nThere are generally two ways a regression algorithm can make predictions: either by transforming the features or by transforming the target. Feature-transforming algorithms learn some mathematical function that takes features as an input and then combines and transforms them to produce an output that matches the target values in the training set. Linear regression and neural nets are of this kind.\nTarget-transforming algorithms use the features to group the target values in the training set and make predictions by averaging values in a group; a set of feature just indicates which group to average. Decision trees and nearest neighbors are of this kind.\nThe important thing is this: feature transformers generally can extrapolate target values beyond the training set given appropriate features as inputs, but the predictions of target transformers will always be bound within the range of the training set. If the time dummy continues counting time steps, linear regression continues drawing the trend line. Given the same time dummy, a decision tree will predict the trend indicated by the last step of the training data into the future forever. Decision trees cannot extrapolate trends. Random forests and gradient boosted decision trees (like XGBoost) are ensembles of decision trees, so they also cannot extrapolate trends.\nDecision Tree Failed to Extrapolate Trend\nSo, we could use linear regression to extrapolate the trend, transform the target to remove the trend, and apply XGBoost to the detrended residuals. To hybridize a neural net (a feature transformer), you could instead include the predictions of another model as a feature, which the neural net would then include as part of its own predictions. The method of fitting to residuals is actually the same method the gradient boosting algorithm uses, so we will call these boosted hybrids; the method of using predictions as features is known as \u0026ldquo;stacking\u0026rdquo;, so we will call these stacked hybrids.\n5.1 Example In this example, we will use the US Retail Sales data set from 1992 to 2019. We will also create a linear regression and XGBoost hybrid for prediction.\nBuildingMaterials\tFoodAndBeverage\t1992-01-01\t8964\t29589 1992-02-01\t9023\t28570 1992-03-01\t10608\t29682 1992-04-01\t11630\t30228 1992-05-01\t12327\t31677 We will start by learning the trend of the series using linear regression (a quadratic trend is used)\ny = retail.copy() # Create trend features dp = DeterministicProcess( index=y.index, # dates from the training data constant=True, # the intercept order=2, # quadratic trend drop=True, # drop terms to avoid collinearity ) X = dp.in_sample() # features for the training data # Test on the years 2016-2019. It will be easier for us later if we # split the date index instead of the dataframe directly. idx_train, idx_test = train_test_split( y.index, test_size=12 * 4, shuffle=False, ) X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :] y_train, y_test = y.loc[idx_train], y.loc[idx_test] # Fit trend model model = LinearRegression(fit_intercept=False) model.fit(X_train, y_train) # Make predictions y_fit = pd.DataFrame( model.predict(X_train), index=y_train.index, columns=y_train.columns, ) y_pred = pd.DataFrame( model.predict(X_test), index=y_test.index, columns=y_test.columns, ) # Plot axs = y_train.plot(color=\u0026#39;0.25\u0026#39;, subplots=True, sharex=True) axs = y_test.plot(color=\u0026#39;0.25\u0026#39;, subplots=True, sharex=True, ax=axs) axs = y_fit.plot(color=\u0026#39;C0\u0026#39;, subplots=True, sharex=True, ax=axs) axs = y_pred.plot(color=\u0026#39;C3\u0026#39;, subplots=True, sharex=True, ax=axs) for ax in axs: ax.legend([]) _ = plt.suptitle(\u0026#34;Trends\u0026#34;) Trend Plot\nLinear regression algorithm is capable of multi-output regression, the XGBoost algorithm is not. To predict multiple series at once with XGBoost, we\u0026rsquo;ll instead convert these series from wide format, with one time series per column, to long format, with series indexed by categories along rows.\n# The `stack` method converts column labels to row labels, pivoting from wide format to long X = retail.stack() # pivot dataset wide to long display(X.head()) y = X.pop(\u0026#39;Sales\u0026#39;) # grab target series Industries\t1992-01-01\tBuildingMaterials\t8964 FoodAndBeverage\t29589 1992-02-01\tBuildingMaterials\t9023 FoodAndBeverage\t28570 1992-03-01\tBuildingMaterials\t10608 Construct the train and test set:\n# Turn row labels into categorical feature columns with a label encoding X = X.reset_index(\u0026#39;Industries\u0026#39;) # Label encoding for \u0026#39;Industries\u0026#39; feature for colname in X.select_dtypes([\u0026#34;object\u0026#34;, \u0026#34;category\u0026#34;]): X[colname], _ = X[colname].factorize() # Label encoding for annual seasonality X[\u0026#34;Month\u0026#34;] = X.index.month # values are 1, 2, ..., 12 # Create splits X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :] y_train, y_test = y.loc[idx_train], y.loc[idx_test] Convert the trend predictions made earlier to long format and then subtract them from the original series. That will give us detrended (residual) series that XGBoost can learn.\n# Pivot wide to long (stack) and convert DataFrame to Series (squeeze) y_fit = y_fit.stack().squeeze() # trend from training set y_pred = y_pred.stack().squeeze() # trend from test set # Create residuals (the collection of detrended series) from the training set y_resid = y_train - y_fit # Train XGBoost on the residuals xgb = XGBRegressor() xgb.fit(X_train, y_resid) # Add the predicted residuals onto the predicted trends y_fit_boosted = xgb.predict(X_train) + y_fit y_pred_boosted = xgb.predict(X_test) + y_pred axs = y_train.unstack([\u0026#39;Industries\u0026#39;]).plot( color=\u0026#39;0.25\u0026#39;, figsize=(11, 5), subplots=True, sharex=True, title=[\u0026#39;BuildingMaterials\u0026#39;, \u0026#39;FoodAndBeverage\u0026#39;], ) axs = y_test.unstack([\u0026#39;Industries\u0026#39;]).plot( color=\u0026#39;0.25\u0026#39;, subplots=True, sharex=True, ax=axs, ) axs = y_fit_boosted.unstack([\u0026#39;Industries\u0026#39;]).plot( color=\u0026#39;C0\u0026#39;, subplots=True, sharex=True, ax=axs, ) axs = y_pred_boosted.unstack([\u0026#39;Industries\u0026#39;]).plot( color=\u0026#39;C3\u0026#39;, subplots=True, sharex=True, ax=axs, ) for ax in axs: ax.legend([]) Forecast Plot\n6. Forecasting with Machine Learning Before we design a forecasting model, we should ask:\nWhat information is available at the time the forecast is made Time period during which you require the forecasted values The forecast origin is time at which you are making a forecast. Practically, you might consider the forecast origin to be the last time for which you have training data for the time being predicted. Everything up to he origin can be used to create features.\nThe forecast horizon is the time for which you are making a forecast. We often describe a forecast by the number of time steps in its horizon: a \u0026ldquo;1-step\u0026rdquo; forecast or \u0026ldquo;5-step\u0026rdquo; forecast, say. The forecast horizon describes the target.\nExample\nThe time between the origin and the horizon is the lead time (or sometimes latency) of the forecast. A forecast\u0026rsquo;s lead time is described by the number of steps from origin to horizon: a \u0026ldquo;1-step ahead\u0026rdquo; or \u0026ldquo;3-step ahead\u0026rdquo; forecast, say. In practice, it may be necessary for a forecast to begin multiple steps ahead of the origin because of delays in data acquisition or processing.\nTo forecast time series with ML algorithm, we have to transform the series into a DataFrame that we can use with the algorithms. Each row in a DataFrame represents a single forecast. The time index of the row is the first time in the forecast horizon, but we arrange values for the entire horizon in the same row. For multistep forecasts, this means we are requiring a model to produce multiple outputs, one for each step.\nDataFrame Example\nThe above illustrates how a dataset would be prepared similar to the Defining a Forecast figure: a three-step forecasting task with a two-step lead time using five lag features. The original time series is y_step_1. The missing values we could either fill in or drop.\n6.1 Multistep Forecasting Strategies Direct Strategy Train a separate model for each step in the horizon: one model forecasts 1-step ahead, another 2-steps ahead, and so on. Forecasting 1-step ahead is a different problem than 2-steps ahead (and so on), so it can help to have a different model make forecasts for each step. The downside is that training lots of models can be computationally expensive.\nRecursive Strategy Train a single one-step model and use its forecasts to update the lag features for the next step. With the recursive method, we feed a model\u0026rsquo;s 1-step forecast back in to that same model to use as a lag feature for the next forecasting step. We only need to train one model, but since errors will propagate from step to step, forecasts can be inaccurate for long horizons.\nDirRec Strategy A combination of the direct and recursive strategies: train a model for each step and use forecasts from previous steps as new lag features. Step by step, each model gets an additional lag input. Since each model always has an up-to-date set of lag features, the DirRec strategy can capture serial dependence better than Direct, but it can also suffer from error propagation like Recursive.\n6.2 Example Here, let\u0026rsquo;s take a look at the flu trends dataset previously used. We will apply multi-output and direct strategy to the dataset for forecast.\nPreparing datset:\ndef make_lags(ts, lags, lead_time=1): return pd.concat( { f\u0026#39;y_lag_{i}\u0026#39;: ts.shift(i) for i in range(lead_time, lags + lead_time) }, axis=1) # Four weeks of lag features y = flu_trends.FluVisits.copy() X = make_lags(y, lags=4).fillna(0.0) def make_multistep_target(ts, steps): return pd.concat( {f\u0026#39;y_step_{i + 1}\u0026#39;: ts.shift(-i) for i in range(steps)}, axis=1) # Eight-week forecast y = make_multistep_target(y, steps=8).dropna() # Shifting has created indexes that don\u0026#39;t match. Only keep times for # which we have both targets and features. y, X = y.align(X, join=\u0026#39;inner\u0026#39;, axis=0) # Create splits X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False) model = LinearRegression() model.fit(X_train, y_train) y_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns) y_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns) train_rmse = mean_squared_error(y_train, y_fit, squared=False) test_rmse = mean_squared_error(y_test, y_pred, squared=False) print((f\u0026#34;Train RMSE: {train_rmse:.2f}\\n\u0026#34; f\u0026#34;Test RMSE: {test_rmse:.2f}\u0026#34;)) palette = dict(palette=\u0026#39;husl\u0026#39;, n_colors=64) fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6)) ax1 = flu_trends.FluVisits[y_fit.index].plot(**plot_params, ax=ax1) ax1 = plot_multistep(y_fit, ax=ax1, palette_kwargs=palette) _ = ax1.legend([\u0026#39;FluVisits (train)\u0026#39;, \u0026#39;Forecast\u0026#39;]) ax2 = flu_trends.FluVisits[y_pred.index].plot(**plot_params, ax=ax2) ax2 = plot_multistep(y_pred, ax=ax2, palette_kwargs=palette) _ = ax2.legend([\u0026#39;FluVisits (test)\u0026#39;, \u0026#39;Forecast\u0026#39;]) XGBoost can\u0026rsquo;t produce multiple outputs for regression tasks. But by applying the Direct reduction strategy, we can still use it to produce multi-step forecasts. This is as easy as wrapping it with scikit-learn\u0026rsquo;s MultiOutputRegressor.\nfrom sklearn.multioutput import MultiOutputRegressor model = MultiOutputRegressor(XGBRegressor()) model.fit(X_train, y_train) y_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns) y_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns) train_rmse = mean_squared_error(y_train, y_fit, squared=False) test_rmse = mean_squared_error(y_test, y_pred, squared=False) print((f\u0026#34;Train RMSE: {train_rmse:.2f}\\n\u0026#34; f\u0026#34;Test RMSE: {test_rmse:.2f}\u0026#34;)) palette = dict(palette=\u0026#39;husl\u0026#39;, n_colors=64) fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6)) ax1 = flu_trends.FluVisits[y_fit.index].plot(**plot_params, ax=ax1) ax1 = plot_multistep(y_fit, ax=ax1, palette_kwargs=palette) _ = ax1.legend([\u0026#39;FluVisits (train)\u0026#39;, \u0026#39;Forecast\u0026#39;]) ax2 = flu_trends.FluVisits[y_pred.index].plot(**plot_params, ax=ax2) ax2 = plot_multistep(y_pred, ax=ax2, palette_kwargs=palette) _ = ax2.legend([\u0026#39;FluVisits (test)\u0026#39;, \u0026#39;Forecast\u0026#39;]) ","permalink":"https://keanteng.github.io/home/docs/2023-08-19-time-series/","summary":"The most common application of machine learning in the real world is forecasting. For example, businesses forecasting product demand, governments forecasting economic growth and meteorologists forecasting the weather.","title":"Time Series"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nIn this course, we will explore on data visualization using seaborn, a Python package to visualize data with a variety of plot types. The package is powerful yet easy to use, check out the below images on the plot types that seaborn is able to generate. You can also scroll to the bottom to see the table summary:\nPlots That seaborn Can Create\nLet\u0026rsquo;s explore the Python code to create different plot type with seaborn\n1. Lineplot In the code below, we use sns.lineplot() where it tells Python that we want to produce a line chart with the specified datasets. We only need to change the data parameters if we would like to plot for a different dataset.\nMoreover, we can also set the size of the plot by calling plt.figure(figsize = (w,h)). By adjusting the height and width of the plot we can set our plot to the desired size.\n# setting the plot size plt.figure(figsize = (16,6)) # width and height # plot line plot sns.lineplot(data = fifa_data) Line Plot\nNow, let\u0026rsquo;s consider that we are using a dataset about the number of steams per day for songs such as:\nShape of You Despacito Something Just Like This We would like to compare the streams between \u0026ldquo;Shape of You\u0026rdquo; and \u0026ldquo;Despacito\u0026rdquo;. But we do not want the plot to include the song \u0026ldquo;Something Just Like This\u0026rdquo;. Here\u0026rsquo;s how we can do it:\nplt.title(\u0026#34;Comparing Two Songs Streams\u0026#34;) sns.lineplot(data = spotify[\u0026#39;Shape of You\u0026#39;], label = \u0026#39;Shape of You\u0026#39;) sns.lineplot(data = spotify[\u0026#39;Despacito\u0026#39;], label = \u0026#39;Shape of You\u0026#39;) plt.xlabel(\u0026#34;Date\u0026#34;) Comparing Two Songs Streams\n2. Bar Charts \u0026amp; Heatmaps For bar chart, we can plot it with sns.barplot(). Let\u0026rsquo;s say we want to visualize the average arrival delay for an airline service by month starting from January to December:\ndata = pd.read_csv(file_name, index_cols = \u0026#39;Month\u0026#39;) sns.barplot(x = data.index, y =data[\u0026#39;delay\u0026#39;]) plt.title(\u0026#34;Average Arrival Delay By Month\u0026#34;) plt.ylabel(\u0026#39;Arrival Delay (in minute)\u0026#39;) Bar Plot\nLet\u0026rsquo;s also look at heatmap where it can be used to illustrate patterns in our dataset by color-coding each cell to its corresponding value:\nsns.heatmap(data = data, annot = True) plt.title(\u0026#39;Average Airline Delay for Each Airline\u0026#39;) plt.xlabel(\u0026#39;Airline\u0026#39;) Heatmap\nThe annot parameter ensures all value appears on the chart. By setting to False, we would have no number for each cell.\n3. Scatter Plot Scatter plot is used to show the relationship between two variables. It is a useful plot to understand the relationship between two variables. Here\u0026rsquo;s how we can plot it using Python:\nsns.scatterplot(x = insurance_data[\u0026#39;bmi\u0026#39;], y = insurance_data[\u0026#39;charge\u0026#39;]) Scatter Plot\nFrom the above plot, it seems that BMI are positive correlated with the insurance costs. Now, let\u0026rsquo;s do a double-checking by adding a regression line to our plot:\nsns.regplot(x = insurance_data[\u0026#39;bmi\u0026#39;], y = insurance_data[\u0026#39;charge\u0026#39;]) Scatter Plot With Regression Line\nIn fact we can also perform some color-coding to our plot by adding the hue parameter. Let\u0026rsquo;s say we want to color-code the plot by separating people that smoke and do not smoke:\nsns.scatterplot(x = insurance_data[\u0026#39;bmi\u0026#39;], y = insurance_data[\u0026#39;charge\u0026#39;], hue = insurance_data[\u0026#39;smoker\u0026#39;]) Color-Coded Scatter Plot\nIt seems that smoker will tend to pay more than non-smoker. Let\u0026rsquo;s check it again by adding two regression lines:\nsns.lmplot(x = \u0026#39;bmi\u0026#39;, y = \u0026#39;charge\u0026#39;, hue = \u0026#39;smoker\u0026#39;, data = insurance_data) Multiple Regression Lines Plot\nAnother interesting plot to look into is known as the categorical scatter plot. The plot can be produced with the sns.swarmplot() command:\nsns.swarmplot(x = insurance_data[\u0026#39;smoker\u0026#39;], y = insurance_data[\u0026#39;charges\u0026#39;]) Swarm Plot\nHere\u0026rsquo;re some insights from the plot:\nNon-smokers are charge less than smoker on average People that pay the most are smokers, while those that pay the least are non-smoker. 4. Distributions In this section, we will explore about histograms as well as the density plots. Histogram is a graph that shows the frequency of numerical data using rectangles while for a density plot, it represents the distribution of a numeric variable.\nHere\u0026rsquo;s an example for histogram:\nsns.histplot(irisi_data[\u0026#39;Petal Length\u0026#39;]) Histogram Plot\nFor density plot, we will use the kernel density estimate plot, it looks like a smoothed histogram. By changing the shade parameter, we can turn the display of the shaded region on and off.\nsns.kdeplot(data = iris_data[\u0026#39;Petal Length\u0026#39;], shade = True) KDE Plot\nHere\u0026rsquo;s the code to produce two-dimensional kernel density plot:\nsns.jointplot(x = irid_data[\u0026#39;Petal Length\u0026#39;], y = [\u0026#39;Sepal Width\u0026#39;], kind = \u0026#39;kde\u0026#39;) 2D KDE Plot\nFurthermore, to color-code the histogram of the kernel density estimate plot, simply add a hue parameter as below:\nsns.histplot(data = iris_data, x = \u0026#39;Petal Length\u0026#39;, hue = \u0026#39;Species\u0026#39;) sns.kdeplot(data = iris_data, x = \u0026#39;Petal Length\u0026#39;, hue \u0026#39;Species\u0026#39;, shade = True) Color-Coded Histogram Plot\nColor-Coded KDE Plot\n5. Plot Style There are several themes available in the seaborn module, you can set the style or theme of your plot before you start plotting:\nsns.set_style(\u0026#39;dark\u0026#39;) # your plot here # try other themes with: drakgrid, whitegrid, dark or white 6. Summary Here\u0026rsquo;s a summary of all the plot you have learned:\nType Category Code Remarks Line Chart Trend sns.lineplot() Show trends over time, multiple lines can be used to show trend in more than a group Bar Chart Relationship sns.barplot() Compare quantities with respect to different groups Heatmap Relationship sns.heatmap() Find color-coded patterns in tables of numbers Scatter Plot Relationship sns.scatterplot() Show relationship between two continuous variables Regression Line Relationship sns.regplot() See linear relationship between two variables Multi-Regression Line Relationship sns.lmplot() See linear relationship between two variables involving group Categorical Scatter Plot Relationship sns.swarmplot() Observe the relationship between continuous variable and categorical variable Histogram Distribution sns.histplot() Show distribution of single numerical variable KDE Plot Distribution sns.kdeplot() Show a smooth distribution of a single or more numerical variable 2D KDE Plot Distribution sns.jointplot(kind = 'kde') Display a 2D KDE plot with each KDE correspond to each variable ","permalink":"https://keanteng.github.io/home/docs/2023-08-15-data-visualization/","summary":"In this course, we will explore on data visualization using \u003ccode\u003eseaborn\u003c/code\u003e, a Python package to visualize data with a variety of plot types. The package is powerful yet easy to use, check out the below images on the plot types that \u003ccode\u003eseaborn\u003c/code\u003e is able to generate","title":"Data Visualization"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nIn this course, we will explore on the Python pandas module which is a popular library for data analysis. With pandas, we can use it to create data and also work or manipulate the existing data.\n1. Introduction pandas has two core objects known as DataFrame and Series. A DataFrame is a table where it consists of individual entries in the form of an array. Each entry corresponds to a row and a column. For example:\nLet us define a DataFrame, we can also write string in the DataFrame:\nimport pandas as pd pd.DataFrame({ \u0026#39;x\u0026#39;: [1,2,3], \u0026#39;y\u0026#39;: [3,2,1], }) | x | y | |---|---| | 1 | 3 | | 2 | 2 | | 3 | 1 | In the above code we are defining x and y as the column names, and we assigned values to the column. Now, let\u0026rsquo;s try to put an index or row labels to the DataFrame:\npd.DataFrame({ \u0026#39;x\u0026#39;: [1,2,3], \u0026#39;y\u0026#39;: [3,2,1]}, index = [\u0026#39;Row 1\u0026#39;, \u0026#39;Row 2\u0026#39;, \u0026#39;Row 3\u0026#39;] ) | | x | y | |-----|---|---| |Row 1| 1 | 3 | |Row 2| 2 | 2 | |Row 3| 3 | 1 | On the other hand, for Series, it represents a sequence of data values. Just now we have shown that DataFrame is in the form of table, but Series is in the form of a list. Let\u0026rsquo;s try defining one:\npd.Series([1,2,3,4]) We can think of a series as only a single column of a DataFrane. In fact, we can add row label or index to Series too:\npd.Series([1,2,3,4], index = [\u0026#39;Row 1\u0026#39;, \u0026#39;Row 2\u0026#39;, \u0026#39;Row 3\u0026#39;, \u0026#39;Row 4\u0026#39;], name = \u0026#34;My Series\u0026#34;) |Row 1| 1 | |Row 2| 2 | |Row 3| 3 | Name: Product A 1.1 Reading Data Files We can use pandas to read .csv file as well, by typing df = pd.read_csv('file.csv').\nTo check how large the DataFrame is:\ndf.shape() Here\u0026rsquo;s how we can take a preview of the first five rows of the DataFrame:\ndf.head() # try df.head(10) for first 10 rows You might be wondering, can we display only the last five rows? Of course:\ndf.tail() 2. Indexing, Selecting \u0026amp; Assigning We have learned how to create a Series and a DataFrame. Now let\u0026rsquo;s look at how do we select specific value in the DataFrame in a quick and effective manner.\nLet\u0026rsquo;s say we have a DataFrame called df with the following column names: country, points, price, province, state and so on. If we would like to only see the country column, we could access the column by doing this:\ndf.country # or alternatively df[\u0026#39;country\u0026#39;] Notice two approaches are provided above. Neither of them is more or less syntactically valid than the other. But let\u0026rsquo;s say we have a column name My House, we just could not write df.My House, it wouldn\u0026rsquo;t work.\nIf we want to know the first item in the country column, we can use the indexing operator to drill down to a single specific value:\ndf[\u0026#39;country\u0026#39;][0] 2.1 Indexing in Pandas In pandas, indexing works in one of two paradigms. We will start with the first one, the index-based selection where data is selected based on its numerical position in the data. To select the first row of a data:\ndf.iloc[0] For the loc and iloc function, row will come first then column.\ndf.iloc[:,0] # display all row of first column The : operator means everything. We can use it to indicate the range of values that we are interested in:\ndf.iloc[:3, 0] # first three rows of the first column (index 3 or fourth row excluded) df.iloc[1:3, 0] # second and third rows of the first column # alternatively df.iloc[[1,2], 0] Interestingly, we can also use negative number to select data items. For negative number, it will start counting from the end of the values:\ndf.iloc[-5:] # get the last five rows for all columns Furthermore, a second paradigm for attribute selection is done by using the loc operator, called as label-based selection. For this approach, it is the data index value, rather than the position that matters.\ndf.loc[0, \u0026#39;country\u0026#39;] # first item in the country column When we compare iloc and loc, iloc seems to be conceptually simpler than loc. This is because we ignore the dataset\u0026rsquo;s indices. Using iloc, dataset is treated like a big matrix, that we index into by position. For loc, we make use of the information in the indices to do its work. Of course, dataset usually comes with indices and this makes loc to be easier to work with.\ndf.loc[:, [\u0026#39;country\u0026#39;, \u0026#39;regions\u0026#39;, \u0026#39;states\u0026#39;]] # display all rows for the column of country, regions and states So, how do we choose between iloc and loc? For iloc, it uses the Python standard library indexing scheme, where the first element in the range is included, and the last one is excluded. But for loc, it will include both the first and last element. Of course, using iloc can be confusing sometimes. Here\u0026rsquo;s a comparison:\nUsing loc: 0:10 Output: 0,1,...,10 Using iloc 0:10 Output: 0,1,...,9 2.2 Manipulation Index When using label-based selection, the power of it comes from the labels in the index. We can manipulate index in any way we see fit as well by using set_index()\ndf.set_index(\u0026#34;country\u0026#34;) 2.3 Conditional Selection In order to do some interesting things with our dataset, we often need to ask questions based on certain conditions. Suppose that we want to know the better-than-average wine produced in Sweden, we can start by checking whether the wine is from Sweden or not:\nwine.country == \u0026#39;Sweden\u0026#39; # alternatively wine.loc[wine.country == \u0026#39;Sweden\u0026#39;] Now we would like to add an extra condition. We want the wine review to also be at least 90 points:\nwine.loc[(wine.country == \u0026#39;Sweden\u0026#39;) \u0026amp; (wine.points \u0026gt;= 90)] If we would like to know whether a wine is from Sweden or a wine is rated above average:\nwine.loc[(wine.country == \u0026#39;Sweden\u0026#39;) | (wine.points \u0026gt;= 90)] What if we want to know whether a wine is produced in Sweden or Italy?\nwine.loc[wine.country.isin([\u0026#39;Sweden\u0026#39;, \u0026#39;Italy\u0026#39;])] We notice that for the wine data produced in Sweden or Italy, there are some rows where the points given is empty, we would like to filter them out:\nwine.loc[wine.points.notnull()] # to check for the null rows wine.loc[wine.points.isnull()] 2.4 Assigning Data Now let\u0026rsquo;s look at the method to assign data to a DataFrame:\nwine[\u0026#39;critic\u0026#39;] = everyone # assingning a constant to the column named critic wine[\u0026#39;index_reverse\u0026#39;] = range(len(\u0026#39;country\u0026#39;),0,-1) 3. Summary Functions \u0026amp; Maps In the previous section, we learned about selecting relevant data from a DataFrame and a Series. However, data does not always come out in the format that we want, and often we would want to do some more works to reformat the data at hand. For starter, we could use the describe function to generate a high-level summary of the attributes of the given column:\nwine.describe() We can select the column that we want to know the summary:\nwinde.critic.describe() Notice that for numerical column, the describe function is telling use about the name, the maximum and the minimum value and a few other information. If we only want to know, says, the mean or the unique labels in a column, we could:\nwine.points.mean() wine.critic.unique() Let\u0026rsquo;s say we want to know for a list of unique value, how frequent they occur in the dataset:\nwine.critic.value_counts() 3.1 Maps Map is a mathematical term where it describes that a function takes one set of values and maps them to another set of value. Oftentimes, we have to create new representations from existing data or perform transformation to the data. Maps are extremely important to help us to achieve our works.\nAn example where we remean the scores the wines received to 0:\nwine_points_mean = wine.points.mean() wine_points_mean.map(lambda p : p - wine_points_mean) # alternative wine_points_mean = wine.points.mean() wine.points - wine_points_mean In the function above, map() expects a single value from the Series and return a transformed version of that value. A new series with all the transformed values will be returned by map().\nOn the other hand, we can use apply() which is also an equivalent method. Now we want to transform the whole DataFrame by calling a function on each row:\ndef remean(row): row.points = row.points - wine_points_mean return row wine.apply(remean, axis = \u0026#39;columns\u0026#39;) If we set the axis = index, then we would need to give a function to transform each column rather than each row.\nmap and apply do not modify the original data they\u0026rsquo;re call on. They return new, transformed series and DataFrame respectively.\nPandas understand what we do when we perform operations between Series of equal length. These operations are faster than using map or apply. Of course, we can use these functions with more advanced case like conditional logic.\nwine.country + \u0026#34; - \u0026#34; + wine_region 4. Grouping \u0026amp; Sorting 4.1 Groupwise Analysis In previous section, we learned about value_counts() to count for the unique labels\u0026rsquo; occurrences. Here\u0026rsquo;s an alternative:\nwine.groupby(\u0026#39;points\u0026#39;).points.count() This groupby function creates a group of reviews which allotted the same point values to the given wine. Then from these groups, we select the points column and count how many times they appear. For example, we want to get the cheapest wine in each point value category:\nwine.groupby(\u0026#39;points\u0026#39;).price.min() We can think of each group as a slice of our DataFrame with only data with values that match. This DataFrane is accessible to use directly using apply, and we can manipulate the data. If we want to select the name of the first wine reviewed from each winery:\nwine.groupby(\u0026#39;winery\u0026#39;).apply(lambda df : df.title.iloc[0]) We can also group more than one column. Let\u0026rsquo;s say we want to pick out the best wine by country and province:\nwine.groupby([\u0026#39;country\u0026#39;, \u0026#39;province\u0026#39;]).apply(lambda x : x.loc[x.points.idmax()]) Furthermore, if we want to run a bunch of different functions on the DataFrame at once, we can generate a simple statistical summary of the dataset with agg:\nwine.groupby([\u0026#39;country\u0026#39;]).price.agg([len, min, max]) 4.2 Multi-indexes A multi-index example:\ncountries_reviewed = wine.groupby([\u0026#39;country\u0026#39;, \u0026#39;province\u0026#39;]).description.agg([len]) countries_reviewed | country | province | len | |---------|------------------|-----| |Argentina| Mendoza Province | 3264| | | Other | 536 | |... | ... | | If we check the type:\nmi = countries_reviewed.index type(mi) # output # pandas.core.indexes.multi.MultiIndex There are a few ways to deal with this tiered structure which are absent for single-level indices. To retrieve a value, we need to provide two levels of labels. For the first method, we can convert the index back to the regular index:\ncountries_reviewed.reset_index() | | country | province | len | |-|---------|------------------|-----| |0|Argentina| Mendoza Province | 3264| |1|Argentina| Other | 536 | |2|... | ... | ... | 4.3. Sorting Previously, we learned about the grouping function to group data in index order. But what about in value order? Can we order the rows from the grouping result based on values in the data rather than index?\ncountries_reviewed = countries_reviewed.reset_index() countries_reviewed.sort_values(by = \u0026#39;len\u0026#39;) By default, the sorting will be in ascending order, we can change that by changing the ascending parameter:\ncountries_reviewed.sort_values(by = \u0026#39;len\u0026#39;, ascending = False) Moreover, if we just want to sort by index values, simply leave the by parameter blank:\ncountries_reviewed.sort_values() Of course, we could sort more than one column at a time:\ncountries_reviewed.sort_values(by = [\u0026#39;country\u0026#39;, \u0026#39;len\u0026#39;]) 5. Data Types \u0026amp; Missing Values In this section, we will explore how to investigate data types within a DataFrame and how to replace entries in it.\nwine.price.dtype() # for the specified column wine.dtype() # for all columns dtype function tells us how pandas store the data internally such as float64 or int64. We can convert a column from one type to the other too:\nwine.points.astype(\u0026#39;float64\u0026#39;) 5.1 Missing Value In a dataset, we might see values being given as NaN or \u0026ldquo;not a number\u0026rdquo;. Using Pandas, we can check for the missing data in a DataFrame:\nwine[pd.isnull(wine.country)] To replace the missing value in data, we can use the fillna() function:\nwine.region.fllna(\u0026#34;Unknown\u0026#34;) If we want to replace value, here\u0026rsquo;s what we can do:\nwine.twitter_handle.replace(\u0026#34;@x\u0026#34;, \u0026#34;@abc\u0026#34;) 6. Renaming \u0026amp; Combining rename is a function to let you change index names or column names:\nwine.rename(column = {\u0026#39;point\u0026#39;: \u0026#39;score\u0026#39;}) This function allows us to rename index or column by specifying an index or a column keyword parameter:\nwine.rename(index = {0: \u0026#39;first\u0026#39;, 1: \u0026#39;second\u0026#39;}) Of course, we could give name to a row index and the column index as well:\nwine.rename_axis(\u0026#39;wines\u0026#39;, axis = \u0026#39;rows\u0026#39;).rename_axis(\u0026#39;fields\u0026#39;, axis = \u0026#39;columns\u0026#39;) 6.1 Combining The simplest combination method in Pandas is concat. This function is useful when we have data in different DataFrame but having the same columns:\npd.concat([can_yt, bri_yt]) Moreover, for join, it will combine different DataFrame which have an index in common:\nleft = canadian_youtube.set_index([\u0026#39;title\u0026#39;, \u0026#39;trending_date\u0026#39;]) right = british_youtube.set_index([\u0026#39;title\u0026#39;, \u0026#39;trending_date\u0026#39;]) left.join(right, lsuffix=\u0026#39;_CAN\u0026#39;, rsuffix=\u0026#39;_UK\u0026#39;) For the above example, the lsuffix and rsuffix parameter is necessary because the data has the same columns names in both of the datasets. If it is not, then we could just ignore that.\n","permalink":"https://keanteng.github.io/home/docs/2023-08-18-pandas/","summary":"In this course, we will explore on the Python pandas module which is a popular library for data analysis. With pandas, we can use it to create data and also work or manipulate the existing data.","title":"Pandas"},{"content":" Images from Unsplash\nIn this project, I make use of Streamlit, which is an open-source Python library that allows us to build and deploy powerful apps with speed and efficiency. It also offers a cloud deployment feature for you to host the Streamlit app that you created online publicly through Streamlit Community Cloud.\nThe purpose of building the app is to get an understanding of the flood impacts towards collateral hold by banks. Floods are a frequent natural disaster that occurs in almost every state in Malaysia. The impact of floods extends beyond the destruction of homes, infrastructure and crops. Often, floods result in displacement of communities, disruptions to vital services like electricity, transportation and communication.\nTherefore, by collecting all the historical flood points in Malaysia and using sophisticated tool such as Google Earth Engine to estimate or forecast the flood extent using Sentinel-1 synthetic-aperture radar (SAR) data, a better understanding of the impact of the flood can be achieved, the economic impact of the flood can be evaluated, and the precautions towards future flood occurrences can be implemented.\nWith the forecasted flood data points as well as collateral location gathered, we can estimate the loss and devise strategies to minimize flood risk and economic loss.\nThe process of deploying the app is a series of workflow, as follows:\nFlood Data Collection from annual report published by Department of Drainage and Irrigation Flood data points geocoding using geocoding services: Nominatim App Building (Flood Statistics Visualization, Market Cluster/Heatmap) via Local Deployment Flood Extent Mapping Feature on Streamlit Cloud Webapp Overview\nRefer to my GitHub repository, for my work on this project, the link to the web app can be found on the landing page.\n1. Flood Data Collection The data used to visualize the flood incidents in Malaysia from 2015 to 2021 can be collected by the annual report published by the Department of Irrigation and Drainage, JPS. Due to no flood data files available online, the flood data could only be extracted by establishing data connection using Power Query on Excel. Moreover, the tables in the report has inconsistent format and typing, a series of process would need to be implemented to clean up the data set:\nData Cleaning Workflow\nThe second issue to overcome would be the huge amount of .xlsx files to merge assuming you created each file for flood incidents in each state and each year.\nTOTAL STATES = 15 (13 States and 2 Federal Territories) TOTAL YEAR = 7 AMOUNT OF FILES = 15 * 7 = 105 We can use Python openpyxl packages to quickly merge all these files with the following code:\n# importing the required modules import glob import pandas as pd # specifying the path to csv files path = folder # csv files in the path file_list = glob.glob(path + \u0026#34;/*.xlsx\u0026#34;) # list of excel files we want to merge. excl_list = [] for file in file_list: excl_list.append(pd.read_excel(file, sheet_name= \u0026#39;Sheet1\u0026#39;)) # concatenate all DataFrames in the list # into a single DataFrame, returns new # DataFrame. excl_merged = pd.concat(excl_list, ignore_index=True) # exports the dataframe into excel file # with specified name. excl_merged.to_excel(\u0026#39;FILENAME_WITH_ALL_THE_DATA.xlsx\u0026#39;, index=False) 2. Flood Data Points Geocoding To convert all the flood location addresses into coordinates identified by longitude and latitude, geocoding services such as Nominatim are used. In the Python âgeopyâ module, it includes the Nominatim API services for such a conversion. A geocoding program is written by me in Python to perform the conversion. The program runs for 146 minutes with a success rate of about 31%. That means, only about 5000 addresses are successfully geocoded.\nSeveral runs of the program are performed to check for improvement in success rate. No significant improvement is observed throughout all the runs due to Nominatim unable to locate a certain address in Malaysia.\n# geocoding function def my_geocoder(row): try: point = geolocator.geocode(row).point return pd.Series({\u0026#39;Latitude\u0026#39;: point.latitude, \u0026#39;Longitude\u0026#39;: point.longitude}) except: return pd.Series({\u0026#39;Latitude\u0026#39;: None, \u0026#39;Longitude\u0026#39;: None}) data[[\u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;]] = data.apply(lambda x: my_geocoder(x[\u0026#39;Name\u0026#39;]), axis=1) # check the percentage of data successfully geocoded print(\u0026#34;{}% of addresses were geocoded!\u0026#34;.format( (1 - sum(np.isnan(data[\u0026#34;Latitude\u0026#34;])) / len(data)) * 100)) 3. App Building via Local Deployment To use Streamlit, we have to install it first py -m pip install streamlit on Windows terminal. You can check out its documentation to see the list of components that can be used to build interactive and powerful web app.\nTo run the app you have created, simply type py -m streamlit run filename.py on terminal. You will deploy the app locally.\nIn my app, I included bar charts using plotly package to visualize the flood statistics that I gathered. Heatmap and Marker Cluster map were also plotted to indicated the location of flood and the areas in a particular state that experienced a higher density of flood incidents. The relevant code can be found in my Git repository.\nMarker Cluster Plot\nHeatmap\n4. Adding Additional Feature via Cloud Deployment In my project, I added some tools for flood extent analysis and features supported by Google Earth Engine using resources from mapaction/flood mapping tool and opengeos/streamlit-geospatial.\nSelecting the area for prediction\nPrediction outcome\nThe reason these features need to be added via cloud deployment of Streamlit is because of a missing package fcntl module that is not available on my Windows machine but on Linux system. Of course, after deploying these feature to cloud, authentication token from Google Earth Engine would also be required. To get the key, got to Windows Terminal:\npy -m pip install ee import ee ee.Authenticate() You will need to paste the authorization code back on the terminal. Once the step is complete, you can find the token on your local machine at C:\\\\Users\\\\Username\\\\.congif\\\\earthengine\\\\credentials. It is also important to note that the token at C:\\\\Users\\\\Username\\\\.congif\\\\earthengine\\\\credentials would need to be put at the secret section of the app that you deployed on cloud using the following format:\nEARTHENGINE_TOKEN = \u0026#39;PASTE WHAT YOU COPY HERE\u0026#39; ee_keys = \u0026#39;PASTE WHAT YOU COPY HERE\u0026#39; The secret section can be found by clicking the app setting \u0026gt; secret.\nAfter that, the app is ready to be shared and used!\n","permalink":"https://keanteng.github.io/home/docs/2023-08-13-a-streamit-app-for-flood-analysis/","summary":"In this project, I make use of Streamlit, which is an open-source Python library that allows us to build and deploy powerful apps with speed and efficiency. It also offers a cloud deployment feature for you to host the Streamlit app that you created online publicly through Streamlit Community Cloud.","title":"A Streamlit App For Flood Analysis"},{"content":" Images from Unsplash\nIn econometrics, the ordinary least square (OLS) model is widely used to estimate the parameter of a linear regression model.\ny = beta0 + beta1*x + epsilon Of course, the model can be used to model risk such as flood risk as well. From the equation above, we let y represents the flood risk. y would be a continuous variable, like a flood risk score. It also could be represented as a binary outcome: 0 for no risk and 1 for risk present. The x is the distance to the historical flood location, it is the distance to the nearest historical flood location. For 0, it is the intercept, it shows the estimated flood risk when x is 0 or when the location is exactly the historical flood site. 1, represents the slope. It is expected to be negative as greater distances from historical flood sites correspond to lower flood risks. For the error term, epsilon, it captures the unobserved factors that affect flood risk but are not included in the model.\nThe outcome from the model will be presented on Streamlit web app together with a flood prediction service page to receive user location input and return the flood risk on the input location.\nThe Streamlit Web App\nRefer to my GitHub repository, for my work on this project, the link to the web app can be found on the landing page.\n1. The Workflow Random address generation using web scraping Geocoding, computing distance and response variable Model fitting and evaluation Models comparison Creating and deploying web app 2. Random Address Generation Using Web Scraping Fitting of the flood risk model requires a predictor and a response variable. The predictor variable is collected by computing the minimum distance between a random location and the nearest historical flood location. To obtain the random location, Python selenium module is used to scrape random addresses online by locating the Xpath components on web pages. About 16, 000 random locations are scraped in 3 hours.\nThe data collected contains information such as state, region, country, addresses, zip code and phone number.\nWeb Scraping In Action\nRefer to my GitHub for the corresponding notebook.\n3. Geocoding, Computing Distance And Response Variable Geocoding services like Nominatim is used to convert the random addresses generated into geographic coordinate with longitude and latitude.\n# geocoding function def my_geocoder(row): try: point = geolocator.geocode(row).point return pd.Series({\u0026#39;Latitude\u0026#39;: point.latitude, \u0026#39;Longitude\u0026#39;: point.longitude}) except: return pd.Series({\u0026#39;Latitude\u0026#39;: None, \u0026#39;Longitude\u0026#39;: None}) data[[\u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;]] = data.apply(lambda x: my_geocoder(x[\u0026#39;Location\u0026#39;]), axis=1) # check the percentage of data successfully geocoded print(\u0026#34;{}% of addresses were geocoded!\u0026#34;.format( (1 - sum(np.isnan(data[\u0026#34;Latitude\u0026#34;])) / len(data)) * 100)) To compute for the minimum distance between any random location with the nearest historical flood location, geopandas module is required. Since initially, the coordinate is in the format of EPSG: 4326, to ensure that we obtain distance output in meter, we would need to convert the coordinate to the format of EPSG: 3857:\ngeocoded_data = gpd.GeoDataFrame(geocoded, geometry = gpd.points_from_xy(geocoded.Longitude, geocoded.Latitude), crs = \u0026#34;EPSG:4326\u0026#34;).to_crs(\u0026#39;EPSG:3857\u0026#39;) flood_points_data = gpd.GeoDataFrame(flood_points, geometry = gpd.points_from_xy(flood_points.Longitude, flood_points.Latitude), crs = \u0026#34;EPSG:4326\u0026#34;).to_crs(\u0026#39;EPSG:3857\u0026#39;) for i in range(0, len(geocoded_data)): distances = flood_points_data.geometry.distance(geocoded_data.iloc[i].geometry) geocoded_data.loc[i, \u0026#39;distance\u0026#39;] = distances.min() Subsequently, the response variable is computed by assuming no flood risk if the distance is less than 500 m, otherwise, flood risk exists.\n4. Model Fitting And Evaluation The flood risk model is fitted using LogisticRegression() from the scikit module. The model requires at least two dimensions predictor variables. Thus, One-Hot encoding is applied to the state column before model fitting taking place.\nOne Hot Encoding Illustration\n# train test split X = data[[\u0026#39;distance\u0026#39;,\u0026#39;state\u0026#39;]] y = data[\u0026#39;flood_risk\u0026#39;] X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0) X_train.shape, X_valid.shape, y_train.shape, y_valid.shape # Apply one-hot encoder to each column with categorical data OH_encoder = OneHotEncoder(handle_unknown=\u0026#39;ignore\u0026#39;, sparse_output=False) # ori: spare = False # Get list of categorical variables s = (X_train.dtypes == \u0026#39;object\u0026#39;) object_cols = list(s[s].index) OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols])) OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols])) # One-hot encoding removed index; put it back OH_cols_train.index = X_train.index OH_cols_valid.index = X_valid.index # Remove categorical columns (will replace with one-hot encoding) num_X_train = X_train.drop(object_cols, axis=1) num_X_valid = X_valid.drop(object_cols, axis=1) # Add one-hot encoded columns to numerical features OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1) OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1) 4.1 Model Evaluation The model is evaluated based on its accuracy, training and test score.\n# print the scores on training and test set print(\u0026#39;Training set score: {:.4f}\u0026#39;.format(logreg.score(OH_X_train, y_train))) print(\u0026#39;Test set score: {:.4f}\u0026#39;.format(logreg.score(OH_X_valid, y_valid))) The model is able to achieve an accuracy of 0.9880, which is remarkably accurate. The model achieved an accuracy score of 0.9880, training set accuracy of 0.9928. Furthermore, the training set score and the test set score are very close to each other, which means that the model is not overfitting.\nTest Set Outcome\nGenerally, training score measures how to model fit in the training data. If a model fits so well in a data with lots of variance it results in overfitting. This will result in a poor test score. The mode curved a lot to fit the training data and generalized very poorly. For test score, since we implement train-test split before fitting the model, it represents a real life scenario. Thus, the higher the test score, the better.\nThe model is then further evaluated with confusion matrix, training and test set score, receiver operating curve (ROC) as well as cross-validation. Of course, comparisons were made with other models such as KNN, SVM, XGB and more. However, logistic regression is chosen as it is able to achieve a high accuracy score with only slightly poor performance compared to XGB classifier (negligible differences).\nMulti-models Comparison\nFrom the confusion matrix below, it can be observed that the true positive is 251, true negative is 575 while the false negative is 8 and the false positive is 2. These results show that our model performs very well in predicting the flood risk of a location in Malaysia. Furthermore, we compute the accuracy (0.9880), classification error (0.0120), precision (0.9921) and sensitivity (0.9691).\n# confusion matrix from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay cm = confusion_matrix(y_valid, y_pred_test) # visualize confusion matrix with seaborn heatmap disp = ConfusionMatrixDisplay(confusion_matrix=cm) disp.plot() Confusion Matrix\nPrecision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). Precision identifies the proportion of correctly predicted positive outcomes. It is more concerned with the positive class than the negative class. Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). Recall is also called Sensitivity.\nfrom sklearn.metrics import classification_report print(classification_report(y_valid, y_pred_test)) ROC curve stands for receiver operating characteristic curve. It shows the performance of classification models at various classification threshold levels. The curve also shows the performance of a classification model at various classification threshold levels. From the image below, we can see that the area under the curve is closed to one. Perfect classifier will have AUC equal to 1 whereas a random classifier will have a ROC-AUC equal to 0.5. Since our model ROC-AUC approximates to 1, we can conclude that our classifier does a good job in predicting whether a location has a flood risk or no flood risk.\nfrom sklearn.metrics import roc_auc_score from sklearn.metrics import roc_curve logit_roc_auc = roc_auc_score(y_valid, logreg.predict(OH_X_valid)) fpr, tpr, thresholds = roc_curve(y_valid, logreg.predict_proba(OH_X_valid)[:,1]) plt.figure() plt.plot(fpr, tpr, label=\u0026#39;Logistic Regression (area = %0.2f)\u0026#39; % logit_roc_auc) plt.plot([0, 1], [0, 1],\u0026#39;r--\u0026#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.title(\u0026#39;Receiver operating characteristic\u0026#39;) plt.legend(loc=\u0026#34;lower right\u0026#34;) plt.savefig(\u0026#39;images/Log_ROC.png\u0026#39;) ROC Curve\n5. Creating And Deploying Web App With the fitted model, a Streamlit web app is created to present my workflows, analysis and findings. A flood prediction service is added to predict flood risk based on user location input. By typing a location in the input box, a function will be called to compute the distance of the input and the nearest flood data points. The distance computed is then passed to the logistic function for flood risk computation. The flood prediction service works based on the logistic function used for the study. After all the computation, an output in 0 or 1 will be returned to the user.\nFlood Risk Prediction Service Page On Streamlit Web App\n","permalink":"https://keanteng.github.io/home/docs/2023-08-13-flood-risk-modeling/","summary":"In econometrics, the ordinary least square (OLS) model is widely used to estimate the parameter of a linear regression model.","title":"Flood Risk Modeling With Logistic Regression"},{"content":" Images from Unsplash\nGoogle Sheet importxml() function is a wonderful tool to allow you to scrape for website information by just identifying the corresponding Xpath. But it has a limit, when you have hundreds or thousands of data items to be gathered, your Sheet\u0026rsquo;s cell will get stuck at endless loading. As you can see from the image below, for about 250 rows of data, after waiting for more than 30 minutes, the loading cell does not refresh.\nGoogle Sheet Data Extraction Using IMPORTXML()\nLuckily, the Python selenium package offers a powerful solution to this issue. With just a few lines of code, you can scrape hundreds or even thousands rows of data and compile them into a Python data frame for you to perform further analysis, with speed and efficiency.\n1. What is Selenium Selenium is a project that provides a range of tools and libraries to support the automation of web browsers. It supports extensions to emulate user interaction with browsers, a distribution of servers for scaling browser allocation and allows you to write interchangeable code for all major web browsers.\nYou can install the package by writing this code on Windows Terminal:\npy -m pip install selenium 2. Finding Xpath Previously, I have shown the Random Malaysia Address on Google Sheet, where IMPORTXML() is used to scrape for random addresses. The website used is here.\nSite For Random Address Generation\nThis site allows user to generate 20 random addresses at once and the addresses are shown in several blue boxes as shown in the above images. In order to locate the addresses, or in other words, all the information inside each blue boxes, we would need to locate their Xpath. The Xpath can be found by opening the developer mode on your web browser by pressing Ctrl + Shift + I and search for the required element. Once you have found the required element, right click and select Copy Full Xpath. The Xpath is as follows:\n/html/body/section[2]/div/div[2]/div[2]/ul/li[1] As mentioned earlier, there are 20 random addresses generated. Thus, notice that the * in /html/body/section[2]/div/div[2]/div[2]/ul/li[*], it will be numbered from 1 to 20, representing 20 different boxes containing random addresses.\nFinding Site Element Xpath\n3. Web Scraping In Action Everything is easy after knowing the Xpath of the web page element you want. We will start by importing the required package:\nfrom selenium import webdriver from selenium.webdriver.common.by import By import time import pandas as pd import random Then, we need to write a function to extract the random address based on the Xpath we copied. We will write a get_address function for that.\ndef get_address(url): driver = webdriver.Chrome() driver.get(url) time.sleep(2) container1 = driver.find_element(By.XPATH, \u0026#34;/html/body/section[2]/div/div[2]/div[2]/ul/li[1]\u0026#34;) driver.close() return container1.text But the output will be in this format:\n//output: Street: G Kenyalang Shopping Centre 6D Jln Datuk Sim Kheng Hong Kenyalang Park Kuching Ma City: Kuching State/province/area: Sarawak Phone number 08233-1761 Zip code 93300 Country calling code +60 Country Malaysia //What we want: Street: G Kenyalang Shopping Centre 6D Jln Datuk Sim Kheng Hong Kenyalang Park Kuching Ma; City: Kuching; State/province/area: Sarawak; Phone number 08233-1761; Zip code 93300; Country calling code +60; Country Malaysia The output makes us hard to put in a data frame. Therefore, we need an additional function called textline to convert the output to one string as shown above:\ndef get_address(url): driver = webdriver.Chrome() driver.get(url) time.sleep(2) container1 = driver.find_element(By.XPATH, \u0026#34;/html/body/section[2]/div/div[2]/div[2]/ul/li[1]\u0026#34;) container2 = driver.find_element(By.XPATH, \u0026#34;/html/body/section[2]/div/div[2]/div[2]/ul/li[2]\u0026#34;) def textLine(poem): lst=list(poem) string=\u0026#39;\u0026#39; for i in lst: string+=i # print(string) lst1=string.split(\u0026#34;\\n\u0026#34;) str1=\u0026#34;\u0026#34; for i in lst1: str1+=i+\u0026#34; ;\u0026#34; str2=str1[:-2] return str2 location1 = textLine(container1.text) location2 = textLine(container2.text) driver.close() temp = [location1, location2] return temp Now, we can put the output into a Python data frame:\nurl = https://www.bestrandoms.com/random-address-in-my?quantity=20 df = pd.DataFrame({\u0026#39;address\u0026#39;:get_address(url)}) And we are done! You can also write a loop if you need more random locations as each function can return at most 20 random locations.\n4. Scrape More Than 1000 Locations Check out my GitHub for the full Jupyter notebook to scrape more than 1000 random locations with a .csv output at the end.\n","permalink":"https://keanteng.github.io/home/docs/2023-08-13-webscraping-on-xpath/","summary":"Google Sheet IMPORTXML() function is a wonderful tool to allow you to scrape for website information by just identifying the corresponding Xpath. But it has a limit, when you have hundreds or thousands of data items to be gathered, your Sheet\u0026rsquo;s cell will get stuck at endless loading.","title":"Webscraping On Xpath"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nGeospatial analysis is the gathering, display and manipulation of imagery, GPS, satellite photography and historical data, described explicitly in terms of geographic coordinates. This course will learn on methods to visualize geospatial data and perform some analysis concerning a particular geographic location or region.\nSome interesting questions that can be addressed with geospatial analysis are:\nWhich areas affected by earthquakes would require additional reinforcement? Where should a popular coffee shop select its next store, if it is considering an expansion? With forest conservation areas set up in some regions, will animals migrate to those areas or other areas instead? 1. Creating Maps To visualize geographic coordinates as a map, we need the help of geopandas library. Note that there are a few geospatial file formats available such as shapefile, GeoJSON, KML and GPKG. But all the files can be loaded with geopandaslibrary:\n# read the shape file full_data = gpd.read_file(\u0026#34;file_name\u0026#34;) Here\u0026rsquo;s how we can create a map for a geospatial data file. In fact, in every GeoDataFrame, there will be a geometry column that describe the geometric objects when we display them with the plot() function. They can be a point, linestring or polygon :\n## 1. Prework # plot the data only wild_lands.plot() # campsites point POI_data = gpd.read_file(\u0026#34;../input/geospatial-learn-course-data/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp\u0026#34;) campsites = POI_data.loc[POI_data.ASSET==\u0026#39;PRIMITIVE CAMPSITE\u0026#39;].copy() # foot trails as linestring roads_trails = gpd.read_file(\u0026#34;../input/geospatial-learn-course-data/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp\u0026#34;) trails = roads_trails.loc[roads_trails.ASSET==\u0026#39;FOOT TRAIL\u0026#39;].copy() # country boundaris as polygon counties = gpd.read_file(\u0026#34;../input/geospatial-learn-course-data/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp\u0026#34;) ## 2. Visualize the map # Plot a base map with counties boundaries ax = counties.plot(figsize=(10,10), color=\u0026#39;none\u0026#39;, edgecolor=\u0026#39;gainsboro\u0026#39;, zorder=3) # Add in the campsites and foot trails wild_lands.plot(color=\u0026#39;lightgreen\u0026#39;, ax=ax) campsites.plot(color=\u0026#39;maroon\u0026#39;, markersize=2, ax=ax) trails.plot(color=\u0026#39;black\u0026#39;, markersize=1, ax=ax) Point, linestring and polygon (geometric objects)\n2. Coordinate Reference System In order to create a GeoDataFrame, we have to set the CRS. The CRS is referenced by European Petroleum Survey Group code - EPSG 32630 used by GeoDataFrame or also known as the \u0026ldquo;Mercator\u0026rdquo; projection that preserves angles and slightly distorts area. What\u0026rsquo;s more, EPSG 4326 corresponds to coordinates in latitude and longitude. It is a coordinate system of latitude and longitude based on an ellipsoidal (squashed sphere) model of the earth.\nHere\u0026rsquo;s how to do it in code:\n# read file facilities_df = pd. read_csv(\u0026#34;file_name\u0026#34;) # convert to geodataframe facilities = gpd.GeoDataFrames(facilities_df, geometry = points_from_xy(facilities_df.Longitude, facilities_df.Latitiude)) # set crs facilities.crs = {\u0026#39;init\u0026#39;: \u0026#39;epsg:4326\u0026#39;} # view first five rows facilities.head() It is also possible to change the CRS so that we can have datasets with matching CRS. If we cannot do it with code, alternatively, we can use proj4 string of CRS to convert to latitude and longitude coordinates using +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs:\n# match facilities crs with regions ax = regions.plot(figsize=(8,8), color=\u0026#39;whitesmoke\u0026#39;, linestyle=\u0026#39;:\u0026#39;, edgecolor=\u0026#39;black\u0026#39;) facilities.to_crs(epsg=32630).plot(markersize=1, ax=ax) # change CRS to EPSG 4326 and display the data regions.to_crs(\u0026#34;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\u0026#34;).head() 2.1 Geometric Objects Attributes Previously, we introduced the geometry column in a GeoDataFrame, in fact they are built-in attributes that could help to give us some interesting information about our data. For example, we want to find the coordinates of each point, the length of linestring or the area of a polygon.\n# find points x-coordinate facilities.geometry.head().x # find area of polygons regions.loc[:, \u0026#34;AREA\u0026#34;] = regions.geometry.area/ 10**6 3. Interactive Maps In this section, we will explore methods to plot interactive maps such as heatmaps, points and choropleth maps. These features are enabled by the folium package.\nSome maps to plot with folium package:\nSimple Map Markers/ Bubbles Clustered markers Heatmaps Choropleth maps To create a simple map for visualization as follows:\n# Create a map m_1 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;openstreetmap\u0026#39;, zoom_start=10) # Display the map m_1 A simple map\nLet\u0026rsquo;s say we want to add some markers to the map. Here\u0026rsquo;s a case where we add markers to denote places that experienced robbery on the map:\n# data preparatin daytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == \u0026#39;Robbery\u0026#39;) \u0026amp; \\ (crimes.HOUR.isin(range(9,18))))] # Create a map m_2 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;cartodbpositron\u0026#39;, zoom_start=13) # Add points to the map for idx, row in daytime_robberies.iterrows(): Marker([row[\u0026#39;Lat\u0026#39;], row[\u0026#39;Long\u0026#39;]]).add_to(m_2) # Display the map m_2 Maps with markers\nNow notice that the markers are scattered all over the place. It is possible to cluster together the markers when we zoom out the map and let the markers spread out as we zoom in with the help of MarkerCluster():\n# Create the map m_3 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;cartodbpositron\u0026#39;, zoom_start=13) # Add points to the map mc = MarkerCluster() for idx, row in daytime_robberies.iterrows(): if not math.isnan(row[\u0026#39;Long\u0026#39;]) and not math.isnan(row[\u0026#39;Lat\u0026#39;]): mc.add_child(Marker([row[\u0026#39;Lat\u0026#39;], row[\u0026#39;Long\u0026#39;]])) m_3.add_child(mc) # Display the map m_3 Clustered markers\nAn alternative to markers on map, we could also use circle for the same purpose - that is bubble maps:\n# Create a base map m_4 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;cartodbpositron\u0026#39;, zoom_start=13) def color_producer(val): if val \u0026lt;= 12: return \u0026#39;forestgreen\u0026#39; else: return \u0026#39;darkred\u0026#39; # Add a bubble map to the base map for i in range(0,len(daytime_robberies)): Circle( location=[daytime_robberies.iloc[i][\u0026#39;Lat\u0026#39;], daytime_robberies.iloc[i][\u0026#39;Long\u0026#39;]], radius=20, color=color_producer(daytime_robberies.iloc[i][\u0026#39;HOUR\u0026#39;])).add_to(m_4) # Display the map m_4 Bubble map\nNow consider that among few cities with different crime rate. We would like to visualize whether which city has relatively more criminal incidents than the other, a heatmap would do a good job to show us which areas of a city are susceptible to more criminal cases:\n# Create a base map m_5 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;cartodbpositron\u0026#39;, zoom_start=12) # Add a heatmap to the base map HeatMap(data=crimes[[\u0026#39;Lat\u0026#39;, \u0026#39;Long\u0026#39;]], radius=10).add_to(m_5) # Display the map m_5 Heat map\nWell, you should notice that heatmap makes geographic boundaries between different areas non-distinguishable. We can also use choropleth maps instead to visualize the crime rate by district.\n# GeoDataFrame with geographical boundaries of Boston police districts districts_full = gpd.read_file(\u0026#39;../input/geospatial-learn-course-data/Police_Districts/Police_Districts/Police_Districts.shp\u0026#39;) districts = districts_full[[\u0026#34;DISTRICT\u0026#34;, \u0026#34;geometry\u0026#34;]].set_index(\u0026#34;DISTRICT\u0026#34;) districts.head() # Number of crimes in each police district plot_dict = crimes.DISTRICT.value_counts() plot_dict.head() # Create a base map m_6 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;cartodbpositron\u0026#39;, zoom_start=12) # Add a choropleth map to the base map Choropleth(geo_data=districts.__geo_interface__, data=plot_dict, key_on=\u0026#34;feature.id\u0026#34;, fill_color=\u0026#39;YlGnBu\u0026#39;, legend_name=\u0026#39;Major criminal incidents (Jan-Aug 2018)\u0026#39; ).add_to(m_6) # Display the map m_6 Choropleth map\n4. Manipulating Geospatial Data When we are using application such a Google Maps, we could easily get a place location on the map by just knowing the address or name. In fact, we are using what it\u0026rsquo;s known as geocoder to generate locations of the places that we want to go.\nHere\u0026rsquo;s an interesting example where we try to geocode 100 top universities in Europe with only the universities name:\n# 1. prepare a dataset with the universitites name universities = pd.read_csv(\u0026#34;../input/geospatial-learn-course-data/top_universities.csv\u0026#34;) universities.head() # 2. apply geocode to each of the universitites def my_geocoder(row): try: point = geolocator.geocode(row).point return pd.Series({\u0026#39;Latitude\u0026#39;: point.latitude, \u0026#39;Longitude\u0026#39;: point.longitude}) except: return None universities[[\u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;]] = universities.apply(lambda x: my_geocoder(x[\u0026#39;Name\u0026#39;]), axis=1) print(\u0026#34;{}% of addresses were geocoded!\u0026#34;.format( (1 - sum(np.isnan(universities[\u0026#34;Latitude\u0026#34;])) / len(universities)) * 100)) # Drop universities that were not successfully geocoded universities = universities.loc[~np.isnan(universities[\u0026#34;Latitude\u0026#34;])] universities = gpd.GeoDataFrame( universities, geometry=gpd.points_from_xy(universities.Longitude, universities.Latitude)) universities.crs = {\u0026#39;init\u0026#39;: \u0026#39;epsg:4326\u0026#39;} universities.head() Now let\u0026rsquo;s plot out the locations to see if they are accurate:\n# Create a map m = folium.Map(location=[54, 15], tiles=\u0026#39;openstreetmap\u0026#39;, zoom_start=2) # Add points to the map for idx, row in universities.iterrows(): Marker([row[\u0026#39;Latitude\u0026#39;], row[\u0026#39;Longitude\u0026#39;]], popup=row[\u0026#39;Name\u0026#39;]).add_to(m) # Display the map m European universitites\n4.1 Table Joins In this section, we will explore on combining data frames with shared index for the case of GeoDataFrame. An example is that we have a dataset with boundaries of every country in Europe and a dataset with their estimated population and GDP, we can perform attribute join to merge the two datasets:\neurope = europe_boundaries.merge(europe_stats, on=\u0026#34;name\u0026#34;) europe.head() Furthermore, it is also possible to merge GeoDataFrame based on spatial relationship between objects in geometry columns. Recall back that we geocode top 100 universities in Europe previously. Can we match each university with its corresponding country? Spatial join allow us to perform match for such as scenario.\nThe spatial join above looks at the \u0026ldquo;geometry\u0026rdquo; columns in both GeoDataFrames. If a Point object from the universities GeoDataFrame intersects a Polygon object from the europe DataFrame, the corresponding rows are combined and added as a single row of the european_universities DataFrame. Otherwise, countries without a matching university (and universities without a matching country) are omitted from the results.\n# Use spatial join to match universities to countries in Europe european_universities = gpd.sjoin(universities, europe) # Investigate the result print(\u0026#34;We located {} universities.\u0026#34;.format(len(universities))) print(\u0026#34;Only {} of the universities were located in Europe (in {} different countries).\u0026#34;.format( len(european_universities), len(european_universities.name.unique()))) european_universities.head() 5. Proximity Analysis For the previous four sections, we are exposed to a lot of the functions in geopandas. Here, we will look into some application areas with the learned functions:\nSome useful application:\nMeasuring distance between points on a map Select points within some radius of a feature To compute distances from two GeoDataFrames, we need to make sure they have the same CRS. The distances can be easily computed in GeoPandas. Moreover, we can find the mean distance between two points with mean too. Here\u0026rsquo;s an example where we deal with dataset with air quality monitoring stations in the same city where we would like to know the mean distance from one monitoring station to the other:\n# check CRS of geodataframes print(stations.crs) print(releases.crs) # Select one release incident in particular recent_release = releases.iloc[360] # Measure distance from release to each station distances = stations.geometry.distance(recent_release.geometry) distances # find the mean distance print(\u0026#39;Mean distance to monitoring stations: {} feet\u0026#39;.format(distances.mean())) 5.1 Creating a Buffer The purpose of creating a buffer is for us to understand points on a map that lies some radius away from a point. For example, there\u0026rsquo;s a toxic gas being release accidentally to the air. There are some air quality monitoring centers nearby. We want to know whether those centers are able to detect the toxic gas.\nA working example would be as follows:\n# 1. creating a two miles buffer two_mile_buffer = stations.geometry.buffer(2*5280) two_mile_buffer.head() # 2. create map with release incidents and monitoring stations m = folium.Map(location=[39.9526,-75.1652], zoom_start=11) HeatMap(data=releases[[\u0026#39;LATITUDE\u0026#39;, \u0026#39;LONGITUDE\u0026#39;]], radius=15).add_to(m) for idx, row in stations.iterrows(): Marker([row[\u0026#39;LATITUDE\u0026#39;], row[\u0026#39;LONGITUDE\u0026#39;]]).add_to(m) # Plot each polygon on the map GeoJson(two_mile_buffer.to_crs(epsg=4326)).add_to(m) # Show the map m Now we want to check if a toxic release occurred within 2 miles of any monitoring station, to do that we would need to test each polygon. But that would be tedious work. Consider combining all the polygons into one object, we can check whether the toxic gas is within the radar of the closest monitoring station:\n# check if output is true my_union.contains(releases.iloc[360].geometry) ","permalink":"https://keanteng.github.io/home/docs/2023-04-19-geospatial-analysis/","summary":"Geospatial analysis is the gathering, display and manipulation of imagery, GPS, satellite photography and historical data, described explicitly in terms of geographic coordinates. This course will learn on methods to visualize geospatial data and perform some analysis concerning a particular geographic location or region.","title":"Geospatial Analysis"},{"content":" Images from Unsplash\nThis article aims to provide a comprehensive overview of how this site is set up and run.\nPre-requisite There are a few things to prepare before your own site can be created, as follows:\nA GitHub account Installed Microsoft Visual Studio Code Installed Git After you create a GitHub account and installing the necessary software, we are ready to begin!\nWebsite Foundations Setup The site that we are going to create will be based on Hugo, an open-source site generators where it provides a framework for us to deploy a site with speed and ease.\nTo build our website, we will need to install Hugo into our local machine. The installation process will be done on Windows Terminal with the support of Go language. Of course, you can also use other language such as chocolatey, scoop and winget.\nAfter installing go, here\u0026rsquo;s how to install Hugo on terminal:\ngo install -tags extended github.com/gohugoio/hugo@latest hugo version # check if you are using the latest version With Hugo installed in our local system, we would like to now create the foundation or framework for our website. Here I encourage you to change your directory first, preferably to \\Desktop so that you can access all your files easily:\ncd C:\\Users\\Username\\Desktop hugo new site \u0026lt;your_site_name\u0026gt; -f yml Now change your directory again and create a page on your website:\ncd \u0026lt;your_site_name\u0026gt; hugo new docs/page.md Website Theme Setup With the site foundation ready, we now add a theme or a specific design to our site for better functionality and appearance. You can check out different theme here, but in my case we will proceed with the PaperMode theme.\nHere we will use git to install the website theme:\ngit init git clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod After that, navigate to GitHub to create an empty repository and head back to terminal where we need to link up our files with the repository created. Since we are using an empty repository, we need to create a first file, usually README.md to avoid causing any error:\necho \u0026#34;# Test\u0026#34; \u0026gt;\u0026gt; README.md git add README.md git commit -m \u0026#34;Initial commit\u0026#34; git branch -M main git remote add origin https://github.com/YOUR-GIT-NAME/REPOSITORY-NAME.git git push -u origin main If you want to see how your website look like, you can deploy your site locally using Hugo. This is a good practice to check for error and website update before deploy your site publicly. Simply click the link or type 127.0.0.1 on your web browser:\nhugo server Website Deployment Workflow Just now your were shown to deploy the site locally, now to do it publicly - meaning on the web, you need the support from GitHub workflow.\nHere we will need to create an additional directory and put in some codes into it:\nmkdir -p .github/workflows After creating this directory, create a file with name deploy.yml in the workflow folder. Then navigate these two files (your can use file explorer) and put in these codes:\nconfig.yml After copying, change the first line baseurl: to the following format \u0026quot;https://YOUR-NAME-ON-GITHUB.github.io/REPOSITORY-NAME/\u0026quot; copy the code from here (https://github.com/adityatelange/hugo-PaperMod/blob/exampleSite/config.yml) deploy.yml name: Publish to GH Pages on: push: branches: - main pull_request: jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout source uses: actions/checkout@v3 with: submodules: true - name: Checkout destination uses: actions/checkout@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: ref: gh-pages path: built-site - name: Setup Hugo run: | curl -L -o /tmp/hugo.tar.gz \u0026#39;https://github.com/gohugoio/hugo/releases/download/v0.110.0/hugo_extended_0.110.0_linux-amd64.tar.gz\u0026#39; tar -C ${RUNNER_TEMP} -zxvf /tmp/hugo.tar.gz hugo - name: Build run: ${RUNNER_TEMP}/hugo - name: Deploy if: github.ref == \u0026#39;refs/heads/main\u0026#39; run: | cp -R public/* ${GITHUB_WORKSPACE}/built-site/ cd ${GITHUB_WORKSPACE}/built-site git add . git config user.name \u0026#39;keanteng\u0026#39; # change to your username git config user.email \u0026#39;u2004763@siswa.um.edu.my\u0026#39; # change to your email git commit -m \u0026#39;Updated site\u0026#39; git push Before we link up our local files with the repository, you need to create a new branch on your repository called git-pages and you need to change the setting for GitHub actions.\nChange the setting here\nFinal Step For the last part of the website setup, we will link up our local files with the repository created so that we can view our site online:\ngit status git add . git commit -m \u0026#34;site update\u0026#34; git push Now, just head to GitHub actions and click on pages build and deployment, and click on your website link on the web!\nClick on the link to view your site\nAcknowledgementsâ¨ I would like to thank Hugo and PaperMode for empowering me with the tool and foundations to build such a beautiful and impressive site. I want to thank dhij for the wonderful tutorial on YouTube for the site set-up, I failed on numerous attempt to set up the site by looking at some blogs on Medium until I look up his video. I am inspired to make this blog as comprehensive as possible so anyone that reads it can follow successfully.\n","permalink":"https://keanteng.github.io/home/docs/2023-04-09-creating-a-website-with-hugo--papermode/","summary":"There are a few things to prepare before your own site can be created, as follows: A GitHub account Installed Microsoft Visual Studio Code Installed Git. After you create a GitHub account and installing the necessary software, we are ready to begin!","title":"Creating a Website With Hugo \u0026 PaperMode"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nComputer vision literally means computer able to see and recognize stuff. Applications such as Google Lens and Google Image Search are some good examples of where computer vision is being used in our daily life.\nIn this course, we will explore some technique used to empower computer with the power of seeing:\nBuilding an image classifier with Keras Concepts of visual feature extraction Custom covnet Apply data augmentation to extend dataset. 1. Convolutional Classifier Convolutional neural networks or âcovnetâ is a neural network specializes in computer vision. A covnet used for image classification has two parts: a convolutional base and a dense head. The base is used to extract the features of an image while the head is used to determine the class of the image. So, the aims of training a neural network are simply to know which feature to extract from a particular image and to know which class the image will belong from the features.\nThe process\nHere is how we can train a covnet on Python to recognize a car and a truck:\n# 1. load pretrained base pretrained_base = tf.keras.models.load_model( # file path \u0026#39;../input/cv-course-models/cv-course-models/vgg16-pretrained-base\u0026#39;, ) pretrained_base.trainable = False # 2. attach classfier head from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ pretrained_base, layers.Flatten(), # transform 2d output to 1d layers.Dense(6, activation = \u0026#39;relu\u0026#39;), layers.Dense(1, activation = \u0026#39;sigmoid\u0026#39;), # transform output to class probability (truck) ]) # 3. model fitting model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;binary_crossentropy\u0026#39;, metrics = [\u0026#39;binary_accurary\u0026#39;], ) history = model.fit( ds_train, validation_data = ds_valid, epochs = 30, verbose = 0, ) # 4. visualize model loss import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_frame.loc[:, [\u0026#39;binary_accurary\u0026#39;, \u0026#39;val_binary_accurary\u0026#39;]].plot(); 2. Features Extraction 2.1 Convolution and ReLU The process of feature extraction does three things. It filters an image for a certain feature; it detects the feature within the filtered image, and it condenses the image to enhance the features.\nFeatur extraction overview\nIn training, the covnet will learn weights from image features and the weights are contained in the convolutional layers. The weights are known as kernels which can be presented as an array of number. A kernel will scan over an image and produce a weighted sum of pixel values (finding the best kernel values) â emphasizing and de-emphasizing certain image patterns and information.\nThe process\nActivations in the network is called feature maps. Feature maps is the result when filter is applied to image â it contains features a kernel extract.\nApplying filter\nAfter filtering, the feature maps will be passed to an activation function which can be though as scoring pixel values according to some measure of importance. For example, ReLU activation assumes negative values are not important, so they are set to zero. In fact, these images are how the head of a network is able to solve the classification problem â looking for a particular characteristic of images that we want to classify.\nAfter passing through activation function\nHereâs how to can perform feature extraction in Python:\n# define kernel import tensorflow as tf kernel =tf.constant([ [-1, -1, -1], [-1, 8, -1], [-1, -1, -1], ]) plt.figure(figsize = (3,3)) show_kernel(kernel) # applying kernel image_filter = tf.nn.conv2d( input = image, fitlers = kernel, strides = 1, # section 3 padding = \u0026#39;SAME\u0026#39;, ) plt.figure(figsize = (6,6)) plt.imshow(tf.squeeze(image_filter)) plt.axis(\u0026#39;off\u0026#39;) plt.show(); # applying activation function image_detect = tf.nn.relu(image_filter) plt.figure(figsize = (6,6)) plt.imshow(tf.squeeze(image_filter)) plt.axis(\u0026#39;off\u0026#39;) plt.show(); 2.2 Maximum Pooling from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Conv2D(filters=64, kernel_size=3), # activation is None layers.MaxPool2D(pool_size=2), # More layers follow ]) Notice that after the Conv2D layer, we will apply a MaxPool2D layer for the condensation step. This layer will not contain any trainable weights as of the previous layer, but it will condense the feature maps to only retain important feature. This is what maximum pooling does. It takes patches of activations in the original feature maps and replaces them with maximum activation in those patches. The pooling steps will increase the proportions of active pixels to zero pixels â intensifying the feature after ReLU activation.\nMaximum pooling\nIs zero pixels unimportant? In fact, zero pixels carries positional information and MaxPool2D function will remove them (positional information of the feature maps) and this will lead to a property in covnet known as translation invariance.\nThis means that a covnet with maximum pooling tend to not distinguish features by their location in image. Notice from the first row of images below, after repeated pooling, the positional information is destroyed and no longer distinguishable. But pooling only causes translation invariance in network over small distance. The second row of the image features two dots far apart and this feature remains distinct after repeated pooling.\nIn fact, such invariance is good for an image classifier as it reduces data for training since we do not need to teach the network differences in perspective and framing when same features are positioned on different part of an original image.\n3. Sliding Window Both convolution and pooling steps are performed over a sliding window with parameters âkernel_sizeâ for convolution and âpool_sizeâ for pooling.\nNotice in section 2.1, when we perform pooling, there are two extra parameters: strides and padding. strides means how far the window will move each step and padding describes how the pixels at the edge are being handle.\nPooling layer will almost always have stride values greater than 1, like (2, 2) or (3, 3), but not larger than the window itself.\nConsidering the sliding window process, is it necessary to always stay within the boundary? In fact, there is a trade-off between staying within and out of bound by changing the parameter padding in our code:\npadding = âvalidâ: Convolution window stay entirely inside input. Output will shrink. It will shrink more for larger kernels. This will limit the number of layers contained in a network, notably small size input. padding = âsameâ: Pad the input with 0âs around the border to make size of output and input the same. However, this will dilute the influence of pixels at the borders. from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\u0026#39;same\u0026#39;, activation=\u0026#39;relu\u0026#39;), layers.MaxPool2D(pool_size=2, strides=1, padding=\u0026#39;same\u0026#39;) # More layers follow ]) Example for visualization:\nshow_extraction( image, kernel, # Window parameters conv_stride=3, pool_size=2, pool_stride=2, subplot_shape=(1, 4), figsize=(14, 6), ) Since the circle is just 1 pixel wide, using stride = 3 is too coarse to produce a decent feature maps. We should then reduce the number of strides for a better feature map.\nThe more horizontal parts of the input end up with the greatest activation as the kernel is designed to detect horizontal lines.\n4. Custom Covnet Through feature extraction, we learned how to extract simple features from an image through filter, detect and pooling. By repeating the extraction process, we can extract more complex and refined features as the process travel deeper into the network.\nThis can be done through convolution blocks with stacks of Conv2D and MaxPool2D layers as below:\nHereâs how we can design a covnet that can extract complex features:\n# 1. define model from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ # First Convolutional Block layers.Conv2D(filters=32, kernel_size=5, activation=\u0026#34;relu\u0026#34;, padding=\u0026#39;same\u0026#39;, input_shape=[128, 128, 3]), layers.MaxPool2D(), # Second Convolutional Block layers.Conv2D(filters=64, kernel_size=3, activation=\u0026#34;relu\u0026#34;, padding=\u0026#39;same\u0026#39;), layers.MaxPool2D(), # Third Convolutional Block layers.Conv2D(filters=128, kernel_size=3, activation=\u0026#34;relu\u0026#34;, padding=\u0026#39;same\u0026#39;), layers.MaxPool2D(), # Classifier Head layers.Flatten(), layers.Dense(units=6, activation=\u0026#34;relu\u0026#34;), layers.Dense(units=1, activation=\u0026#34;sigmoid\u0026#34;), ]) model.summary() # 2. model training model.compile( optimizer=tf.keras.optimizers.Adam(epsilon=0.01), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;binary_accuracy\u0026#39;] ) history = model.fit( ds_train, validation_data=ds_valid, epochs=40, verbose=0, ) # 3. model loss evaluation import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_frame.loc[:, [\u0026#39;binary_accuracy\u0026#39;, \u0026#39;val_binary_accuracy\u0026#39;]].plot(); 5. Data Augmentation More data will generally help a model performs better â to better differentiate image. In this section, we will learn to augment our data by applying transformation to our datasets such as rotation, flipping, warping and changing of contrast and color tone. Hereâs how to perform data augmentation in Python:\nData augmentation example\n# 1. define model - with augmentation from tensorflow import keras from tensorflow.keras import layers pretrained_base = tf.keras.models.load_model( \u0026#39;../input/cv-course-models/cv-course-models/vgg16-pretrained-base\u0026#39;, ) pretrained_base.trainable = False model = keras.Sequential([ # Preprocessing layers.RandomFlip(\u0026#39;horizontal\u0026#39;), # flip left-to-right layers.RandomContrast(0.5), # contrast change by up to 50% # Base pretrained_base, # Head layers.Flatten(), layers.Dense(6, activation=\u0026#39;relu\u0026#39;), layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;), ]) # 2. model training model.compile( optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;binary_accuracy\u0026#39;], ) history = model.fit( ds_train, validation_data=ds_valid, epochs=30, verbose=0, ) # 3. model loss evaluation import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_frame.loc[:, [\u0026#39;binary_accuracy\u0026#39;, \u0026#39;val_binary_accuracy\u0026#39;]].plot(); ","permalink":"https://keanteng.github.io/home/docs/2023-03-20-computer-vision/","summary":"Computer vision literally means computer able to see and recognize stuff. Applications such as Google Lens and Google Image Search are some good examples of where computer vision is being used in our daily life. In this course, we will explore some technique used to empower computer with the power of seeing:","title":"Computer Vision"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nFor examining and assessing huge datasets and databases, SQL or structured programming language skills plays a vital role to enable us to design and manage data.\nSome common keywords used in SQL as follows:\nSELECT, WHERE, FROM GROUP BY, HAVING, COUNT ORDER BY AS, WITH JOIN 1. Data Preparation Since we will be using Python to apply SQL, there are some steps required for data preparation. We will first start by establishing a connection to the BigQuery service to create a client object that hold projects. We will then select a project from the biquery-public-data which holds a collection of datasets for each project. The datasets will contain the tables that we want for our analysis. Hereâs how we can do that in Python:\nfrom google.cloud import bigquery # 1. create a client object client = bigquery.Client() dataset_ref = client.dataset(\u0026#34;hacker-news\u0026#34;, project = \u0026#34;bigquery-public-data\u0026#34;) # 2. get the hacker news dataset from a project dataset = client.get_dataset(dataset_ref) # 3. list the tables in the datasets tables = list(client.list_tables(dataset)) # 4. print the all the table names for table in table: print(table.table_id) Now from the code we learn about all the table names in the datasets from the printed outputs. Hereâs how we can fetch the table with code:\n# create a reference to the table name full table_ref = dataset_ref.table(\u0026#34;full\u0026#34;) # get the table table = client.get_table(table_ref) Oftentimes, when we want to preview at the table before our analysis, but it contains a huge number of rows and might consume resources to load, we can choose to display only a certain number of rows of the table.\n# Restrict rows and convert the result to dataframe client.list_rows(table, max_results = 5).to_dataframe() 1.1 Table Schema The structure of a table is known as table schema, and it will tell us information about each specific columns in our table. The information is:\nName of the column Datatype of the column Mode of the column (by default it will allow null values â nullable) Description of the data in the particular column # code command table.schema # Output: [SchemaField(\u0026#39;title\u0026#39;, \u0026#39;STRING\u0026#39;, \u0026#39;NULLABLE\u0026#39;, \u0026#39;Story title\u0026#39;, (), None)] 2. Queries Foundations We will start by three keywords which represents the foundational components of a SQL query, namely SELECT, FROM and WHERE.\nBy SELECT, it refers to the columns that we want to select, FROM refers to the table that we are after and finally WHERE refers to the return condition that we want to impose on the output. Below is how we can select the cat ownerâs name from an imaginary dataset:\nAn imaginary dataset\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT Name FROM `bigquery-public-data.pet_records.pets` WHERE Animal = \u0026#39;Dog\u0026#39; \u0026#34;\u0026#34;\u0026#34; 2.1 Big Datasets Some BigQuery datasets can be huge and costly to compute. If you are running them on cloud services with limited computing hours, your limit might drain easily with huge datasets. We can avoid this issue by imposing a function to limit the amount of data that we are capable to scan to avoid running over our computing limit.\nHereâs how we can check how much data a query will scan, alternatively we can also impose a limit with a parameter on how much data that will be scanned:\n# query query = \u0026#34;\u0026#34;\u0026#34;\u0026#34; SELECT score, title FROM `bigquery-public-data.hacker_news.full` WHERE type = \u0026#34;job\u0026#34; \u0026#34;\u0026#34;\u0026#34; ## 1. Cost estimation # create object for cost estimation without running it dry_run_config = bigquery.QueryJobConfig(dry_run = True) # estimate cost dry_run_query_job = client.query(query, job_config = dry_run_config) print(\u0026#34;This query will process {} bytes.\u0026#34;.format(dry_run_query_job.total_bytes_processed)) ## 2. Setting parameter ONE_GB = 1000*1000*1000 # set up the query safe_config = bigquery.QueryJobConfig(maximum_bytes_billed = ONE_GB) # run the query safe_query_job = client.query(query, job_config = safe_config) job_posts_scores = safe_query_job.to_dataframe() job_posts_scores.mean() 3. More Queries 1 This section will add three new techniques: GROUP BY, HAVING and COUNT() to assist us in getting more interesting insights from our queries.\nGROUP BY means grouping together rows with the same value in that column as a single group, COUNT() will return a count of things such as the number of entries of a column while HAVING is used in combination with GROUP BY to ignore groups that do not meet a certain criterion.\nHere is how we can apply these techniques to find the most discussed new comments on the Hacker News project:\n# select comment with \u0026gt; 10 replies query_popular = \u0026#34;\u0026#34;\u0026#34; SELECT parent, COUNT(1) AS NumPosts FROM `bigquery-public-data.hacker_news.comments` GROUP BY parent HAVING COUNT(1) \u0026gt; 10 \u0026#34;\u0026#34;\u0026#34; # limit how much data will be scan safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10) query_job = client.query(query_popular, job_config=safe_config) # run query and convert to data frame popular_comments = query_job.to_dataframe() # print the first five rows popular_comments.head() 4. More Queries 2 In this section, we will learn about ORDER BY clause where it is used to sort the returned results by the rest of the query. When we use ORDER BY the returned results will be sorted in ascending order whether it is string or numeric. If we would like to have the order reverse, we can add in the DESC argument.\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT Name FROM `bigquery-public-data.pet_records.pets` WHERE Animal = \u0026#39;Dog\u0026#39; ORDER BY Name DESC \u0026#34;\u0026#34;\u0026#34; Now considering we have a column that tell us about the date that a pet owner getting a pet, we can use the EXTRACT clause to get the day, month, week and year information from the column.\n5. More Queries 3 In this section, we will work on two keywords that helps to organize our query for better readability â if we happen we have a complex query. Starting with AS, it is used to rename columns generated by our queries. WITH is used together with AS to create a temporary table so that we can write query against them. This helps us to split queries into readable chunks and makes the data cleaning work easier.\nConsider that we want to know the pet IDs that are owned for more than five years:\nquery = \u0026#34;\u0026#34;\u0026#34; WITH Seniors AS ( SELECT Name, ID FROM `bigquery-public-data.pet_records.pets` WHERE Years_old \u0026gt; 5 ) SELECT ID FROM Seniors \u0026#34;\u0026#34;\u0026#34; Another example is to find how many bitcoins transactions being made in a month:\nquery = \u0026#34;\u0026#34;\u0026#34;\u0026#34; WITH Time AS ( SELECT DATE(block_timestamp) AS trans_date FROM `bigquery-public-data.crypto_bitcoin.transactions` ) SELECT COUNT(1) AS transactions, trans_date FROM Time GROUP BY trans_date ORDER BY trans_date \u0026#34;\u0026#34;\u0026#34; # limit how much data will be scanned safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10) query_job = client.query(query, job_config=safe_config) # run query and convert to data frame transactions_by_date = query_job.to_dataframe() # Print the first five rows transactions_by_date.head() # Extra: visualize the result transactions_by_date.set_index(\u0026#39;trans_date\u0026#39;).plot() 6. Combining Data Sources For the last clause in this course, we will learn about JOIN â INNER JOIN in this section. For INNERJOIN, a row will only be selected if the column that we used to combine in one table also matches with the table that we used for joining. JOIN clause allows us to query and combine the information from different tables. For example, we have two tables; one contains the pet name and the other one contains the petâs owner name. Here how we can associate the pet with its owner:\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT p.Name AS Pet_Name, 0.Name AS Owner_Name FROM `bigquery-public-data.pet_records.pets` AS P INNER JOIN bigquery-public-data.pet_records.owners` AS O ON p.ID = o.Pet_ID \u0026#34;\u0026#34;\u0026#34; Hereâs another example where we want to find how many files are covered by each type of software license on GitHub.\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT L.license, COUNT(1) AS number_of_files FROM `bigquery-public-data.github_repos.sample_files` AS sf INNER JOIN `bigquery-public-data.github_repos.licenses` AS L ON sf.repo_name = L.repo_name GROUP BY L.license ORDER BY number_of_files DESC \u0026#34;\u0026#34;\u0026#34; # limit how much data will be scanned safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10) query_job = client.query(query, job_config=safe_config) # run query and convert to data frame file_count_by_license = query_job.to_dataframe() file_count_by_license ","permalink":"https://keanteng.github.io/home/docs/2023-03-15-intro-to-sql/","summary":"For examining and assessing huge datasets and databases, SQL or structured programming language skills plays a vital role to enable us to design and manage data. Some common keywords used in SQL as follows: SELECT, WHERE FROM GROUP BY, HAVING, COUNT ORDER BY AS, WITH JOIN. Date Preparation Since we will be","title":"Intro to SQL"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nUsing machine learning in data analysis is a rather procedural approach. As we can notice from the approach, we will start by Preparing data \u0026raquo; Defining a model \u0026raquo; Model diagnostic checking \u0026raquo; Model prediction to complete our workflow. This workflow is often and commonly practiced when we are doing data analysis work. In this course, we will learn some important and useful technique that can be used in our work to achieve a better model.\nMain learnings:\nApproaches towards missing data values and categorical variables (non-numeric) Construct pipeline to improve our workflow and code flow. Cross-validation technique Build state-of-the-art model such as XGBoost Approaches to avoid data leakage. 1. Missing Values \u0026amp; Categorial Variables 1.1 Missing values We can deal with missing values with the following three ways:\nDrop the columns containing missing values (not recommended, might loss access to important information) Imputation to fill the empty cells with some number. Imputation, then add a new column that shows the missing entries. Visualize the methods\nImputation will perform better than dropping the entire columns. This is how we can do that in code:\n## 1. drop columns with missing values # get the columns name cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()] # perform drop reduced_X_train = X_train.drop(cols_with_missing, axis = 1) ## 2. imputation from sklearn.impute import SimpleImputer my_imputer = SimpleImputer() imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train)) # imputation removed column names, put back imputed_X_train.columns = X_train.columns ## 3. extended imputation X_train_plus = X_train.copy() for col in cols_with_missing: X_train_plus[col + \u0026#39;_was_missing\u0026#39;] = X_train_plus[col].isnull() my_imputer = SimpleImputer() imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus)) imputed_X_train_plus.columns = X_train_plus.columns 1.2 Categorical variables There are three ways to deal with categorical variables (non-numeric data):\nDropping the categorical variables (if the columns does not provide any useful information) Ordinal encoding â assign a unique value in the dataset to a different integer. One-hot encoding â create new columns to indicate the presence of each possible value in the original data One hot encoding example\nFor One-hot encoding, it means that if a column with 100 rows contains 100 unique values, it will create an extra (100 rows *100 unique values â100 original rows) new entries.\nWe can apply the 3 approaches with the following code:\n## 1. droppping categorical variables dorp_X_train = X_train.select_dtypes(exclude= [\u0026#39;object\u0026#39;]) ## 2. ordinal encoding from sklearn.preprocessing import OrdinalEncoder label_X_train = X_train.copy() ordinal_encoder = OrdinalEncoder() label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols]) ## 3. one hot encoding from sklearn.preprocessing import OneHotEncoder # one hot encode categorical data OH_encoder = OneHotEncoder(handle_unknown = \u0026#39;ignore\u0026#39;, sparse = False) OH_cols_train = pd.DataFrame(OH_encoder.fit_transform[object_cols]) # add back removed index OH_cols_train.index = X_train.index # remove categorical column and add back the one hot encoded columns num_X_train = X_train.drop(object_cols, axis = 1) OH_X_train = pd.concat([num_X_train, OH_cols_train], axis = 1) 2. Pipelines Pipeline is a way to bundle our preprocessing and modelling steps to keep our code organized. The benefits of using pipeline are it gives a cleaner code, reduces bugs and make our model easier to be implemented.\nHereâs how we can apply pipeline to impute missing numerical entries and one-hot encode missing categorical entries:\nfrom sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder # preprocess numerical data numerical_transformer = SimpleImputer(strategy = \u0026#39;constant\u0026#39;) # preprocess categorical data categorical_transformer = Pipeline(steps = [ (\u0026#39;imputer\u0026#39;, SimpleImputer(strategy = \u0026#39;most frequent\u0026#39;)), (\u0026#39;onehot\u0026#39;, OneHotEncoder(handle_unknown = \u0026#39;ignore\u0026#39;)) ]) # bundle the two preprocesses preprocessor = ColumnTransformer( transformers = [ (\u0026#39;num\u0026#39;, numerical_transformer, numerical_cols), (\u0026#39;cat\u0026#39;, categorical_transformer, categorical_cols) ]) # bundle preprocessing and modelling my_pipeline = Pipeline(steps = [ (\u0026#39;preprocessor\u0026#39;, preprocessor), (\u0026#39;model\u0026#39;, model) ]) # model evaluation my_pipeline.fit(X)train, y_train) preds = my_pipeline.predict(X_valid) mean_absolute_error(y_valid, preds) 3. Cross-validation Cross-validation means we run our modelling process on different subsets of the data to get several measures of our model quality. Although this technique gives a more accurate measure of model quality, it can take some time to run as it need to estimate multiple models as we can see from the below images.\nIt is recommended to run cross-validation for smaller datasets while for larger datasets, a single validation if often suffice.\nCross-Validation\nHereâs how we can apply cross-validation in Python together with pipeline which we learned earlier on:\nfrom sklearn.model_selection import cross_val_score # split the data to 5 sets for validation scores = -1 * cross_val_score(my_pipeline, X, y, cv = 5, scoring = \u0026#39;neg_mean_absolute_error\u0026#39;) print(scores.mean()) It is surprising to see that negative mean absolute error is used in the code. This is because âsklearnâ has a convention where all metrics are defined so a high number is better. Thus, the use of negatives allows convention consistency.\n4. XGBoost â Gradient Boosting In the random forest method, we improve a model prediction by averaging the prediction of many decision trees. Random forest method is one of an ensemble method where we combine the prediction of several models. A state-of-the-art method would be to apply gradient boosting where we perform iterative cycles to add models into an ensemble to result in better prediction.\nConcepts:\nUse the current ensemble to generate predictions for each observation in the dataset. Use the prediction to calculate a loss function. Use the loss function to fit a new model that will be added to the ensemble which will reduce the loss. Add this new model to the ensemble. Repeat Hereâs how to do it in code:\nfrom xgboost import XGBRegressor from sklearn.metrics import mean_absolute_error my_model = XGBRegressor() my_model.fit(X_train, y_train) predictions = my_model.predict(X_valid) mean_absolute_error(predictions, y_valid) There are a few parameters in the xgregressor function that might affect the accuracy of our result:\nn_estimators which means how many times to go through the modelling cycle (concepts above), value too high or too low might result in overfitting or underfitting respectively early_stopping_rounds which means stopping the model when the validation score stops improving with imposed criteria learning_rate which means multiply the predictions from each model by a small number before adding up the prediction from each component model n_jobs which aims to improve modelâs runtime. We can set the number equal to the cores of our machine model = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, n_jobs = 4) model.fit(X_train,y_train, early_stopping_rounds = 5, eval_set = [(X_valid, y_valid)], verbose = False) 5. Data Leakage Data leakage happens when training data contains information about the target, but similar data will not be available when we used the model to perform prediction. The two main types of data leakage are target leakage and train-test contamination.\n5.1 Target leakage Target leakage occurs when predictors include data that will not be available at the time when predictions is made. A way to overcome this issue is to think about timing or chronological order that data becomes available rather than whether a feature will help to make good predictions.\n5.2 Train-test contamination Train-test contamination happens when validation data affects the preprocessing behavior as the validation process is corrupted.\nFor example, imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions. In the courses, there are several interesting case studies on data leakage that worth looking to improve our acumen when interpreting results from work.\nCase 1\nGuide:\nThis is tricky, and it depends on details of how data is collected (which is common when thinking about leakage). Would you at the beginning of the month decide how much leather will be used that month? If so, this is ok. But if that is determined during the month, you would not have access to it when you make the prediction. If you have a guess at the beginning of the month, and it is subsequently changed during the month, the actual amount used during the month cannot be used as a feature (because it causes leakage).\n","permalink":"https://keanteng.github.io/home/docs/2023-03-02-intermediate-machine-learning/","summary":"Using machine learning in data analysis is a rather procedural approach. As we can notice from the approach, we will start by \u003cem\u003ePreparing data \u0026raquo; Defining a model \u0026raquo; Model diagnostic checking \u0026raquo; Model prediction\u003c/em\u003e to complete our workflow. This workflow is often and commonly practiced when we are doing data analysis work.","title":"Intermediate Machine Learning"},{"content":" Images from Unsplash\nDisclaimer: This article is for educational purpose only and the author does not suggest any illegal usage of this technology (generative AI). Please be responsible for your generations and creations. Do not use them for any malicious intent or harm in any form and be respectful.\nThere are two ways to use Stable Diffusion â either locally on your own PC or through a cloud computing services such as Google Collab and Kaggle.\nIt has been a frustration for many without a decent GPU and sufficient VRAM. This has caused long computation time and a lot of friction for you to tune your model parameter/ prompt, and worse, you are not even able to load the Stable Diffusion model.\nSources: Stable Diffusion WebUI https://cdn.changelog.com/uploads/news_items/RdAG/large.png?v=63829970528\nI will break this article in 3 parts according to the resources I gathered, and you are free to discover the one that you see fit.\n1. Google Collab â Realistic \u0026amp; Animated Image Generation Terms: google collab, civitai, account\nGoogle Collab offers a decent and fast GPU and long-running hours for any Google users with a Google account. You can check out the YouTube video here by Nolan Aatama where you can learn about some quick installation steps to run Stable Diffusion in your own account. The account provides installation guide on various models such as Dreamshaper and more.\nYou can check out the models offered on Civitai to look for prompt examples and user review of the model as well as other models offered on the platform.\n# prompts example positive: space, rocket, stallite, earth, milky ways, space dust, high res, 8k negative: lowres, bad hands, bad fingers, sketeches, paintings, If you want to check the prompts for any of the generated images you like can use this Stable Diffusion Decomposer to get the details of each generated images.\nStable Diffusion Decomposer\nYou need to create a Civitai account to post your review and submit comment.\nDo note that it takes around 10 minutes to run the code. It took around 20 seconds to generate a 512x1024 pixels image which is decent. For Google Collab, I think there is no computing quota being set and you can run the program for a few hours. But do avoid leaving the tabs idle as Google will reconnect your program which caused you to re-run all the code again.\n2. Kaggle â Animated Image Generation Terms: kaggle, webui, gpu, telegram\nI only manage to find scripts that provide animated image generation on Kaggle, you can look at it here. You just need to copy and run all the codes in the notebook to access the WebUI. Do note that on Kaggle, you only have 30 hours of GPU resources being allocated, so use it wisely and shut down the connection when you are not running the program to conserve the quota allocated to you.\nTurn off GPU if it is not in use\nIf you like animated image and you donât want to always re-run the scripts to access WebUI, you can also check out this site, a site that host the WebUI for animated image generation. You can get around 100 quotas by signing in daily (which allows you to generate about 90+ images). You also need to acquire the sign-in token with a Telegram account.\nSome of the model offered on the site\n3. Your Own PC Terms: local, google translation, extension, apply and restart\nIf you want to run Stable Diffusion locally, you can have a read on this article on the installation guide. You can use Google Translate for the article as it is in Chinese.\nBasically, this is what you need to do:\nInstallation of Stable Diffusion WebUI Load models, put in models/Stable-diffusion/ files after the installation step above (can get from Civitai website) After opening the local URL, go to the Extensions tab and install https://github.com/civitai/sd_civitai_extension. Go to Installed tab and click Apply and Restarts UI Done! ","permalink":"https://keanteng.github.io/home/docs/2023-02-25-stable-diffusion-webui-with-civitai-loras/","summary":"There are two ways to use Stable Diffusion â either locally on your own PC or through a cloud computing services such as Google Collab and Kaggle. It has been a frustration for many without a decent GPU and sufficient VRAM. This","title":"Stable Diffusion WebUI with Civitai LORAs"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\n0. Concepts Creation and evaluation of a deep learning model is a procedural work with steps. Data preparation \u0026raquo; Model optimization\u0026raquo; Model fitting \u0026raquo; Model evaluation \u0026raquo; Model prediction\nNote:\nModel optimization (adam optimizer) Model fitting (batch, epoch) Model evaluation(dropout, batch normalization) 1. Modelling \u0026amp; Neural Network Terms: layer, activation function, neural network, dense, rectified linear unit\nSimple Neural Network (linear)\nThe above neural network can be generated with code using Python. The network above organized neurons into layers. Collecting linear units with common set of inputs result in a dense layer.\nfrom tensorflow import keras from tensorflow.keras import layers # 1 linear unit network model = keras.Sequential([ layers.Dense(units = 1, input_shape = [1]) ]) model = keras.Sequential([ layers.Dense(units = 1, input_shape = [3]) ]) If we want to fit a curve (non-linear), a feature called activation function is needed, otherwise the neural network can only learn linear relationships. A common example is the rectifier function max(0,x), where we get a rectified linear unit when we attach the function to a linear unit.\nWe can also stack layers to achieve a complex data transformation.\nVisualizing the Network\nUsing code to define the network:\nfrom tensorflow import keras from tensorflow.keras import keras # [layer1, layer2, layer3,...] model = keras.Sequential([ layers.Dense(units = 4, activation = \u0026#39;relu\u0026#39;, input_shape = [2], layers.Dense(units = 3, activation = \u0026#39;relu\u0026#39;), layers.Dense(units = 1) ]) 2. Model Optimization \u0026amp; Fitting Terms: loss function, stochastic gradient descent, batch, epoch, error/loss\nSimilar to regression, we need to know how well our model fits the data. We use loss function to measures the difference between observed and predicted values. The common measure would be the mean absolute error where it computed the average length between the fitted curve and data points. Other errors examples are mean-squared error or Huber loss.\nHow can we minimize the error? We can use optimization algorithms â the stochastic gradient descent. The concepts as follows:\nGet random sample (batch) from original dataset and run through the network for prediction. Measure the error (loss) Adjust the weight in a direction that makes the loss smaller (an epoch for each complete round a training) We can use a built-in âadamâ optimizer to optimize our model. This allows self-tuning to minimize loss. Below is code to fit 256 rows of training data for 10 times:\n# get training data dimension (row, column) print(X_train.shape) # define model from tensorflow import keras from tensorflow.keras import layers mode = keras.Sequential([ layers.Dense(512, activation = \u0026#39;relu\u0026#39;, input_shape = [11]), layers.Dense(512, activation=\u0026#39;relu\u0026#39;), layers.Dense(512, activation=\u0026#39;relu\u0026#39;), layers.Dense(1), ]) # add in optimizer and loss function model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;mae\u0026#39;, ) # model fitting history = model.fit( X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 256, epochs = 10, ) # visualize outcome import pandas as pd # training history to data frame history_df = pd.DataFrame(history.history) # plot the loss history_df[\u0026#39;loss\u0026#39;].plot(); 3. Model Evaluation 1 Terms: underfitting, overfitting, capacity, training, callback, stopping criteria\nTraining data normally contains signal and noise where signal helps our model make prediction from new data and noise represent the random fluctuation coming from data in the real world. To see if our model fits the data well, we will compare learning curve between training set and validation set.\nUnderfitting the training set is when the loss is not as low as it could be because the model hasnât learned enough signal. Overfitting the training set is when the loss is not as low as it could be because the model learned too much noise.\nA modelâs capacity is the size and complexity of the patterns it is able to learn. We can adjust a modelâs capacity if we detect overfitting or underfitting scenarios.\n# sample model model = keras.Sequential([ layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.Dense(1), ]) # wider model = keras.Sequential([ layers.Dense(32, activation = \u0026#39;relu\u0026#39;), layers.Dense(1), ]) # deeper model = keras.Sequential([ layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.Dense(1), ]) Sometimes the validation increase during training after a certain point although it kept decreasing early on. This can due to a model is learning noise from the datasets. We can overcome this issue by imposing an early stopping criterion.\nA callback function is used where we detect when the validation loss starts to rise again, and we reset the weights back the where the minimum occurred. An example âif there is not at least 0.001 improvement in the validation loss over 20 epochs, then stop training and keep the best model you foundâ.\nfrom tensorflow.keras.callbacks import EarlyStopping early_stopping = EarlyStopping( min_delta = 0. 001, # min change to count as improvement patience = 20, # how many epochs to wait before stopping restore_best_weights = True, ) 4. Model Evaluation 2 Terms: dropout, batch normalization\nTo correct overfitting in our model, we can implement the idea of dropout where we randomly drop out some fraction of a layerâs input units every step of training. This avoids the network to learn spurious patterns in the training data which leads to overfitting.\nmodel = keras.Sequential([ ... layers.Dropout(rate = 0.3) # 30% dropout to the next layer layers.Dense(16), ... ]) To correct slow or unstable training, we can apply batch normalization to put all data on a common scale. SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.\nmodel = keras.Sequential([ layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.BatchNormalization(), ]) # or model = keras.Sequential([ layers.Dense(16), layers.BatchNormalization(), layers.Activation(\u0026#39;relu\u0026#39;), ]) Example for a full model fitting process:\nfrom tensorflow import keras from tensorflow.keras import layers # define network model = keras.Sequential([ layers.Dense(1024, activation=\u0026#39;relu\u0026#39;, input_shape=[11]), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1024, activation=\u0026#39;relu\u0026#39;), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1024, activation=\u0026#39;relu\u0026#39;), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1), ]) # add optimier model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;mae\u0026#39;, ) # model fitting history = model.fit( X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 256, epochs = 100, verbose = 0, ) # visualize history_df = pd.DataFrame(history.history) history_df.loc[:,[\u0026#39;loss\u0026#39;,\u0026#39;val_loss\u0026#39;]].plot(); 5. Binary Classification Terms: sigmoid, binary, accuracy\nWe cannot use accuracy to measure model performance as it does not change smoothly â it changes in jumps as it represents ratio counts. Thus, a replacement would be the cross-entropy function where it measures the distance between probabilities. To convert outputs from dense layer into probabilities, we will use the sigmoid activation function.\nCode example:\nfrom tensorflow import keras from tensorflow.keras import layers # define network model = keras.Sequential([ layers.Dense(4, activation=\u0026#39;relu\u0026#39;, input_shape=[33]), layers.Dense(4, activation=\u0026#39;relu\u0026#39;), # final layer need sigmoid function to produce class probabilities layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;), ]) # add optimizer model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;binary_crossentropy\u0026#39;, metrics = [\u0026#39;binary_accuracy\u0026#39;] ) # add stopping criteria early_stopping = keras.callbacks.EarlyStopping( patience = 10, min_delta = 0.001, restore_best_weights = True, ) # model fitting history = model.fit( X_train, y_train, validation_data=(X_valid, y_valid), batch_size=512, epochs=1000, callbacks = [early_stopping], verbose = 0 # hide output since too many to display ) # model levaluation history_df = pd.DataFrame(history.history) # starts at epoch 5 history_df.loc[5:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_df.loc[5:, [\u0026#39;binary_accuracy\u0026#39;, \u0026#39;val_binary_accuracy\u0026#39;]].plot() print((\u0026#34;Best Validation Loss: {:0.4f}\u0026#34; +\\ \u0026#34;\\nBest Validation Accuracy: {:0.4f}\u0026#34;)\\ .format(history_df[\u0026#39;val_loss\u0026#39;].min(), history_df[\u0026#39;val_binary_accuracy\u0026#39;].max())) 6. Reference Learn Intro to Deep Learning Tutorials - Kaggle ","permalink":"https://keanteng.github.io/home/docs/2023-02-24-intro-to-deep-learning/","summary":"Creation and evaluation of a deep learning model is a procedural work with steps. *Data preparation \u0026raquo; Model optimization\u0026raquo; Model fitting \u0026raquo; Model evaluation \u0026raquo; Model prediction Note Model optimization (adam optimizer) Model fitting (batch, epoch)","title":"Intro to Deep Learning"},{"content":" Images from Unsplash\nIn this article, we will learn how to image scraping on this National Geographic article: Asteroids vs. comets: How do they differ, and do they pose a threat to Earth? We will learn how to download in bulk the high-quality images in the webpage with Python code.\nThe process:\nFind the webpage data structure for the URL specified. Search for the images link and save in a list. Download all the images. Find the webpage data structure for the URL specified We will be using Visual Studio Code with Python and Jupyter notebook configured. However, a Python code file will also do the job. Here are the three packages to be installed using Window PowerShell:\n# Lauch Power Shell by pressing Start and then type Power Shell in the search bar # installation procedure py -m pip install requests py -m pip install bs4 py -m pip install urllib First, we start by importing the modules needed. Then, we need to make HTTP request to view the webpage data structure we want. This can be done as follows:\nimport requests from bs4 import BeautifulSoup import urllib.request def getdata(url): r = requests.get(url) return r.text htmldata = getdata(\u0026#34;https://www.nationalgeographic.co.uk/space/2023/01/asteroids-vs-comets-how-do-they-differ-and-do-they-pose-a-threat-to-earth\u0026#34;) soup = BeautifulSoup(htmldata, \u0026#39;html.parser\u0026#39;) You can check the output by typing the variable name:\nOutput representing webpage structure.\nSearch for the images link and save in a list Now, we want to search for all the images links that embedded images in the webpage. We will use a test list to store each of the link as list of elements for download later. The print function is used to print out all the links that are being searched.\ntest = list() for item in soup.find_all(\u0026#39;img\u0026#39;): print(item[\u0026#39;src\u0026#39;]) test.append(item.get(\u0026#39;src\u0026#39;)) All the searched links in list\nDownload all the images Now simply using a for loop, we can save all the images inside our current folder. The number of iterations, n will be the number of images in the list. In our code, we will also rename the images with number 0,1,2,â¦,n to avoid duplicate file name.\nfor i in range(len(test)): urllib.request.urlretrieve(test[i], str(i)+\u0026#34;.jpg\u0026#34;) All the images being downloaded\nExtra: Some website will have several images urls mixed up including .svg, .png., .gif and .jpg. We can use if statement to counter this issue:\ntemp = list() for i in range(len(test)): if test[i].__contains__(\u0026#39;.png\u0026#39; or \u0026#39;.jpg\u0026#39; or \u0026#39;.jpeg\u0026#39;): temp.append(test[i]) for i in range(len(temp)): urllib.request.urlretrieve(temp[i], str(i)+\u0026#34;.jpg\u0026#34;) That said, job well done. Do note that this approach has restriction on the type of web page for scraping. It works best for articles pages with images embedded. If you encounter connection time out error with your request, do have a check on the site accessibility (Internet Service Provider). Otherwise, it should work perfectly!\nReferences: Beautiful Soup Documentation â Beautiful Soup 4.4.0 documentation (beautiful-soup-4.readthedocs.io) urllib.request â Extensible library for opening URLs â Python 3.11.2 documentation ","permalink":"https://keanteng.github.io/home/docs/2023-02-19-images-scraping-from-web-pages/","summary":"In this article, we will learn how to image scraping on this National Geographic article: Asteroids vs. comets: How do they differ, and do they pose a threat to Earth? We will learn how to download in bulk the high-quality images in the webpage with Python code.","title":"Images Scraping from Web Pages"},{"content":"In this article, I will be showing the process of scraping some listed companies market capitalization in Malaysia data using Google Sheet. We will perform web scraping on the i3 Investor site.\nImages from Unsplash\nGenerate Webpage for Scraping First of all, open a new google sheet and create a table like this:\nCreate a table like this\nInside the table, we have a few companies name and their listed code. Notice that in cell C3, we put a link â this link will serve as a âprefixâ. If the stock code is put at the back of the link, it will direct to the webpage of the particular page.\nprefix link: https://klse.i3investor.com/web/stock/overview/ link to webpage: https://klse.i3investor.com/web/stock/overview/1023 Now, we use the concatenate function to append the code to the prefix links for all the companies in the table:\nUse CONCAT() function to create the links\nWeb Scraping in Action Before we start web scraping, we need to learn about this function â IMPORTXML().\n=importxml(\u0026#34;url\u0026#34;, \u0026#34;query\u0026#34;) Notice that we already have all the URLs needed as we have created the links on the table. Now, the missing piece is called âqueryâ â simply means what do we want to know and where can it be found?\nThe query is called the XPath where it is used in web browser, now letâs hope into Maybank stock page and get the XPath query.\nHighlight the market capitalization amount\nAfter clicking inspect, a window will pop out highlighting a code segment. Here, we need to right click and select Copy full XPath.\nClick copy full XPath\nLetâs hope back to Google Sheet, you will be pasting the query as follow:\n=importxml(\u0026#34;url\u0026#34;,\u0026#34;/html/body/div[3]/div/div[2]/div[8]/div[1]/div[2]/div[1]/div[2]/p/strong\u0026#34;) Note: The URL will be the URLs in the Reference column After applying the formula to the Market Cap column, you will manage to scrape all the Market Capitalization data on the sheet â the data will be updated live.\nData will be loaded once your apply the formula\n","permalink":"https://keanteng.github.io/home/docs/2022-11-05-google-sheet-simple-web-scraping/","summary":"In this article, I will be showing the process of scraping some listed companies market capitalization in Malaysia data using Google Sheet. We will perform web scraping on the i3 Investor site. First of all, open a new google sheet and","title":"Google Sheet Simple Web Scraping"},{"content":" Where can I check on the raw code that this site used to set up? You can find it at my GitHub repository How to post question to you or is there any interactive features such as add to reading list? If you have any question, you can perhaps use the message feature on Medium. Since the majority of the site traffic comes from there. You can find my articles at my Medium blogs. Do you consider adding comment feature to the site? I will consider adding the feature if requested. ","permalink":"https://keanteng.github.io/home/faqs/","summary":"Where can I check on the raw code that this site used to set up? You can find it at my GitHub repository How to post question to you or is there any interactive features such as add to reading list? If you have any question, you can perhaps use the message feature on Medium. Since the majority of the site traffic comes from there. You can find my articles at my Medium blogs.","title":"FAQs"}]