[{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nIn this course, we will explore on data visualization using seaborn, a Python package to visualize data with a variety of plot types. The package is powerful yet easy to use, check out the below images on the plot types that seaborn is able to generate. You can also scroll to the bottom to see the table summary:\nPlots That seaborn Can Create\nLet\u0026rsquo;s explore the Python code to create different plot type with seaborn\n1. Lineplot In the code below, we use sns.lineplot() where it tells Python that we want to produce a line charts with the specified datasets. We only need to change the data parameters if we would like to plot for a different dataset.\nMoreover, we can also set the size of the plot by calling plt.figure(figsize = (w,h)). By adjusting the height and width of the plot we can set our plot to the desired size.\n# setting the plot size plt.figure(figsize = (16,6)) # width and height # plot line plot sns.lineplot(data = fifa_data) Line Plot\nNow, let\u0026rsquo;s consider that we are using a dataset about the number of steams per day for songs such as:\nShape of You Despacito Something Just Like This We would like to compare the streams between \u0026ldquo;Shape of You\u0026rdquo; and \u0026ldquo;Despacito\u0026rdquo;. But we do not want the plot to include the song \u0026ldquo;Something Just Like This\u0026rdquo;. Here\u0026rsquo;s how we can do it:\nplt.title(\u0026#34;Comparing Two Songs Streams\u0026#34;) sns.lineplot(data = spotify[\u0026#39;Shape of You\u0026#39;], label = \u0026#39;Shape of You\u0026#39;) sns.lineplot(data = spotify[\u0026#39;Despacito\u0026#39;], label = \u0026#39;Shape of You\u0026#39;) plt.xlabel(\u0026#34;Date\u0026#34;) Comparing Two Songs Streams\n2. Bar Charts \u0026amp; Heatmaps For bar chart, we can plot it with sns.barplot(). Let\u0026rsquo;s say we want to visualize the average arrival delay for an airline service by month starting from January to December:\ndata = pd.read_csv(file_name, index_cols = \u0026#39;Month\u0026#39;) sns.barplot(x = data.index, y =data[\u0026#39;delay\u0026#39;]) plt.title(\u0026#34;Average Arrival Delay By Month\u0026#34;) plt.ylabel(\u0026#39;Arrival Delay (in minute)\u0026#39;) Bar Plot\nLet\u0026rsquo;s also look at heatmap where it can be used to illustrate patterns in our dataset by color-coding each cell to its corresponding value:\nsns.heatmap(data = data, annot = True) plt.title(\u0026#39;Average Airline Delay for Each Airline\u0026#39;) plt.xlabel(\u0026#39;Airline\u0026#39;) Heatmap\nThe annot parameter ensures all value appears on the chart. By setting to False, we would have no number for each cell.\n3. Scatter Plot Scatter plot is used to show the relationship between two variables. It is a useful plot to understand the relationship between two variables. Here\u0026rsquo;s how we can plot it using Python:\nsns.scatterplot(x = insurance_data[\u0026#39;bmi\u0026#39;], y = insurance_data[\u0026#39;charge\u0026#39;]) Scatter Plot\nFrom the above plot, it seems that BMI are positive correlated with the insurance costs. Now, let\u0026rsquo;s do a double-checking by adding a regression line to our plot:\nsns.regplot(x = insurance_data[\u0026#39;bmi\u0026#39;], y = insurance_data[\u0026#39;charge\u0026#39;]) Scatter Plot With Regression Line\nIn fact we can also perform some color-coding to our plot by adding the hue parameter. Let\u0026rsquo;s say we want to color-code the plot by separating people that smoke and do not smoke:\nsns.scatterplot(x = insurance_data[\u0026#39;bmi\u0026#39;], y = insurance_data[\u0026#39;charge\u0026#39;], hue = insurance_data[\u0026#39;smoker\u0026#39;]) Color-Coded Scatter Plot\nIt seems that smoker will tend to pay more than non-smoker. Let\u0026rsquo;s check it again by adding two regression lines:\nsns.lmplot(x = \u0026#39;bmi\u0026#39;, y = \u0026#39;charge\u0026#39;, hue = \u0026#39;smoker\u0026#39;, data = insurance_data) Multiple Regression Lines Plot\nAnother interesting plot to look into is known as the categorical scatter plot. The plot can be produced with the sns.swarmplot() command:\nsns.swarmplot(x = insurance_data[\u0026#39;smoker\u0026#39;], y = insurance_data[\u0026#39;charges\u0026#39;]) Swarm Plot\nHere\u0026rsquo;re some insights from the plot:\nNon-smokers are charge less than smoker on average People that pay the most are smokers, while those that pay the least are non-smoker. 4. Distributions In this section, we will explore about histograms as well as the density plots. Histogram is a graph that shows the frequency of numerical data using rectangles while for a density plot, it represents the distribution of a numeric variable.\nHere\u0026rsquo;s an example for histogram:\nsns.histplot(irisi_data[\u0026#39;Petal Length\u0026#39;]) Histogram Plot\nFor density plot, we will use the kernel density estimate plot, it looks like a smoothed histogram. By changing the shade parameter, we can turn the display of the shaded region on and off.\nsns.kdeplot(data = iris_data[\u0026#39;Petal Length\u0026#39;], shade = True) !\nKDE Plot\nHere\u0026rsquo;s the code to produce two-dimensional kernel density plot:\nsns.jointplot(x = irid_data[\u0026#39;Petal Length\u0026#39;], y = [\u0026#39;Sepal Width\u0026#39;], kind = \u0026#39;kde\u0026#39;) 2D KDE Plot\nFurthermore, to color-code the histogram of the kernel density estimate plot, simply add a hue parameter as below:\nsns.histplot(data = iris_data, x = \u0026#39;Petal Length\u0026#39;, hue = \u0026#39;Species\u0026#39;) sns.kdeplot(data = iris_data, x = \u0026#39;Petal Length\u0026#39;, hue \u0026#39;Species\u0026#39;, shade = True) Color-Coded Histogram Plot\nColor-Coded KDE Plot\n5. Plot Style There are several themes available in the seaborn module, you can set the style or theme of your plot before you start plotting:\nsns.set_style(\u0026#39;dark\u0026#39;) # your plot here # try other themes with: drakgrid, whitegrid, dark or white 6. Summary Here\u0026rsquo;s a summary of all the plot you have learned:\nType Category Code Remarks Line Chart Trend sns.lineplot() Show trends over time, multiple lines can be used to show trend in more than a group Bar Chart Relationship sns.barplot() Compare quantities with respect to different groups Heatmap Relationship sns.heatmap() Find color-coded patterns in tables of numbers Scatter Plot Relationship sns.scatterplot() Show relationship between two continuous variables Regression Line Relationship sns.regplot() See linear relationship between two variables Multi-Regression Line Relationship sns.lmplot() See linear relationship between two variables involving group Categorical Scatter Plot Relationship sns.swarmplot() Observe the relationship between continuous variable and categorical variable Histogram Distribution sns.histplot() Show distribution of single numerical variable KDE Plot Distribution sns.kdeplot() Show a smooth distribution of a single or more numerical variable 2D KDE Plot Distribution sns.jointplot(kind = 'kde') Display a 2D KDE plot with each KDE correspond to each variable ","permalink":"https://keanteng.github.io/home/docs/2023-08-15-data-visualization/","summary":"In this course, we will explore on data visualization using \u003ccode\u003eseaborn\u003c/code\u003e, a Python package to visualize data with a variety of plot types. The package is powerful yet easy to use, check out the below images on the plot types that \u003ccode\u003eseaborn\u003c/code\u003e is able to generate","title":"Data Visualization"},{"content":" Images from Unsplash\nIn this project, I make use of Streamlit, which is an open-source Python library that allows us to build and deploy powerful apps with speed and efficiency. It also offers a cloud deployment feature for you to host the Streamlit app that you created online publicly through Streamlit Community Cloud.\nThe purpose of building the app is to get an understanding of the flood impacts towards collateral hold by banks. Floods are a frequent natural disaster that occurs in almost every state in Malaysia. The impact of floods extends beyond the destruction of homes, infrastructure and crops. Often, floods result in displacement of communities, disruptions to vital services like electricity, transportation and communication.\nTherefore, by collecting all the historical flood points in Malaysia and using sophisticated tool such as Google Earth Engine to estimate or forecast the flood extent using Sentinel-1 synthetic-aperture radar (SAR) data, a better understanding of the impact of the flood can be achieved, the economic impact of the flood can be evaluated, and the precautions towards future flood occurrences can be implemented.\nWith the forecasted flood data points as well as collateral location gathered, we can estimate the loss and devise strategies to minimize flood risk and economic loss.\nThe process of deploying the app is a series of workflow, as follows:\nFlood Data Collection from annual report published by Department of Drainage and Irrigation Flood data points geocoding using geocoding services: Nominatim App Building (Flood Statistics Visualization, Market Cluster/Heatmap) via Local Deployment Flood Extent Mapping Feature on Streamlit Cloud Webapp Overview\nRefer to my GitHub repository, for my work on this project, the link to the web app can be found on the landing page.\n1. Flood Data Collection The data used to visualize the flood incidents in Malaysia from 2015 to 2021 can be collected by the annual report published by the Department of Irrigation and Drainage, JPS. Due to no flood data files available online, the flood data could only be extracted by establishing data connection using Power Query on Excel. Moreover, the tables in the report has inconsistent format and typing, a series of process would need to be implemented to clean up the data set:\nData Cleaning Workflow\nThe second issue to overcome would be the huge amount of .xlsx files to merge assuming you created each file for flood incidents in each state and each year.\nTOTAL STATES = 15 (13 States and 2 Federal Territories) TOTAL YEAR = 7 AMOUNT OF FILES = 15 * 7 = 105 We can use Python openpyxl packages to quickly merge all these files with the following code:\n# importing the required modules import glob import pandas as pd # specifying the path to csv files path = folder # csv files in the path file_list = glob.glob(path + \u0026#34;/*.xlsx\u0026#34;) # list of excel files we want to merge. excl_list = [] for file in file_list: excl_list.append(pd.read_excel(file, sheet_name= \u0026#39;Sheet1\u0026#39;)) # concatenate all DataFrames in the list # into a single DataFrame, returns new # DataFrame. excl_merged = pd.concat(excl_list, ignore_index=True) # exports the dataframe into excel file # with specified name. excl_merged.to_excel(\u0026#39;FILENAME_WITH_ALL_THE_DATA.xlsx\u0026#39;, index=False) 2. Flood Data Points Geocoding To convert all the flood location addresses into coordinates identified by longitude and latitude, geocoding services such as Nominatim are used. In the Python “geopy” module, it includes the Nominatim API services for such a conversion. A geocoding program is written by me in Python to perform the conversion. The program runs for 146 minutes with a success rate of about 31%. That means, only about 5000 addresses are successfully geocoded.\nSeveral runs of the program are performed to check for improvement in success rate. No significant improvement is observed throughout all the runs due to Nominatim unable to locate a certain address in Malaysia.\n# geocoding function def my_geocoder(row): try: point = geolocator.geocode(row).point return pd.Series({\u0026#39;Latitude\u0026#39;: point.latitude, \u0026#39;Longitude\u0026#39;: point.longitude}) except: return pd.Series({\u0026#39;Latitude\u0026#39;: None, \u0026#39;Longitude\u0026#39;: None}) data[[\u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;]] = data.apply(lambda x: my_geocoder(x[\u0026#39;Name\u0026#39;]), axis=1) # check the percentage of data successfully geocoded print(\u0026#34;{}% of addresses were geocoded!\u0026#34;.format( (1 - sum(np.isnan(data[\u0026#34;Latitude\u0026#34;])) / len(data)) * 100)) 3. App Building via Local Deployment To use Streamlit, we have to install it first py -m pip install streamlit on Windows terminal. You can check out its documentation to see the list of components that can be used to build interactive and powerful web app.\nTo run the app you have created, simply type py -m streamlit run filename.py on terminal. You will deploy the app locally.\nIn my app, I included bar charts using plotly package to visualize the flood statistics that I gathered. Heatmap and Marker Cluster map were also plotted to indicated the location of flood and the areas in a particular state that experienced a higher density of flood incidents. The relevant code can be found in my Git repository.\nMarker Cluster Plot\nHeatmap\n4. Adding Additional Feature via Cloud Deployment In my project, I added some tools for flood extent analysis and features supported by Google Earth Engine using resources from mapaction/flood mapping tool and opengeos/streamlit-geospatial.\nSelecting the area for prediction\nPrediction outcome\nThe reason these features need to be added via cloud deployment of Streamlit is because of a missing package fcntl module that is not available on my Windows machine but on Linux system. Of course, after deploying these feature to cloud, authentication token from Google Earth Engine would also be required. To get the key, got to Windows Terminal:\npy -m pip install ee import ee ee.Authenticate() You will need to paste the authorization code back on the terminal. Once the step is complete, you can find the token on your local machine at C:\\\\Users\\\\Username\\\\.congif\\\\earthengine\\\\credentials. It is also important to note that the token at C:\\\\Users\\\\Username\\\\.congif\\\\earthengine\\\\credentials would need to be put at the secret section of the app that you deployed on cloud using the following format:\nEARTHENGINE_TOKEN = \u0026#39;PASTE WHAT YOU COPY HERE\u0026#39; ee_keys = \u0026#39;PASTE WHAT YOU COPY HERE\u0026#39; The secret section can be found by clicking the app setting \u0026gt; secret.\nAfter that, the app is ready to be shared and used!\n","permalink":"https://keanteng.github.io/home/docs/2023-08-13-a-streamit-app-for-flood-analysis/","summary":"In this project, I make use of Streamlit, which is an open-source Python library that allows us to build and deploy powerful apps with speed and efficiency. It also offers a cloud deployment feature for you to host the Streamlit app that you created online publicly through Streamlit Community Cloud.","title":"A Streamlit App For Flood Analysis"},{"content":" Images from Unsplash\nIn econometrics, the ordinary least square (OLS) model is widely used to estimate the parameter of a linear regression model.\ny = beta0 + beta1*x + epsilon Of course, the model can be used to model risk such as flood risk as well. From the equation above, we let y represents the flood risk. y would be a continuous variable, like a flood risk score. It also could be represented as a binary outcome: 0 for no risk and 1 for risk present. The x is the distance to the historical flood location, it is the distance to the nearest historical flood location. For 0, it is the intercept, it shows the estimated flood risk when x is 0 or when the location is exactly the historical flood site. 1, represents the slope. It is expected to be negative as greater distances from historical flood sites correspond to lower flood risks. For the error term, epsilon, it captures the unobserved factors that affect flood risk but are not included in the model.\nThe outcome from the model will be presented on Streamlit web app together with a flood prediction service page to receive user location input and return the flood risk on the input location.\nThe Streamlit Web App\nRefer to my GitHub repository, for my work on this project, the link to the web app can be found on the landing page.\n1. The Workflow Random address generation using web scraping Geocoding, computing distance and response variable Model fitting and evaluation Models comparison Creating and deploying web app 2. Random Address Generation Using Web Scraping Fitting of the flood risk model requires a predictor and a response variable. The predictor variable is collected by computing the minimum distance between a random location and the nearest historical flood location. To obtain the random location, Python selenium module is used to scrape random addresses online by locating the Xpath components on web pages. About 16, 000 random locations are scraped in 3 hours.\nThe data collected contains information such as state, region, country, addresses, zip code and phone number.\nWeb Scraping In Action\nRefer to my GitHub for the corresponding notebook.\n3. Geocoding, Computing Distance And Response Variable Geocoding services like Nominatim is used to convert the random addresses generated into geographic coordinate with longitude and latitude.\n# geocoding function def my_geocoder(row): try: point = geolocator.geocode(row).point return pd.Series({\u0026#39;Latitude\u0026#39;: point.latitude, \u0026#39;Longitude\u0026#39;: point.longitude}) except: return pd.Series({\u0026#39;Latitude\u0026#39;: None, \u0026#39;Longitude\u0026#39;: None}) data[[\u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;]] = data.apply(lambda x: my_geocoder(x[\u0026#39;Location\u0026#39;]), axis=1) # check the percentage of data successfully geocoded print(\u0026#34;{}% of addresses were geocoded!\u0026#34;.format( (1 - sum(np.isnan(data[\u0026#34;Latitude\u0026#34;])) / len(data)) * 100)) To compute for the minimum distance between any random location with the nearest historical flood location, geopandas module is required. Since initially, the coordinate is in the format of EPSG: 4326, to ensure that we obtain distance output in meter, we would need to convert the coordinate to the format of EPSG: 3857:\ngeocoded_data = gpd.GeoDataFrame(geocoded, geometry = gpd.points_from_xy(geocoded.Longitude, geocoded.Latitude), crs = \u0026#34;EPSG:4326\u0026#34;).to_crs(\u0026#39;EPSG:3857\u0026#39;) flood_points_data = gpd.GeoDataFrame(flood_points, geometry = gpd.points_from_xy(flood_points.Longitude, flood_points.Latitude), crs = \u0026#34;EPSG:4326\u0026#34;).to_crs(\u0026#39;EPSG:3857\u0026#39;) for i in range(0, len(geocoded_data)): distances = flood_points_data.geometry.distance(geocoded_data.iloc[i].geometry) geocoded_data.loc[i, \u0026#39;distance\u0026#39;] = distances.min() Subsequently, the response variable is computed by assuming no flood risk if the distance is less than 500 m, otherwise, flood risk exists.\n4. Model Fitting And Evaluation The flood risk model is fitted using LogisticRegression() from the scikit module. The model requires at least two dimensions predictor variables. Thus, One-Hot encoding is applied to the state column before model fitting taking place.\nOne Hot Encoding Illustration\n# train test split X = data[[\u0026#39;distance\u0026#39;,\u0026#39;state\u0026#39;]] y = data[\u0026#39;flood_risk\u0026#39;] X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0) X_train.shape, X_valid.shape, y_train.shape, y_valid.shape # Apply one-hot encoder to each column with categorical data OH_encoder = OneHotEncoder(handle_unknown=\u0026#39;ignore\u0026#39;, sparse_output=False) # ori: spare = False # Get list of categorical variables s = (X_train.dtypes == \u0026#39;object\u0026#39;) object_cols = list(s[s].index) OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols])) OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols])) # One-hot encoding removed index; put it back OH_cols_train.index = X_train.index OH_cols_valid.index = X_valid.index # Remove categorical columns (will replace with one-hot encoding) num_X_train = X_train.drop(object_cols, axis=1) num_X_valid = X_valid.drop(object_cols, axis=1) # Add one-hot encoded columns to numerical features OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1) OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1) 4.1 Model Evaluation The model is evaluated based on its accuracy, training and test score.\n# print the scores on training and test set print(\u0026#39;Training set score: {:.4f}\u0026#39;.format(logreg.score(OH_X_train, y_train))) print(\u0026#39;Test set score: {:.4f}\u0026#39;.format(logreg.score(OH_X_valid, y_valid))) The model is able to achieve an accuracy of 0.9880, which is remarkably accurate. The model achieved an accuracy score of 0.9880, training set accuracy of 0.9928. Furthermore, the training set score and the test set score are very close to each other, which means that the model is not overfitting.\nTest Set Outcome\nGenerally, training score measures how to model fit in the training data. If a model fits so well in a data with lots of variance it results in overfitting. This will result in a poor test score. The mode curved a lot to fit the training data and generalized very poorly. For test score, since we implement train-test split before fitting the model, it represents a real life scenario. Thus, the higher the test score, the better.\nThe model is then further evaluated with confusion matrix, training and test set score, receiver operating curve (ROC) as well as cross-validation. Of course, comparisons were made with other models such as KNN, SVM, XGB and more. However, logistic regression is chosen as it is able to achieve a high accuracy score with only slightly poor performance compared to XGB classifier (negligible differences).\nMulti-models Comparison\nFrom the confusion matrix below, it can be observed that the true positive is 251, true negative is 575 while the false negative is 8 and the false positive is 2. These results show that our model performs very well in predicting the flood risk of a location in Malaysia. Furthermore, we compute the accuracy (0.9880), classification error (0.0120), precision (0.9921) and sensitivity (0.9691).\n# confusion matrix from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay cm = confusion_matrix(y_valid, y_pred_test) # visualize confusion matrix with seaborn heatmap disp = ConfusionMatrixDisplay(confusion_matrix=cm) disp.plot() Confusion Matrix\nPrecision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). Precision identifies the proportion of correctly predicted positive outcomes. It is more concerned with the positive class than the negative class. Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). Recall is also called Sensitivity.\nfrom sklearn.metrics import classification_report print(classification_report(y_valid, y_pred_test)) ROC curve stands for receiver operating characteristic curve. It shows the performance of classification models at various classification threshold levels. The curve also shows the performance of a classification model at various classification threshold levels. From the image below, we can see that the area under the curve is closed to one. Perfect classifier will have AUC equal to 1 whereas a random classifier will have a ROC-AUC equal to 0.5. Since our model ROC-AUC approximates to 1, we can conclude that our classifier does a good job in predicting whether a location has a flood risk or no flood risk.\nfrom sklearn.metrics import roc_auc_score from sklearn.metrics import roc_curve logit_roc_auc = roc_auc_score(y_valid, logreg.predict(OH_X_valid)) fpr, tpr, thresholds = roc_curve(y_valid, logreg.predict_proba(OH_X_valid)[:,1]) plt.figure() plt.plot(fpr, tpr, label=\u0026#39;Logistic Regression (area = %0.2f)\u0026#39; % logit_roc_auc) plt.plot([0, 1], [0, 1],\u0026#39;r--\u0026#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.title(\u0026#39;Receiver operating characteristic\u0026#39;) plt.legend(loc=\u0026#34;lower right\u0026#34;) plt.savefig(\u0026#39;images/Log_ROC.png\u0026#39;) ROC Curve\n5. Creating And Deploying Web App With the fitted model, a Streamlit web app is created to present my workflows, analysis and findings. A flood prediction service is added to predict flood risk based on user location input. By typing a location in the input box, a function will be called to compute the distance of the input and the nearest flood data points. The distance computed is then passed to the logistic function for flood risk computation. The flood prediction service works based on the logistic function used for the study. After all the computation, an output in 0 or 1 will be returned to the user.\nFlood Risk Prediction Service Page On Streamlit Web App\n","permalink":"https://keanteng.github.io/home/docs/2023-08-13-flood-risk-modeling/","summary":"In econometrics, the ordinary least square (OLS) model is widely used to estimate the parameter of a linear regression model.","title":"Flood Risk Modeling With Logistic Regression"},{"content":" Images from Unsplash\nGoogle Sheet importxml() function is a wonderful tool to allow you to scrape for website information by just identifying the corresponding Xpath. But it has a limit, when you have hundreds or thousands of data items to be gathered, your Sheet\u0026rsquo;s cell will get stuck at endless loading. As you can see from the image below, for about 250 rows of data, after waiting for more than 30 minutes, the loading cell does not refresh.\nGoogle Sheet Data Extraction Using IMPORTXML()\nLuckily, the Python selenium package offers a powerful solution to this issue. With just a few lines of code, you can scrape hundreds or even thousands rows of data and compile them into a Python data frame for you to perform further analysis, with speed and efficiency.\n1. What is Selenium Selenium is a project that provides a range of tools and libraries to support the automation of web browsers. It supports extensions to emulate user interaction with browsers, a distribution of servers for scaling browser allocation and allows you to write interchangeable code for all major web browsers.\nYou can install the package by writing this code on Windows Terminal:\npy -m pip install selenium 2. Finding Xpath Previously, I have shown the Random Malaysia Address on Google Sheet, where IMPORTXML() is used to scrape for random addresses. The website used is here.\nSite For Random Address Generation\nThis site allows user to generate 20 random addresses at once and the addresses are shown in several blue boxes as shown in the above images. In order to locate the addresses, or in other words, all the information inside each blue boxes, we would need to locate their Xpath. The Xpath can be found by opening the developer mode on your web browser by pressing Ctrl + Shift + I and search for the required element. Once you have found the required element, right click and select Copy Full Xpath. The Xpath is as follows:\n/html/body/section[2]/div/div[2]/div[2]/ul/li[1] As mentioned earlier, there are 20 random addresses generated. Thus, notice that the * in /html/body/section[2]/div/div[2]/div[2]/ul/li[*], it will be numbered from 1 to 20, representing 20 different boxes containing random addresses.\nFinding Site Element Xpath\n3. Web Scraping In Action Everything is easy after knowing the Xpath of the web page element you want. We will start by importing the required package:\nfrom selenium import webdriver from selenium.webdriver.common.by import By import time import pandas as pd import random Then, we need to write a function to extract the random address based on the Xpath we copied. We will write a get_address function for that.\ndef get_address(url): driver = webdriver.Chrome() driver.get(url) time.sleep(2) container1 = driver.find_element(By.XPATH, \u0026#34;/html/body/section[2]/div/div[2]/div[2]/ul/li[1]\u0026#34;) driver.close() return container1.text But the output will be in this format:\n//output: Street: G Kenyalang Shopping Centre 6D Jln Datuk Sim Kheng Hong Kenyalang Park Kuching Ma City: Kuching State/province/area: Sarawak Phone number 08233-1761 Zip code 93300 Country calling code +60 Country Malaysia //What we want: Street: G Kenyalang Shopping Centre 6D Jln Datuk Sim Kheng Hong Kenyalang Park Kuching Ma; City: Kuching; State/province/area: Sarawak; Phone number 08233-1761; Zip code 93300; Country calling code +60; Country Malaysia The output makes us hard to put in a data frame. Therefore, we need an additional function called textline to convert the output to one string as shown above:\ndef get_address(url): driver = webdriver.Chrome() driver.get(url) time.sleep(2) container1 = driver.find_element(By.XPATH, \u0026#34;/html/body/section[2]/div/div[2]/div[2]/ul/li[1]\u0026#34;) container2 = driver.find_element(By.XPATH, \u0026#34;/html/body/section[2]/div/div[2]/div[2]/ul/li[2]\u0026#34;) def textLine(poem): lst=list(poem) string=\u0026#39;\u0026#39; for i in lst: string+=i # print(string) lst1=string.split(\u0026#34;\\n\u0026#34;) str1=\u0026#34;\u0026#34; for i in lst1: str1+=i+\u0026#34; ;\u0026#34; str2=str1[:-2] return str2 location1 = textLine(container1.text) location2 = textLine(container2.text) driver.close() temp = [location1, location2] return temp Now, we can put the output into a Python data frame:\nurl = https://www.bestrandoms.com/random-address-in-my?quantity=20 df = pd.DataFrame({\u0026#39;address\u0026#39;:get_address(url)}) And we are done! You can also write a loop if you need more random locations as each function can return at most 20 random locations.\n4. Scrape More Than 1000 Locations Check out my GitHub for the full Jupyter notebook to scrape more than 1000 random locations with a .csv output at the end.\n","permalink":"https://keanteng.github.io/home/docs/2023-08-13-webscraping-on-xpath/","summary":"Google Sheet IMPORTXML() function is a wonderful tool to allow you to scrape for website information by just identifying the corresponding Xpath. But it has a limit, when you have hundreds or thousands of data items to be gathered, your Sheet\u0026rsquo;s cell will get stuck at endless loading.","title":"Webscraping On Xpath"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nGeospatial analysis is the gathering, display and manipulation of imagery, GPS, satellite photography and historical data, described explicitly in terms of geographic coordinates. This course will learn on methods to visualize geospatial data and perform some analysis concerning a particular geographic location or region.\nSome interesting questions that can be addressed with geospatial analysis are:\nWhich areas affected by earthquakes would require additional reinforcement? Where should a popular coffee shop select its next store, if it is considering an expansion? With forest conservation areas set up in some regions, will animals migrate to those areas or other areas instead? 1. Creating Maps To visualize geographic coordinates as a map, we need the help of geopandas library. Note that there are a few geospatial file formats available such as shapefile, GeoJSON, KML and GPKG. But all the files can be loaded with geopandaslibrary:\n# read the shape file full_data = gpd.read_file(\u0026#34;file_name\u0026#34;) Here\u0026rsquo;s how we can create a map for a geospatial data file. In fact, in every GeoDataFrame, there will be a geometry column that describe the geometric objects when we display them with the plot() function. They can be a point, linestring or polygon :\n## 1. Prework # plot the data only wild_lands.plot() # campsites point POI_data = gpd.read_file(\u0026#34;../input/geospatial-learn-course-data/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp\u0026#34;) campsites = POI_data.loc[POI_data.ASSET==\u0026#39;PRIMITIVE CAMPSITE\u0026#39;].copy() # foot trails as linestring roads_trails = gpd.read_file(\u0026#34;../input/geospatial-learn-course-data/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp\u0026#34;) trails = roads_trails.loc[roads_trails.ASSET==\u0026#39;FOOT TRAIL\u0026#39;].copy() # country boundaris as polygon counties = gpd.read_file(\u0026#34;../input/geospatial-learn-course-data/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp\u0026#34;) ## 2. Visualize the map # Plot a base map with counties boundaries ax = counties.plot(figsize=(10,10), color=\u0026#39;none\u0026#39;, edgecolor=\u0026#39;gainsboro\u0026#39;, zorder=3) # Add in the campsites and foot trails wild_lands.plot(color=\u0026#39;lightgreen\u0026#39;, ax=ax) campsites.plot(color=\u0026#39;maroon\u0026#39;, markersize=2, ax=ax) trails.plot(color=\u0026#39;black\u0026#39;, markersize=1, ax=ax) Point, linestring and polygon (geometric objects)\n2. Coordinate Reference System In order to create a GeoDataFrame, we have to set the CRS. The CRS is referenced by European Petroleum Survey Group code - EPSG 32630 used by GeoDataFrame or also known as the \u0026ldquo;Mercator\u0026rdquo; projection that preserves angles and slightly distorts area. What\u0026rsquo;s more, EPSG 4326 corresponds to coordinates in latitude and longitude. It is a coordinate system of latitude and longitude based on an ellipsoidal (squashed sphere) model of the earth.\nHere\u0026rsquo;s how to do it in code:\n# read file facilities_df = pd. read_csv(\u0026#34;file_name\u0026#34;) # convert to geodataframe facilities = gpd.GeoDataFrames(facilities_df, geometry = points_from_xy(facilities_df.Longitude, facilities_df.Latitiude)) # set crs facilities.crs = {\u0026#39;init\u0026#39;: \u0026#39;epsg:4326\u0026#39;} # view first five rows facilities.head() It is also possible to change the CRS so that we can have datasets with matching CRS. If we cannot do it with code, alternatively, we can use proj4 string of CRS to convert to latitude and longitude coordinates using +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs:\n# match facilities crs with regions ax = regions.plot(figsize=(8,8), color=\u0026#39;whitesmoke\u0026#39;, linestyle=\u0026#39;:\u0026#39;, edgecolor=\u0026#39;black\u0026#39;) facilities.to_crs(epsg=32630).plot(markersize=1, ax=ax) # change CRS to EPSG 4326 and display the data regions.to_crs(\u0026#34;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\u0026#34;).head() 2.1 Geometric Objects Attributes Previously, we introduced the geometry column in a GeoDataFrame, in fact they are built-in attributes that could help to give us some interesting information about our data. For example, we want to find the coordinates of each point, the length of linestring or the area of a polygon.\n# find points x-coordinate facilities.geometry.head().x # find area of polygons regions.loc[:, \u0026#34;AREA\u0026#34;] = regions.geometry.area/ 10**6 3. Interactive Maps In this section, we will explore methods to plot interactive maps such as heatmaps, points and choropleth maps. These features are enabled by the folium package.\nSome maps to plot with folium package:\nSimple Map Markers/ Bubbles Clustered markers Heatmaps Choropleth maps To create a simple map for visualization as follows:\n# Create a map m_1 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;openstreetmap\u0026#39;, zoom_start=10) # Display the map m_1 A simple map\nLet\u0026rsquo;s say we want to add some markers to the map. Here\u0026rsquo;s a case where we add markers to denote places that experienced robbery on the map:\n# data preparatin daytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == \u0026#39;Robbery\u0026#39;) \u0026amp; \\ (crimes.HOUR.isin(range(9,18))))] # Create a map m_2 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;cartodbpositron\u0026#39;, zoom_start=13) # Add points to the map for idx, row in daytime_robberies.iterrows(): Marker([row[\u0026#39;Lat\u0026#39;], row[\u0026#39;Long\u0026#39;]]).add_to(m_2) # Display the map m_2 Maps with markers\nNow notice that the markers are scattered all over the place. It is possible to cluster together the markers when we zoom out the map and let the markers spread out as we zoom in with the help of MarkerCluster():\n# Create the map m_3 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;cartodbpositron\u0026#39;, zoom_start=13) # Add points to the map mc = MarkerCluster() for idx, row in daytime_robberies.iterrows(): if not math.isnan(row[\u0026#39;Long\u0026#39;]) and not math.isnan(row[\u0026#39;Lat\u0026#39;]): mc.add_child(Marker([row[\u0026#39;Lat\u0026#39;], row[\u0026#39;Long\u0026#39;]])) m_3.add_child(mc) # Display the map m_3 Clustered markers\nAn alternative to markers on map, we could also use circle for the same purpose - that is bubble maps:\n# Create a base map m_4 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;cartodbpositron\u0026#39;, zoom_start=13) def color_producer(val): if val \u0026lt;= 12: return \u0026#39;forestgreen\u0026#39; else: return \u0026#39;darkred\u0026#39; # Add a bubble map to the base map for i in range(0,len(daytime_robberies)): Circle( location=[daytime_robberies.iloc[i][\u0026#39;Lat\u0026#39;], daytime_robberies.iloc[i][\u0026#39;Long\u0026#39;]], radius=20, color=color_producer(daytime_robberies.iloc[i][\u0026#39;HOUR\u0026#39;])).add_to(m_4) # Display the map m_4 Bubble map\nNow consider that among few cities with different crime rate. We would like to visualize whether which city has relatively more criminal incidents than the other, a heatmap would do a good job to show us which areas of a city are susceptible to more criminal cases:\n# Create a base map m_5 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;cartodbpositron\u0026#39;, zoom_start=12) # Add a heatmap to the base map HeatMap(data=crimes[[\u0026#39;Lat\u0026#39;, \u0026#39;Long\u0026#39;]], radius=10).add_to(m_5) # Display the map m_5 Heat map\nWell, you should notice that heatmap makes geographic boundaries between different areas non-distinguishable. We can also use choropleth maps instead to visualize the crime rate by district.\n# GeoDataFrame with geographical boundaries of Boston police districts districts_full = gpd.read_file(\u0026#39;../input/geospatial-learn-course-data/Police_Districts/Police_Districts/Police_Districts.shp\u0026#39;) districts = districts_full[[\u0026#34;DISTRICT\u0026#34;, \u0026#34;geometry\u0026#34;]].set_index(\u0026#34;DISTRICT\u0026#34;) districts.head() # Number of crimes in each police district plot_dict = crimes.DISTRICT.value_counts() plot_dict.head() # Create a base map m_6 = folium.Map(location=[42.32,-71.0589], tiles=\u0026#39;cartodbpositron\u0026#39;, zoom_start=12) # Add a choropleth map to the base map Choropleth(geo_data=districts.__geo_interface__, data=plot_dict, key_on=\u0026#34;feature.id\u0026#34;, fill_color=\u0026#39;YlGnBu\u0026#39;, legend_name=\u0026#39;Major criminal incidents (Jan-Aug 2018)\u0026#39; ).add_to(m_6) # Display the map m_6 Choropleth map\n4. Manipulating Geospatial Data When we are using application such a Google Maps, we could easily get a place location on the map by just knowing the address or name. In fact, we are using what it\u0026rsquo;s known as geocoder to generate locations of the places that we want to go.\nHere\u0026rsquo;s an interesting example where we try to geocode 100 top universities in Europe with only the universities name:\n# 1. prepare a dataset with the universitites name universities = pd.read_csv(\u0026#34;../input/geospatial-learn-course-data/top_universities.csv\u0026#34;) universities.head() # 2. apply geocode to each of the universitites def my_geocoder(row): try: point = geolocator.geocode(row).point return pd.Series({\u0026#39;Latitude\u0026#39;: point.latitude, \u0026#39;Longitude\u0026#39;: point.longitude}) except: return None universities[[\u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;]] = universities.apply(lambda x: my_geocoder(x[\u0026#39;Name\u0026#39;]), axis=1) print(\u0026#34;{}% of addresses were geocoded!\u0026#34;.format( (1 - sum(np.isnan(universities[\u0026#34;Latitude\u0026#34;])) / len(universities)) * 100)) # Drop universities that were not successfully geocoded universities = universities.loc[~np.isnan(universities[\u0026#34;Latitude\u0026#34;])] universities = gpd.GeoDataFrame( universities, geometry=gpd.points_from_xy(universities.Longitude, universities.Latitude)) universities.crs = {\u0026#39;init\u0026#39;: \u0026#39;epsg:4326\u0026#39;} universities.head() Now let\u0026rsquo;s plot out the locations to see if they are accurate:\n# Create a map m = folium.Map(location=[54, 15], tiles=\u0026#39;openstreetmap\u0026#39;, zoom_start=2) # Add points to the map for idx, row in universities.iterrows(): Marker([row[\u0026#39;Latitude\u0026#39;], row[\u0026#39;Longitude\u0026#39;]], popup=row[\u0026#39;Name\u0026#39;]).add_to(m) # Display the map m European universitites\n4.1 Table Joins In this section, we will explore on combining data frames with shared index for the case of GeoDataFrame. An example is that we have a dataset with boundaries of every country in Europe and a dataset with their estimated population and GDP, we can perform attribute join to merge the two datasets:\neurope = europe_boundaries.merge(europe_stats, on=\u0026#34;name\u0026#34;) europe.head() Furthermore, it is also possible to merge GeoDataFrame based on spatial relationship between objects in geometry columns. Recall back that we geocode top 100 universities in Europe previously. Can we match each university with its corresponding country? Spatial join allow us to perform match for such as scenario.\nThe spatial join above looks at the \u0026ldquo;geometry\u0026rdquo; columns in both GeoDataFrames. If a Point object from the universities GeoDataFrame intersects a Polygon object from the europe DataFrame, the corresponding rows are combined and added as a single row of the european_universities DataFrame. Otherwise, countries without a matching university (and universities without a matching country) are omitted from the results.\n# Use spatial join to match universities to countries in Europe european_universities = gpd.sjoin(universities, europe) # Investigate the result print(\u0026#34;We located {} universities.\u0026#34;.format(len(universities))) print(\u0026#34;Only {} of the universities were located in Europe (in {} different countries).\u0026#34;.format( len(european_universities), len(european_universities.name.unique()))) european_universities.head() 5. Proximity Analysis For the previous four sections, we are exposed to a lot of the functions in geopandas. Here, we will look into some application areas with the learned functions:\nSome useful application:\nMeasuring distance between points on a map Select points within some radius of a feature To compute distances from two GeoDataFrames, we need to make sure they have the same CRS. The distances can be easily computed in GeoPandas. Moreover, we can find the mean distance between two points with mean too. Here\u0026rsquo;s an example where we deal with dataset with air quality monitoring stations in the same city where we would like to know the mean distance from one monitoring station to the other:\n# check CRS of geodataframes print(stations.crs) print(releases.crs) # Select one release incident in particular recent_release = releases.iloc[360] # Measure distance from release to each station distances = stations.geometry.distance(recent_release.geometry) distances # find the mean distance print(\u0026#39;Mean distance to monitoring stations: {} feet\u0026#39;.format(distances.mean())) 5.1 Creating a Buffer The purpose of creating a buffer is for us to understand points on a map that lies some radius away from a point. For example, there\u0026rsquo;s a toxic gas being release accidentally to the air. There are some air quality monitoring centers nearby. We want to know whether those centers are able to detect the toxic gas.\nA working example would be as follows:\n# 1. creating a two miles buffer two_mile_buffer = stations.geometry.buffer(2*5280) two_mile_buffer.head() # 2. create map with release incidents and monitoring stations m = folium.Map(location=[39.9526,-75.1652], zoom_start=11) HeatMap(data=releases[[\u0026#39;LATITUDE\u0026#39;, \u0026#39;LONGITUDE\u0026#39;]], radius=15).add_to(m) for idx, row in stations.iterrows(): Marker([row[\u0026#39;LATITUDE\u0026#39;], row[\u0026#39;LONGITUDE\u0026#39;]]).add_to(m) # Plot each polygon on the map GeoJson(two_mile_buffer.to_crs(epsg=4326)).add_to(m) # Show the map m Now we want to check if a toxic release occurred within 2 miles of any monitoring station, to do that we would need to test each polygon. But that would be tedious work. Consider combining all the polygons into one object, we can check whether the toxic gas is within the radar of the closest monitoring station:\n# check if output is true my_union.contains(releases.iloc[360].geometry) ","permalink":"https://keanteng.github.io/home/docs/2023-04-19-geospatial-analysis/","summary":"Geospatial analysis is the gathering, display and manipulation of imagery, GPS, satellite photography and historical data, described explicitly in terms of geographic coordinates. This course will learn on methods to visualize geospatial data and perform some analysis concerning a particular geographic location or region.","title":"Geospatial Analysis"},{"content":" Images from Unsplash\nThis article aims to provide a comprehensive overview of how this site is set up and run.\nPre-requisite There are a few things to prepare before your own site can be created, as follows:\nA GitHub account Installed Microsoft Visual Studio Code Installed Git After you create a GitHub account and installing the necessary software, we are ready to begin!\nWebsite Foundations Setup The site that we are going to create will be based on Hugo, an open-source site generators where it provides a framework for us to deploy a site with speed and ease.\nTo build our website, we will need to install Hugo into our local machine. The installation process will be done on Windows Terminal with the support of Go language. Of course, you can also use other language such as chocolatey, scoop and winget.\nAfter installing go, here\u0026rsquo;s how to install Hugo on terminal:\ngo install -tags extended github.com/gohugoio/hugo@latest hugo version # check if you are using the latest version With Hugo installed in our local system, we would like to now create the foundation or framework for our website. Here I encourage you to change your directory first, preferably to \\Desktop so that you can access all your files easily:\ncd C:\\Users\\Username\\Desktop hugo new site \u0026lt;your_site_name\u0026gt; -f yml Now change your directory again and create a page on your website:\ncd \u0026lt;your_site_name\u0026gt; hugo new docs/page.md Website Theme Setup With the site foundation ready, we now add a theme or a specific design to our site for better functionality and appearance. You can check out different theme here, but in my case we will proceed with the PaperMode theme.\nHere we will use git to install the website theme:\ngit init git clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod After that, navigate to GitHub to create an empty repository and head back to terminal where we need to link up our files with the repository created. Since we are using an empty repository, we need to create a first file, usually README.md to avoid causing any error:\necho \u0026#34;# Test\u0026#34; \u0026gt;\u0026gt; README.md git add README.md git commit -m \u0026#34;Initial commit\u0026#34; git branch -M main git remote add origin https://github.com/YOUR-GIT-NAME/REPOSITORY-NAME.git git push -u origin main If you want to see how your website look like, you can deploy your site locally using Hugo. This is a good practice to check for error and website update before deploy your site publicly. Simply click the link or type 127.0.0.1 on your web browser:\nhugo server Website Deployment Workflow Just now your were shown to deploy the site locally, now to do it publicly - meaning on the web, you need the support from GitHub workflow.\nHere we will need to create an additional directory and put in some codes into it:\nmkdir -p .github/workflows After creating this directory, create a file with name deploy.yml in the workflow folder. Then navigate these two files (your can use file explorer) and put in these codes:\nconfig.yml After copying, change the first line baseurl: to the following format \u0026quot;https://YOUR-NAME-ON-GITHUB.github.io/REPOSITORY-NAME/\u0026quot; copy the code from here (https://github.com/adityatelange/hugo-PaperMod/blob/exampleSite/config.yml) deploy.yml name: Publish to GH Pages on: push: branches: - main pull_request: jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout source uses: actions/checkout@v3 with: submodules: true - name: Checkout destination uses: actions/checkout@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: ref: gh-pages path: built-site - name: Setup Hugo run: | curl -L -o /tmp/hugo.tar.gz \u0026#39;https://github.com/gohugoio/hugo/releases/download/v0.110.0/hugo_extended_0.110.0_linux-amd64.tar.gz\u0026#39; tar -C ${RUNNER_TEMP} -zxvf /tmp/hugo.tar.gz hugo - name: Build run: ${RUNNER_TEMP}/hugo - name: Deploy if: github.ref == \u0026#39;refs/heads/main\u0026#39; run: | cp -R public/* ${GITHUB_WORKSPACE}/built-site/ cd ${GITHUB_WORKSPACE}/built-site git add . git config user.name \u0026#39;keanteng\u0026#39; # change to your username git config user.email \u0026#39;u2004763@siswa.um.edu.my\u0026#39; # change to your email git commit -m \u0026#39;Updated site\u0026#39; git push Before we link up our local files with the repository, you need to create a new branch on your repository called git-pages and you need to change the setting for GitHub actions.\nChange the setting here\nFinal Step For the last part of the website setup, we will link up our local files with the repository created so that we can view our site online:\ngit status git add . git commit -m \u0026#34;site update\u0026#34; git push Now, just head to GitHub actions and click on pages build and deployment, and click on your website link on the web!\nClick on the link to view your site\nAcknowledgements✨ I would like to thank Hugo and PaperMode for empowering me with the tool and foundations to build such a beautiful and impressive site. I want to thank dhij for the wonderful tutorial on YouTube for the site set-up, I failed on numerous attempt to set up the site by looking at some blogs on Medium until I look up his video. I am inspired to make this blog as comprehensive as possible so anyone that reads it can follow successfully.\n","permalink":"https://keanteng.github.io/home/docs/2023-04-09-creating-a-website-with-hugo--papermode/","summary":"There are a few things to prepare before your own site can be created, as follows: A GitHub account Installed Microsoft Visual Studio Code Installed Git. After you create a GitHub account and installing the necessary software, we are ready to begin!","title":"Creating a Website With Hugo \u0026 PaperMode"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nComputer vision literally means computer able to see and recognize stuff. Applications such as Google Lens and Google Image Search are some good examples of where computer vision is being used in our daily life.\nIn this course, we will explore some technique used to empower computer with the power of seeing:\nBuilding an image classifier with Keras Concepts of visual feature extraction Custom covnet Apply data augmentation to extend dataset. 1. Convolutional Classifier Convolutional neural networks or “covnet” is a neural network specializes in computer vision. A covnet used for image classification has two parts: a convolutional base and a dense head. The base is used to extract the features of an image while the head is used to determine the class of the image. So, the aims of training a neural network are simply to know which feature to extract from a particular image and to know which class the image will belong from the features.\nThe process\nHere is how we can train a covnet on Python to recognize a car and a truck:\n# 1. load pretrained base pretrained_base = tf.keras.models.load_model( # file path \u0026#39;../input/cv-course-models/cv-course-models/vgg16-pretrained-base\u0026#39;, ) pretrained_base.trainable = False # 2. attach classfier head from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ pretrained_base, layers.Flatten(), # transform 2d output to 1d layers.Dense(6, activation = \u0026#39;relu\u0026#39;), layers.Dense(1, activation = \u0026#39;sigmoid\u0026#39;), # transform output to class probability (truck) ]) # 3. model fitting model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;binary_crossentropy\u0026#39;, metrics = [\u0026#39;binary_accurary\u0026#39;], ) history = model.fit( ds_train, validation_data = ds_valid, epochs = 30, verbose = 0, ) # 4. visualize model loss import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_frame.loc[:, [\u0026#39;binary_accurary\u0026#39;, \u0026#39;val_binary_accurary\u0026#39;]].plot(); 2. Features Extraction 2.1 Convolution and ReLU The process of feature extraction does three things. It filters an image for a certain feature; it detects the feature within the filtered image, and it condenses the image to enhance the features.\nFeatur extraction overview\nIn training, the covnet will learn weights from image features and the weights are contained in the convolutional layers. The weights are known as kernels which can be presented as an array of number. A kernel will scan over an image and produce a weighted sum of pixel values (finding the best kernel values) — emphasizing and de-emphasizing certain image patterns and information.\nThe process\nActivations in the network is called feature maps. Feature maps is the result when filter is applied to image — it contains features a kernel extract.\nApplying filter\nAfter filtering, the feature maps will be passed to an activation function which can be though as scoring pixel values according to some measure of importance. For example, ReLU activation assumes negative values are not important, so they are set to zero. In fact, these images are how the head of a network is able to solve the classification problem — looking for a particular characteristic of images that we want to classify.\nAfter passing through activation function\nHere’s how to can perform feature extraction in Python:\n# define kernel import tensorflow as tf kernel =tf.constant([ [-1, -1, -1], [-1, 8, -1], [-1, -1, -1], ]) plt.figure(figsize = (3,3)) show_kernel(kernel) # applying kernel image_filter = tf.nn.conv2d( input = image, fitlers = kernel, strides = 1, # section 3 padding = \u0026#39;SAME\u0026#39;, ) plt.figure(figsize = (6,6)) plt.imshow(tf.squeeze(image_filter)) plt.axis(\u0026#39;off\u0026#39;) plt.show(); # applying activation function image_detect = tf.nn.relu(image_filter) plt.figure(figsize = (6,6)) plt.imshow(tf.squeeze(image_filter)) plt.axis(\u0026#39;off\u0026#39;) plt.show(); 2.2 Maximum Pooling from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Conv2D(filters=64, kernel_size=3), # activation is None layers.MaxPool2D(pool_size=2), # More layers follow ]) Notice that after the Conv2D layer, we will apply a MaxPool2D layer for the condensation step. This layer will not contain any trainable weights as of the previous layer, but it will condense the feature maps to only retain important feature. This is what maximum pooling does. It takes patches of activations in the original feature maps and replaces them with maximum activation in those patches. The pooling steps will increase the proportions of active pixels to zero pixels — intensifying the feature after ReLU activation.\nMaximum pooling\nIs zero pixels unimportant? In fact, zero pixels carries positional information and MaxPool2D function will remove them (positional information of the feature maps) and this will lead to a property in covnet known as translation invariance.\nThis means that a covnet with maximum pooling tend to not distinguish features by their location in image. Notice from the first row of images below, after repeated pooling, the positional information is destroyed and no longer distinguishable. But pooling only causes translation invariance in network over small distance. The second row of the image features two dots far apart and this feature remains distinct after repeated pooling.\nIn fact, such invariance is good for an image classifier as it reduces data for training since we do not need to teach the network differences in perspective and framing when same features are positioned on different part of an original image.\n3. Sliding Window Both convolution and pooling steps are performed over a sliding window with parameters “kernel_size” for convolution and “pool_size” for pooling.\nNotice in section 2.1, when we perform pooling, there are two extra parameters: strides and padding. strides means how far the window will move each step and padding describes how the pixels at the edge are being handle.\nPooling layer will almost always have stride values greater than 1, like (2, 2) or (3, 3), but not larger than the window itself.\nConsidering the sliding window process, is it necessary to always stay within the boundary? In fact, there is a trade-off between staying within and out of bound by changing the parameter padding in our code:\npadding = ‘valid’: Convolution window stay entirely inside input. Output will shrink. It will shrink more for larger kernels. This will limit the number of layers contained in a network, notably small size input. padding = ‘same’: Pad the input with 0’s around the border to make size of output and input the same. However, this will dilute the influence of pixels at the borders. from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\u0026#39;same\u0026#39;, activation=\u0026#39;relu\u0026#39;), layers.MaxPool2D(pool_size=2, strides=1, padding=\u0026#39;same\u0026#39;) # More layers follow ]) Example for visualization:\nshow_extraction( image, kernel, # Window parameters conv_stride=3, pool_size=2, pool_stride=2, subplot_shape=(1, 4), figsize=(14, 6), ) Since the circle is just 1 pixel wide, using stride = 3 is too coarse to produce a decent feature maps. We should then reduce the number of strides for a better feature map.\nThe more horizontal parts of the input end up with the greatest activation as the kernel is designed to detect horizontal lines.\n4. Custom Covnet Through feature extraction, we learned how to extract simple features from an image through filter, detect and pooling. By repeating the extraction process, we can extract more complex and refined features as the process travel deeper into the network.\nThis can be done through convolution blocks with stacks of Conv2D and MaxPool2D layers as below:\nHere’s how we can design a covnet that can extract complex features:\n# 1. define model from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ # First Convolutional Block layers.Conv2D(filters=32, kernel_size=5, activation=\u0026#34;relu\u0026#34;, padding=\u0026#39;same\u0026#39;, input_shape=[128, 128, 3]), layers.MaxPool2D(), # Second Convolutional Block layers.Conv2D(filters=64, kernel_size=3, activation=\u0026#34;relu\u0026#34;, padding=\u0026#39;same\u0026#39;), layers.MaxPool2D(), # Third Convolutional Block layers.Conv2D(filters=128, kernel_size=3, activation=\u0026#34;relu\u0026#34;, padding=\u0026#39;same\u0026#39;), layers.MaxPool2D(), # Classifier Head layers.Flatten(), layers.Dense(units=6, activation=\u0026#34;relu\u0026#34;), layers.Dense(units=1, activation=\u0026#34;sigmoid\u0026#34;), ]) model.summary() # 2. model training model.compile( optimizer=tf.keras.optimizers.Adam(epsilon=0.01), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;binary_accuracy\u0026#39;] ) history = model.fit( ds_train, validation_data=ds_valid, epochs=40, verbose=0, ) # 3. model loss evaluation import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_frame.loc[:, [\u0026#39;binary_accuracy\u0026#39;, \u0026#39;val_binary_accuracy\u0026#39;]].plot(); 5. Data Augmentation More data will generally help a model performs better — to better differentiate image. In this section, we will learn to augment our data by applying transformation to our datasets such as rotation, flipping, warping and changing of contrast and color tone. Here’s how to perform data augmentation in Python:\nData augmentation example\n# 1. define model - with augmentation from tensorflow import keras from tensorflow.keras import layers pretrained_base = tf.keras.models.load_model( \u0026#39;../input/cv-course-models/cv-course-models/vgg16-pretrained-base\u0026#39;, ) pretrained_base.trainable = False model = keras.Sequential([ # Preprocessing layers.RandomFlip(\u0026#39;horizontal\u0026#39;), # flip left-to-right layers.RandomContrast(0.5), # contrast change by up to 50% # Base pretrained_base, # Head layers.Flatten(), layers.Dense(6, activation=\u0026#39;relu\u0026#39;), layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;), ]) # 2. model training model.compile( optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;binary_accuracy\u0026#39;], ) history = model.fit( ds_train, validation_data=ds_valid, epochs=30, verbose=0, ) # 3. model loss evaluation import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_frame.loc[:, [\u0026#39;binary_accuracy\u0026#39;, \u0026#39;val_binary_accuracy\u0026#39;]].plot(); ","permalink":"https://keanteng.github.io/home/docs/2023-03-20-computer-vision/","summary":"Computer vision literally means computer able to see and recognize stuff. Applications such as Google Lens and Google Image Search are some good examples of where computer vision is being used in our daily life. In this course, we will explore some technique used to empower computer with the power of seeing:","title":"Computer Vision"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nFor examining and assessing huge datasets and databases, SQL or structured programming language skills plays a vital role to enable us to design and manage data.\nSome common keywords used in SQL as follows:\nSELECT, WHERE, FROM GROUP BY, HAVING, COUNT ORDER BY AS, WITH JOIN 1. Data Preparation Since we will be using Python to apply SQL, there are some steps required for data preparation. We will first start by establishing a connection to the BigQuery service to create a client object that hold projects. We will then select a project from the biquery-public-data which holds a collection of datasets for each project. The datasets will contain the tables that we want for our analysis. Here’s how we can do that in Python:\nfrom google.cloud import bigquery # 1. create a client object client = bigquery.Client() dataset_ref = client.dataset(\u0026#34;hacker-news\u0026#34;, project = \u0026#34;bigquery-public-data\u0026#34;) # 2. get the hacker news dataset from a project dataset = client.get_dataset(dataset_ref) # 3. list the tables in the datasets tables = list(client.list_tables(dataset)) # 4. print the all the table names for table in table: print(table.table_id) Now from the code we learn about all the table names in the datasets from the printed outputs. Here’s how we can fetch the table with code:\n# create a reference to the table name full table_ref = dataset_ref.table(\u0026#34;full\u0026#34;) # get the table table = client.get_table(table_ref) Oftentimes, when we want to preview at the table before our analysis, but it contains a huge number of rows and might consume resources to load, we can choose to display only a certain number of rows of the table.\n# Restrict rows and convert the result to dataframe client.list_rows(table, max_results = 5).to_dataframe() 1.1 Table Schema The structure of a table is known as table schema, and it will tell us information about each specific columns in our table. The information is:\nName of the column Datatype of the column Mode of the column (by default it will allow null values — nullable) Description of the data in the particular column # code command table.schema # Output: [SchemaField(\u0026#39;title\u0026#39;, \u0026#39;STRING\u0026#39;, \u0026#39;NULLABLE\u0026#39;, \u0026#39;Story title\u0026#39;, (), None)] 2. Queries Foundations We will start by three keywords which represents the foundational components of a SQL query, namely SELECT, FROM and WHERE.\nBy SELECT, it refers to the columns that we want to select, FROM refers to the table that we are after and finally WHERE refers to the return condition that we want to impose on the output. Below is how we can select the cat owner’s name from an imaginary dataset:\nAn imaginary dataset\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT Name FROM `bigquery-public-data.pet_records.pets` WHERE Animal = \u0026#39;Dog\u0026#39; \u0026#34;\u0026#34;\u0026#34; 2.1 Big Datasets Some BigQuery datasets can be huge and costly to compute. If you are running them on cloud services with limited computing hours, your limit might drain easily with huge datasets. We can avoid this issue by imposing a function to limit the amount of data that we are capable to scan to avoid running over our computing limit.\nHere’s how we can check how much data a query will scan, alternatively we can also impose a limit with a parameter on how much data that will be scanned:\n# query query = \u0026#34;\u0026#34;\u0026#34;\u0026#34; SELECT score, title FROM `bigquery-public-data.hacker_news.full` WHERE type = \u0026#34;job\u0026#34; \u0026#34;\u0026#34;\u0026#34; ## 1. Cost estimation # create object for cost estimation without running it dry_run_config = bigquery.QueryJobConfig(dry_run = True) # estimate cost dry_run_query_job = client.query(query, job_config = dry_run_config) print(\u0026#34;This query will process {} bytes.\u0026#34;.format(dry_run_query_job.total_bytes_processed)) ## 2. Setting parameter ONE_GB = 1000*1000*1000 # set up the query safe_config = bigquery.QueryJobConfig(maximum_bytes_billed = ONE_GB) # run the query safe_query_job = client.query(query, job_config = safe_config) job_posts_scores = safe_query_job.to_dataframe() job_posts_scores.mean() 3. More Queries 1 This section will add three new techniques: GROUP BY, HAVING and COUNT() to assist us in getting more interesting insights from our queries.\nGROUP BY means grouping together rows with the same value in that column as a single group, COUNT() will return a count of things such as the number of entries of a column while HAVING is used in combination with GROUP BY to ignore groups that do not meet a certain criterion.\nHere is how we can apply these techniques to find the most discussed new comments on the Hacker News project:\n# select comment with \u0026gt; 10 replies query_popular = \u0026#34;\u0026#34;\u0026#34; SELECT parent, COUNT(1) AS NumPosts FROM `bigquery-public-data.hacker_news.comments` GROUP BY parent HAVING COUNT(1) \u0026gt; 10 \u0026#34;\u0026#34;\u0026#34; # limit how much data will be scan safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10) query_job = client.query(query_popular, job_config=safe_config) # run query and convert to data frame popular_comments = query_job.to_dataframe() # print the first five rows popular_comments.head() 4. More Queries 2 In this section, we will learn about ORDER BY clause where it is used to sort the returned results by the rest of the query. When we use ORDER BY the returned results will be sorted in ascending order whether it is string or numeric. If we would like to have the order reverse, we can add in the DESC argument.\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT Name FROM `bigquery-public-data.pet_records.pets` WHERE Animal = \u0026#39;Dog\u0026#39; ORDER BY Name DESC \u0026#34;\u0026#34;\u0026#34; Now considering we have a column that tell us about the date that a pet owner getting a pet, we can use the EXTRACT clause to get the day, month, week and year information from the column.\n5. More Queries 3 In this section, we will work on two keywords that helps to organize our query for better readability — if we happen we have a complex query. Starting with AS, it is used to rename columns generated by our queries. WITH is used together with AS to create a temporary table so that we can write query against them. This helps us to split queries into readable chunks and makes the data cleaning work easier.\nConsider that we want to know the pet IDs that are owned for more than five years:\nquery = \u0026#34;\u0026#34;\u0026#34; WITH Seniors AS ( SELECT Name, ID FROM `bigquery-public-data.pet_records.pets` WHERE Years_old \u0026gt; 5 ) SELECT ID FROM Seniors \u0026#34;\u0026#34;\u0026#34; Another example is to find how many bitcoins transactions being made in a month:\nquery = \u0026#34;\u0026#34;\u0026#34;\u0026#34; WITH Time AS ( SELECT DATE(block_timestamp) AS trans_date FROM `bigquery-public-data.crypto_bitcoin.transactions` ) SELECT COUNT(1) AS transactions, trans_date FROM Time GROUP BY trans_date ORDER BY trans_date \u0026#34;\u0026#34;\u0026#34; # limit how much data will be scanned safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10) query_job = client.query(query, job_config=safe_config) # run query and convert to data frame transactions_by_date = query_job.to_dataframe() # Print the first five rows transactions_by_date.head() # Extra: visualize the result transactions_by_date.set_index(\u0026#39;trans_date\u0026#39;).plot() 6. Combining Data Sources For the last clause in this course, we will learn about JOIN — INNER JOIN in this section. For INNERJOIN, a row will only be selected if the column that we used to combine in one table also matches with the table that we used for joining. JOIN clause allows us to query and combine the information from different tables. For example, we have two tables; one contains the pet name and the other one contains the pet’s owner name. Here how we can associate the pet with its owner:\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT p.Name AS Pet_Name, 0.Name AS Owner_Name FROM `bigquery-public-data.pet_records.pets` AS P INNER JOIN bigquery-public-data.pet_records.owners` AS O ON p.ID = o.Pet_ID \u0026#34;\u0026#34;\u0026#34; Here’s another example where we want to find how many files are covered by each type of software license on GitHub.\nquery = \u0026#34;\u0026#34;\u0026#34; SELECT L.license, COUNT(1) AS number_of_files FROM `bigquery-public-data.github_repos.sample_files` AS sf INNER JOIN `bigquery-public-data.github_repos.licenses` AS L ON sf.repo_name = L.repo_name GROUP BY L.license ORDER BY number_of_files DESC \u0026#34;\u0026#34;\u0026#34; # limit how much data will be scanned safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10) query_job = client.query(query, job_config=safe_config) # run query and convert to data frame file_count_by_license = query_job.to_dataframe() file_count_by_license ","permalink":"https://keanteng.github.io/home/docs/2023-03-15-intro-to-sql/","summary":"For examining and assessing huge datasets and databases, SQL or structured programming language skills plays a vital role to enable us to design and manage data. Some common keywords used in SQL as follows: SELECT, WHERE FROM GROUP BY, HAVING, COUNT ORDER BY AS, WITH JOIN. Date Preparation Since we will be","title":"Intro to SQL"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nUsing machine learning in data analysis is a rather procedural approach. As we can notice from the approach, we will start by Preparing data \u0026raquo; Defining a model \u0026raquo; Model diagnostic checking \u0026raquo; Model prediction to complete our workflow. This workflow is often and commonly practiced when we are doing data analysis work. In this course, we will learn some important and useful technique that can be used in our work to achieve a better model.\nMain learnings:\nApproaches towards missing data values and categorical variables (non-numeric) Construct pipeline to improve our workflow and code flow. Cross-validation technique Build state-of-the-art model such as XGBoost Approaches to avoid data leakage. 1. Missing Values \u0026amp; Categorial Variables 1.1 Missing values We can deal with missing values with the following three ways:\nDrop the columns containing missing values (not recommended, might loss access to important information) Imputation to fill the empty cells with some number. Imputation, then add a new column that shows the missing entries. Visualize the methods\nImputation will perform better than dropping the entire columns. This is how we can do that in code:\n## 1. drop columns with missing values # get the columns name cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()] # perform drop reduced_X_train = X_train.drop(cols_with_missing, axis = 1) ## 2. imputation from sklearn.impute import SimpleImputer my_imputer = SimpleImputer() imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train)) # imputation removed column names, put back imputed_X_train.columns = X_train.columns ## 3. extended imputation X_train_plus = X_train.copy() for col in cols_with_missing: X_train_plus[col + \u0026#39;_was_missing\u0026#39;] = X_train_plus[col].isnull() my_imputer = SimpleImputer() imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus)) imputed_X_train_plus.columns = X_train_plus.columns 1.2 Categorical variables There are three ways to deal with categorical variables (non-numeric data):\nDropping the categorical variables (if the columns does not provide any useful information) Ordinal encoding — assign a unique value in the dataset to a different integer. One-hot encoding — create new columns to indicate the presence of each possible value in the original data One hot encoding example\nFor One-hot encoding, it means that if a column with 100 rows contains 100 unique values, it will create an extra (100 rows *100 unique values –100 original rows) new entries.\nWe can apply the 3 approaches with the following code:\n## 1. droppping categorical variables dorp_X_train = X_train.select_dtypes(exclude= [\u0026#39;object\u0026#39;]) ## 2. ordinal encoding from sklearn.preprocessing import OrdinalEncoder label_X_train = X_train.copy() ordinal_encoder = OrdinalEncoder() label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols]) ## 3. one hot encoding from sklearn.preprocessing import OneHotEncoder # one hot encode categorical data OH_encoder = OneHotEncoder(handle_unknown = \u0026#39;ignore\u0026#39;, sparse = False) OH_cols_train = pd.DataFrame(OH_encoder.fit_transform[object_cols]) # add back removed index OH_cols_train.index = X_train.index # remove categorical column and add back the one hot encoded columns num_X_train = X_train.drop(object_cols, axis = 1) OH_X_train = pd.concat([num_X_train, OH_cols_train], axis = 1) 2. Pipelines Pipeline is a way to bundle our preprocessing and modelling steps to keep our code organized. The benefits of using pipeline are it gives a cleaner code, reduces bugs and make our model easier to be implemented.\nHere’s how we can apply pipeline to impute missing numerical entries and one-hot encode missing categorical entries:\nfrom sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder # preprocess numerical data numerical_transformer = SimpleImputer(strategy = \u0026#39;constant\u0026#39;) # preprocess categorical data categorical_transformer = Pipeline(steps = [ (\u0026#39;imputer\u0026#39;, SimpleImputer(strategy = \u0026#39;most frequent\u0026#39;)), (\u0026#39;onehot\u0026#39;, OneHotEncoder(handle_unknown = \u0026#39;ignore\u0026#39;)) ]) # bundle the two preprocesses preprocessor = ColumnTransformer( transformers = [ (\u0026#39;num\u0026#39;, numerical_transformer, numerical_cols), (\u0026#39;cat\u0026#39;, categorical_transformer, categorical_cols) ]) # bundle preprocessing and modelling my_pipeline = Pipeline(steps = [ (\u0026#39;preprocessor\u0026#39;, preprocessor), (\u0026#39;model\u0026#39;, model) ]) # model evaluation my_pipeline.fit(X)train, y_train) preds = my_pipeline.predict(X_valid) mean_absolute_error(y_valid, preds) 3. Cross-validation Cross-validation means we run our modelling process on different subsets of the data to get several measures of our model quality. Although this technique gives a more accurate measure of model quality, it can take some time to run as it need to estimate multiple models as we can see from the below images.\nIt is recommended to run cross-validation for smaller datasets while for larger datasets, a single validation if often suffice.\nCross-Validation\nHere’s how we can apply cross-validation in Python together with pipeline which we learned earlier on:\nfrom sklearn.model_selection import cross_val_score # split the data to 5 sets for validation scores = -1 * cross_val_score(my_pipeline, X, y, cv = 5, scoring = \u0026#39;neg_mean_absolute_error\u0026#39;) print(scores.mean()) It is surprising to see that negative mean absolute error is used in the code. This is because “sklearn” has a convention where all metrics are defined so a high number is better. Thus, the use of negatives allows convention consistency.\n4. XGBoost — Gradient Boosting In the random forest method, we improve a model prediction by averaging the prediction of many decision trees. Random forest method is one of an ensemble method where we combine the prediction of several models. A state-of-the-art method would be to apply gradient boosting where we perform iterative cycles to add models into an ensemble to result in better prediction.\nConcepts:\nUse the current ensemble to generate predictions for each observation in the dataset. Use the prediction to calculate a loss function. Use the loss function to fit a new model that will be added to the ensemble which will reduce the loss. Add this new model to the ensemble. Repeat Here’s how to do it in code:\nfrom xgboost import XGBRegressor from sklearn.metrics import mean_absolute_error my_model = XGBRegressor() my_model.fit(X_train, y_train) predictions = my_model.predict(X_valid) mean_absolute_error(predictions, y_valid) There are a few parameters in the xgregressor function that might affect the accuracy of our result:\nn_estimators which means how many times to go through the modelling cycle (concepts above), value too high or too low might result in overfitting or underfitting respectively early_stopping_rounds which means stopping the model when the validation score stops improving with imposed criteria learning_rate which means multiply the predictions from each model by a small number before adding up the prediction from each component model n_jobs which aims to improve model’s runtime. We can set the number equal to the cores of our machine model = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, n_jobs = 4) model.fit(X_train,y_train, early_stopping_rounds = 5, eval_set = [(X_valid, y_valid)], verbose = False) 5. Data Leakage Data leakage happens when training data contains information about the target, but similar data will not be available when we used the model to perform prediction. The two main types of data leakage are target leakage and train-test contamination.\n5.1 Target leakage Target leakage occurs when predictors include data that will not be available at the time when predictions is made. A way to overcome this issue is to think about timing or chronological order that data becomes available rather than whether a feature will help to make good predictions.\n5.2 Train-test contamination Train-test contamination happens when validation data affects the preprocessing behavior as the validation process is corrupted.\nFor example, imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions. In the courses, there are several interesting case studies on data leakage that worth looking to improve our acumen when interpreting results from work.\nCase 1\nGuide:\nThis is tricky, and it depends on details of how data is collected (which is common when thinking about leakage). Would you at the beginning of the month decide how much leather will be used that month? If so, this is ok. But if that is determined during the month, you would not have access to it when you make the prediction. If you have a guess at the beginning of the month, and it is subsequently changed during the month, the actual amount used during the month cannot be used as a feature (because it causes leakage).\n","permalink":"https://keanteng.github.io/home/docs/2023-03-02-intermediate-machine-learning/","summary":"Using machine learning in data analysis is a rather procedural approach. As we can notice from the approach, we will start by \u003cem\u003ePreparing data \u0026raquo; Defining a model \u0026raquo; Model diagnostic checking \u0026raquo; Model prediction\u003c/em\u003e to complete our workflow. This workflow is often and commonly practiced when we are doing data analysis work.","title":"Intermediate Machine Learning"},{"content":" Images from Unsplash\nDisclaimer: This article is for educational purpose only and the author does not suggest any illegal usage of this technology (generative AI). Please be responsible for your generations and creations. Do not use them for any malicious intent or harm in any form and be respectful.\nThere are two ways to use Stable Diffusion — either locally on your own PC or through a cloud computing services such as Google Collab and Kaggle.\nIt has been a frustration for many without a decent GPU and sufficient VRAM. This has caused long computation time and a lot of friction for you to tune your model parameter/ prompt, and worse, you are not even able to load the Stable Diffusion model.\nSources: Stable Diffusion WebUI https://cdn.changelog.com/uploads/news_items/RdAG/large.png?v=63829970528\nI will break this article in 3 parts according to the resources I gathered, and you are free to discover the one that you see fit.\n1. Google Collab — Realistic \u0026amp; Animated Image Generation Terms: google collab, civitai, account\nGoogle Collab offers a decent and fast GPU and long-running hours for any Google users with a Google account. You can check out the YouTube video here by Nolan Aatama where you can learn about some quick installation steps to run Stable Diffusion in your own account. The account provides installation guide on various models such as Dreamshaper and more.\nYou can check out the models offered on Civitai to look for prompt examples and user review of the model as well as other models offered on the platform.\n# prompts example positive: space, rocket, stallite, earth, milky ways, space dust, high res, 8k negative: lowres, bad hands, bad fingers, sketeches, paintings, If you want to check the prompts for any of the generated images you like can use this Stable Diffusion Decomposer to get the details of each generated images.\nStable Diffusion Decomposer\nYou need to create a Civitai account to post your review and submit comment.\nDo note that it takes around 10 minutes to run the code. It took around 20 seconds to generate a 512x1024 pixels image which is decent. For Google Collab, I think there is no computing quota being set and you can run the program for a few hours. But do avoid leaving the tabs idle as Google will reconnect your program which caused you to re-run all the code again.\n2. Kaggle — Animated Image Generation Terms: kaggle, webui, gpu, telegram\nI only manage to find scripts that provide animated image generation on Kaggle, you can look at it here. You just need to copy and run all the codes in the notebook to access the WebUI. Do note that on Kaggle, you only have 30 hours of GPU resources being allocated, so use it wisely and shut down the connection when you are not running the program to conserve the quota allocated to you.\nTurn off GPU if it is not in use\nIf you like animated image and you don’t want to always re-run the scripts to access WebUI, you can also check out this site, a site that host the WebUI for animated image generation. You can get around 100 quotas by signing in daily (which allows you to generate about 90+ images). You also need to acquire the sign-in token with a Telegram account.\nSome of the model offered on the site\n3. Your Own PC Terms: local, google translation, extension, apply and restart\nIf you want to run Stable Diffusion locally, you can have a read on this article on the installation guide. You can use Google Translate for the article as it is in Chinese.\nBasically, this is what you need to do:\nInstallation of Stable Diffusion WebUI Load models, put in models/Stable-diffusion/ files after the installation step above (can get from Civitai website) After opening the local URL, go to the Extensions tab and install https://github.com/civitai/sd_civitai_extension. Go to Installed tab and click Apply and Restarts UI Done! ","permalink":"https://keanteng.github.io/home/docs/2023-02-25-stable-diffusion-webui-with-civitai-loras/","summary":"There are two ways to use Stable Diffusion — either locally on your own PC or through a cloud computing services such as Google Collab and Kaggle. It has been a frustration for many without a decent GPU and sufficient VRAM. This","title":"Stable Diffusion WebUI with Civitai LORAs"},{"content":" Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\n0. Concepts Creation and evaluation of a deep learning model is a procedural work with steps. Data preparation \u0026raquo; Model optimization\u0026raquo; Model fitting \u0026raquo; Model evaluation \u0026raquo; Model prediction\nNote:\nModel optimization (adam optimizer) Model fitting (batch, epoch) Model evaluation(dropout, batch normalization) 1. Modelling \u0026amp; Neural Network Terms: layer, activation function, neural network, dense, rectified linear unit\nSimple Neural Network (linear)\nThe above neural network can be generated with code using Python. The network above organized neurons into layers. Collecting linear units with common set of inputs result in a dense layer.\nfrom tensorflow import keras from tensorflow.keras import layers # 1 linear unit network model = keras.Sequential([ layers.Dense(units = 1, input_shape = [1]) ]) model = keras.Sequential([ layers.Dense(units = 1, input_shape = [3]) ]) If we want to fit a curve (non-linear), a feature called activation function is needed, otherwise the neural network can only learn linear relationships. A common example is the rectifier function max(0,x), where we get a rectified linear unit when we attach the function to a linear unit.\nWe can also stack layers to achieve a complex data transformation.\nVisualizing the Network\nUsing code to define the network:\nfrom tensorflow import keras from tensorflow.keras import keras # [layer1, layer2, layer3,...] model = keras.Sequential([ layers.Dense(units = 4, activation = \u0026#39;relu\u0026#39;, input_shape = [2], layers.Dense(units = 3, activation = \u0026#39;relu\u0026#39;), layers.Dense(units = 1) ]) 2. Model Optimization \u0026amp; Fitting Terms: loss function, stochastic gradient descent, batch, epoch, error/loss\nSimilar to regression, we need to know how well our model fits the data. We use loss function to measures the difference between observed and predicted values. The common measure would be the mean absolute error where it computed the average length between the fitted curve and data points. Other errors examples are mean-squared error or Huber loss.\nHow can we minimize the error? We can use optimization algorithms — the stochastic gradient descent. The concepts as follows:\nGet random sample (batch) from original dataset and run through the network for prediction. Measure the error (loss) Adjust the weight in a direction that makes the loss smaller (an epoch for each complete round a training) We can use a built-in “adam” optimizer to optimize our model. This allows self-tuning to minimize loss. Below is code to fit 256 rows of training data for 10 times:\n# get training data dimension (row, column) print(X_train.shape) # define model from tensorflow import keras from tensorflow.keras import layers mode = keras.Sequential([ layers.Dense(512, activation = \u0026#39;relu\u0026#39;, input_shape = [11]), layers.Dense(512, activation=\u0026#39;relu\u0026#39;), layers.Dense(512, activation=\u0026#39;relu\u0026#39;), layers.Dense(1), ]) # add in optimizer and loss function model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;mae\u0026#39;, ) # model fitting history = model.fit( X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 256, epochs = 10, ) # visualize outcome import pandas as pd # training history to data frame history_df = pd.DataFrame(history.history) # plot the loss history_df[\u0026#39;loss\u0026#39;].plot(); 3. Model Evaluation 1 Terms: underfitting, overfitting, capacity, training, callback, stopping criteria\nTraining data normally contains signal and noise where signal helps our model make prediction from new data and noise represent the random fluctuation coming from data in the real world. To see if our model fits the data well, we will compare learning curve between training set and validation set.\nUnderfitting the training set is when the loss is not as low as it could be because the model hasn’t learned enough signal. Overfitting the training set is when the loss is not as low as it could be because the model learned too much noise.\nA model’s capacity is the size and complexity of the patterns it is able to learn. We can adjust a model’s capacity if we detect overfitting or underfitting scenarios.\n# sample model model = keras.Sequential([ layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.Dense(1), ]) # wider model = keras.Sequential([ layers.Dense(32, activation = \u0026#39;relu\u0026#39;), layers.Dense(1), ]) # deeper model = keras.Sequential([ layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.Dense(1), ]) Sometimes the validation increase during training after a certain point although it kept decreasing early on. This can due to a model is learning noise from the datasets. We can overcome this issue by imposing an early stopping criterion.\nA callback function is used where we detect when the validation loss starts to rise again, and we reset the weights back the where the minimum occurred. An example “if there is not at least 0.001 improvement in the validation loss over 20 epochs, then stop training and keep the best model you found”.\nfrom tensorflow.keras.callbacks import EarlyStopping early_stopping = EarlyStopping( min_delta = 0. 001, # min change to count as improvement patience = 20, # how many epochs to wait before stopping restore_best_weights = True, ) 4. Model Evaluation 2 Terms: dropout, batch normalization\nTo correct overfitting in our model, we can implement the idea of dropout where we randomly drop out some fraction of a layer’s input units every step of training. This avoids the network to learn spurious patterns in the training data which leads to overfitting.\nmodel = keras.Sequential([ ... layers.Dropout(rate = 0.3) # 30% dropout to the next layer layers.Dense(16), ... ]) To correct slow or unstable training, we can apply batch normalization to put all data on a common scale. SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.\nmodel = keras.Sequential([ layers.Dense(16, activation = \u0026#39;relu\u0026#39;), layers.BatchNormalization(), ]) # or model = keras.Sequential([ layers.Dense(16), layers.BatchNormalization(), layers.Activation(\u0026#39;relu\u0026#39;), ]) Example for a full model fitting process:\nfrom tensorflow import keras from tensorflow.keras import layers # define network model = keras.Sequential([ layers.Dense(1024, activation=\u0026#39;relu\u0026#39;, input_shape=[11]), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1024, activation=\u0026#39;relu\u0026#39;), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1024, activation=\u0026#39;relu\u0026#39;), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1), ]) # add optimier model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;mae\u0026#39;, ) # model fitting history = model.fit( X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 256, epochs = 100, verbose = 0, ) # visualize history_df = pd.DataFrame(history.history) history_df.loc[:,[\u0026#39;loss\u0026#39;,\u0026#39;val_loss\u0026#39;]].plot(); 5. Binary Classification Terms: sigmoid, binary, accuracy\nWe cannot use accuracy to measure model performance as it does not change smoothly — it changes in jumps as it represents ratio counts. Thus, a replacement would be the cross-entropy function where it measures the distance between probabilities. To convert outputs from dense layer into probabilities, we will use the sigmoid activation function.\nCode example:\nfrom tensorflow import keras from tensorflow.keras import layers # define network model = keras.Sequential([ layers.Dense(4, activation=\u0026#39;relu\u0026#39;, input_shape=[33]), layers.Dense(4, activation=\u0026#39;relu\u0026#39;), # final layer need sigmoid function to produce class probabilities layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;), ]) # add optimizer model.compile( optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;binary_crossentropy\u0026#39;, metrics = [\u0026#39;binary_accuracy\u0026#39;] ) # add stopping criteria early_stopping = keras.callbacks.EarlyStopping( patience = 10, min_delta = 0.001, restore_best_weights = True, ) # model fitting history = model.fit( X_train, y_train, validation_data=(X_valid, y_valid), batch_size=512, epochs=1000, callbacks = [early_stopping], verbose = 0 # hide output since too many to display ) # model levaluation history_df = pd.DataFrame(history.history) # starts at epoch 5 history_df.loc[5:, [\u0026#39;loss\u0026#39;, \u0026#39;val_loss\u0026#39;]].plot() history_df.loc[5:, [\u0026#39;binary_accuracy\u0026#39;, \u0026#39;val_binary_accuracy\u0026#39;]].plot() print((\u0026#34;Best Validation Loss: {:0.4f}\u0026#34; +\\ \u0026#34;\\nBest Validation Accuracy: {:0.4f}\u0026#34;)\\ .format(history_df[\u0026#39;val_loss\u0026#39;].min(), history_df[\u0026#39;val_binary_accuracy\u0026#39;].max())) 6. Reference Learn Intro to Deep Learning Tutorials - Kaggle ","permalink":"https://keanteng.github.io/home/docs/2023-02-24-intro-to-deep-learning/","summary":"Creation and evaluation of a deep learning model is a procedural work with steps. *Data preparation \u0026raquo; Model optimization\u0026raquo; Model fitting \u0026raquo; Model evaluation \u0026raquo; Model prediction Note Model optimization (adam optimizer) Model fitting (batch, epoch)","title":"Intro to Deep Learning"},{"content":" Images from Unsplash\nIn this article, we will learn how to image scraping on this National Geographic article: Asteroids vs. comets: How do they differ, and do they pose a threat to Earth? We will learn how to download in bulk the high-quality images in the webpage with Python code.\nThe process:\nFind the webpage data structure for the URL specified. Search for the images link and save in a list. Download all the images. Find the webpage data structure for the URL specified We will be using Visual Studio Code with Python and Jupyter notebook configured. However, a Python code file will also do the job. Here are the three packages to be installed using Window PowerShell:\n# Lauch Power Shell by pressing Start and then type Power Shell in the search bar # installation procedure py -m pip install requests py -m pip install bs4 py -m pip install urllib First, we start by importing the modules needed. Then, we need to make HTTP request to view the webpage data structure we want. This can be done as follows:\nimport requests from bs4 import BeautifulSoup import urllib.request def getdata(url): r = requests.get(url) return r.text htmldata = getdata(\u0026#34;https://www.nationalgeographic.co.uk/space/2023/01/asteroids-vs-comets-how-do-they-differ-and-do-they-pose-a-threat-to-earth\u0026#34;) soup = BeautifulSoup(htmldata, \u0026#39;html.parser\u0026#39;) You can check the output by typing the variable name:\nOutput representing webpage structure.\nSearch for the images link and save in a list Now, we want to search for all the images links that embedded images in the webpage. We will use a test list to store each of the link as list of elements for download later. The print function is used to print out all the links that are being searched.\ntest = list() for item in soup.find_all(\u0026#39;img\u0026#39;): print(item[\u0026#39;src\u0026#39;]) test.append(item.get(\u0026#39;src\u0026#39;)) All the searched links in list\nDownload all the images Now simply using a for loop, we can save all the images inside our current folder. The number of iterations, n will be the number of images in the list. In our code, we will also rename the images with number 0,1,2,…,n to avoid duplicate file name.\nfor i in range(len(test)): urllib.request.urlretrieve(test[i], str(i)+\u0026#34;.jpg\u0026#34;) All the images being downloaded\nExtra: Some website will have several images urls mixed up including .svg, .png., .gif and .jpg. We can use if statement to counter this issue:\ntemp = list() for i in range(len(test)): if test[i].__contains__(\u0026#39;.png\u0026#39; or \u0026#39;.jpg\u0026#39; or \u0026#39;.jpeg\u0026#39;): temp.append(test[i]) for i in range(len(temp)): urllib.request.urlretrieve(temp[i], str(i)+\u0026#34;.jpg\u0026#34;) That said, job well done. Do note that this approach has restriction on the type of web page for scraping. It works best for articles pages with images embedded. If you encounter connection time out error with your request, do have a check on the site accessibility (Internet Service Provider). Otherwise, it should work perfectly!\nReferences: Beautiful Soup Documentation — Beautiful Soup 4.4.0 documentation (beautiful-soup-4.readthedocs.io) urllib.request — Extensible library for opening URLs — Python 3.11.2 documentation ","permalink":"https://keanteng.github.io/home/docs/2023-02-19-images-scraping-from-web-pages/","summary":"In this article, we will learn how to image scraping on this National Geographic article: Asteroids vs. comets: How do they differ, and do they pose a threat to Earth? We will learn how to download in bulk the high-quality images in the webpage with Python code.","title":"Images Scraping from Web Pages"},{"content":"In this article, I will be showing the process of scraping some listed companies market capitalization in Malaysia data using Google Sheet. We will perform web scraping on the i3 Investor site.\nImages from Unsplash\nGenerate Webpage for Scraping First of all, open a new google sheet and create a table like this:\nCreate a table like this\nInside the table, we have a few companies name and their listed code. Notice that in cell C3, we put a link — this link will serve as a “prefix”. If the stock code is put at the back of the link, it will direct to the webpage of the particular page.\nprefix link: https://klse.i3investor.com/web/stock/overview/ link to webpage: https://klse.i3investor.com/web/stock/overview/1023 Now, we use the concatenate function to append the code to the prefix links for all the companies in the table:\nUse CONCAT() function to create the links\nWeb Scraping in Action Before we start web scraping, we need to learn about this function — IMPORTXML().\n=importxml(\u0026#34;url\u0026#34;, \u0026#34;query\u0026#34;) Notice that we already have all the URLs needed as we have created the links on the table. Now, the missing piece is called “query” — simply means what do we want to know and where can it be found?\nThe query is called the XPath where it is used in web browser, now let’s hope into Maybank stock page and get the XPath query.\nHighlight the market capitalization amount\nAfter clicking inspect, a window will pop out highlighting a code segment. Here, we need to right click and select Copy full XPath.\nClick copy full XPath\nLet’s hope back to Google Sheet, you will be pasting the query as follow:\n=importxml(\u0026#34;url\u0026#34;,\u0026#34;/html/body/div[3]/div/div[2]/div[8]/div[1]/div[2]/div[1]/div[2]/p/strong\u0026#34;) Note: The URL will be the URLs in the Reference column After applying the formula to the Market Cap column, you will manage to scrape all the Market Capitalization data on the sheet — the data will be updated live.\nData will be loaded once your apply the formula\n","permalink":"https://keanteng.github.io/home/docs/2022-11-05-google-sheet-simple-web-scraping/","summary":"In this article, I will be showing the process of scraping some listed companies market capitalization in Malaysia data using Google Sheet. We will perform web scraping on the i3 Investor site. First of all, open a new google sheet and","title":"Google Sheet Simple Web Scraping"},{"content":" Where can I check on the raw code that this site used to set up? You can find it at my GitHub repository How to post question to you or is there any interactive features such as add to reading list? If you have any question, you can perhaps use the message feature on Medium. Since the majority of the site traffic comes from there. You can find my articles at my Medium blogs. Do you consider adding comment feature to the site? I will consider adding the feature if requested. ","permalink":"https://keanteng.github.io/home/faqs/","summary":"Where can I check on the raw code that this site used to set up? You can find it at my GitHub repository How to post question to you or is there any interactive features such as add to reading list? If you have any question, you can perhaps use the message feature on Medium. Since the majority of the site traffic comes from there. You can find my articles at my Medium blogs.","title":"FAQs"}]