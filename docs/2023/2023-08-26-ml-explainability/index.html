<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Machine Learning Explainability | Kean Teng Blog</title>
<meta name="keywords" content="Python, “Machine Learning&#34;, SHAP">
<meta name="description" content="My Kaggle Learning Note ">
<meta name="author" content="Kean Teng Blog">
<link rel="canonical" href="https://keanteng.github.io/home/docs/2023/2023-08-26-ml-explainability/">
<link crossorigin="anonymous" href="/home/assets/css/stylesheet.42b796e06d1330447cad2b7cf31d0edbb28b5959a021373fc0b6b1750e6c2824.css" integrity="sha256-QreW4G0TMER8rSt88x0O27KLWVmgITc/wLaxdQ5sKCQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://i.postimg.cc/0Qq5g2fX/favicon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://keanteng.github.io/home/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://keanteng.github.io/home/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://keanteng.github.io/home/apple-touch-icon.png">
<link rel="mask-icon" href="https://keanteng.github.io/home/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://keanteng.github.io/home/docs/2023/2023-08-26-ml-explainability/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-PPTWMGE94F"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-PPTWMGE94F');
        }
      </script><meta property="og:url" content="https://keanteng.github.io/home/docs/2023/2023-08-26-ml-explainability/">
  <meta property="og:site_name" content="Kean Teng Blog">
  <meta property="og:title" content="Machine Learning Explainability">
  <meta property="og:description" content="My Kaggle Learning Note ">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
    <meta property="article:published_time" content="2023-08-26T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-08-26T00:00:00+00:00">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="“Machine Learning&#34;">
    <meta property="article:tag" content="SHAP">
      <meta property="og:image" content="https://keanteng.github.io/home/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://keanteng.github.io/home/papermod-cover.png">
<meta name="twitter:title" content="Machine Learning Explainability">
<meta name="twitter:description" content="My Kaggle Learning Note ">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Docs",
      "item": "https://keanteng.github.io/home/docs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning Explainability",
      "item": "https://keanteng.github.io/home/docs/2023/2023-08-26-ml-explainability/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Machine Learning Explainability",
  "name": "Machine Learning Explainability",
  "description": "My Kaggle Learning Note ",
  "keywords": [
    "Python", "“Machine Learning\"", "SHAP"
  ],
  "articleBody": " Images from Unsplash\nDisclaimer: This article is my learning note from the courses I took from Kaggle.\nMany people tend to say that machine learning models are “black boxes” because they can make good predictions but cannot understand the logic behind those predictions.\nIn this course, we will learn on methods to extract insights from model:\nWhat features in the data the model think as the most important? How each feature affect the prediction? So, why do we need model insights? Model insights can be useful in a couple of ways:\nDebugging\nGiven the frequency and potentially disastrous consequences of bugs, debugging is one of the most valuable skills in data science. Understanding the patterns a model is finding will help you identify when those are at odds with your knowledge of the real world, and this is typically the first step in tracking down bugs.\nInforming Feature Engineering\nFeature engineering is usually the most effective way to improve model accuracy. Feature engineering usually involves repeatedly creating new features using transformations of your raw data or features you have previously created.\nSometimes you can go through this process using nothing but intuition about the underlying topic. But you’ll need more direction when you have 100s of raw features or when you lack background knowledge about the topic you are working on.\nDirecting Future Data Collection\nYou have no control over datasets you download online. But many businesses and organizations using data science have opportunities to expand what types of data they collect. Collecting new types of data can be expensive or inconvenient, so they only want to do this if they know it will be worthwhile. Model-based insights give you a good understanding of the value of features you currently have, which will help you reason about what new values may be most helpful.\nInforming Human Decision-Making\nSome decisions are made automatically by models. Amazon doesn’t have humans (or elves) scurry to decide what to show you whenever you go to their website. But many important decisions are made by humans. For these decisions, insights can be more valuable than predictions.\nBuilding Trust\nMany people won’t assume they can trust your model for important decisions without verifying some basic facts. This is a smart precaution given the frequency of data errors. In practice, showing insights that fit their general understanding of the problem will help build trust, even among people with little deep knowledge of data science.\n1. Permutation Importance Permutation importance is calculated after a model has been fitted. Imagine that now, we want to predict a person’s height when they become 20 years old using only data that is available at age 10.\nNow, if we randomly shuffle a single column of the validation data but all the other columns remain in place, how would the accuracy of the prediction be affected?\nOf course, such an approach would reduce the model accuracy, since the data no longer corresponds to what we can observe in the real world. Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions. In this case, shuffling height at age 10 would cause terrible predictions. If we shuffled socks owned instead, the resulting predictions wouldn’t suffer nearly as much.\nSo, here is what we can do:\nGet a trained model Shuffle values in a single column and make prediction from it. Calculate the difference of prediction and target value with a loss function. The performance deterioration is the importance of the variable that we shuffled Return to step 2 until all the importance for each column is calculated 1.1 Example Here is how to calculate the importance with eli5 library:\nimport eli5 from eli5.sklearn import PermutationImportance perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y) eli5.show_weights(perm, feature_names = val_X.columns.tolist()) From the above figure, the values towards the top is the most important features. The first number in each row shows how much the model performance decreased with a random shuffling. There is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next.\nYou’ll occasionally see negative values for permutation importance. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn’t matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance.\n2. Partial Plots Similar to permutation importance, partial dependence plots are calculated after a model has been fit. To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.\nWe will use the fitted model to predict our outcome (probability their player won “man of the match”). But we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).\n2.1 Example Let’s get a decision tree from the model:\nfrom sklearn import tree import graphviz tree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names) graphviz.Source(tree_graph) Produce a partial dependence plot:\nfrom matplotlib import pyplot as plt from sklearn.inspection import PartialDependenceDisplay # Create and plot the data disp1 = PartialDependenceDisplay.from_estimator(tree_model, val_X, ['Goal Scored']) plt.show() The y-axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning “Man of The Match.” But extra goals beyond that appear to have little impact on predictions.\nfeature_to_plot = 'Distance Covered (Kms)' disp2 = PartialDependenceDisplay.from_estimator(tree_model, val_X, [feature_to_plot]) plt.show() This graph seems too simple to represent reality. But that’s because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model’s structure.\n# Build Random Forest model rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) disp3 = PartialDependenceDisplay.from_estimator(rf_model, val_X, [feature_to_plot]) plt.show() This model thinks you are more likely to win Man of the Match if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model. Though this dataset is small enough that we would be careful in how we interpret any model.\n2.2 2D Partial Dependence Plots We will use the same datasets as of above:\nfig, ax = plt.subplots(figsize=(8, 6)) f_names = [('Goal Scored', 'Distance Covered (Kms)')] # Similar to previous PDP plot except we use tuple of features instead of single feature disp4 = PartialDependenceDisplay.from_estimator(tree_model, val_X, f_names, ax=ax) plt.show() From the plot above, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn’t matter. Can you see this by tracing through the decision tree with 0 goals?\nBut distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot.\n3. SHAP Value SHAP or SHapley Additive exPlanations is used to break down a prediction t0 show the impact of each feature. It interprets the impact of having a certain value for a given feature in comparison to the prediction we would make if that feature took some baseline value.\nFor example, consider the Man of the Match award example for previous section, we could ask questions like how much prediction driven by the fact that the team scored 3 goals?\nBut for each team, they are many features, so if we answer for the number of goals, we could repeat the process for other features too. SHAP values of all features sum up to explain why my prediction was different from the baseline.\nsum(SHAP values for all features) = pred_for_team - pred_for_baseline_values To interpret the graph:\nWe predicted 0.7, whereas the base_value is 0.4979. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature’s effect. Feature values decreasing the prediction are in blue. The biggest impact comes from Goal Scored being 2. Though the ball possession value has a meaningful effect decreasing the prediction.\nIf you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.\nHow to Do That in Code Let’s get the model ready:\nimport numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv') y = (data['Man of the Match'] == \"Yes\") # Convert from string \"Yes\"/\"No\" to binary feature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]] X = data[feature_names] train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) We will look for SHAP value for a single row of the dataset. Let’s check on the raw prediction first:\nrow_to_show = 5 data_for_prediction = val_X.iloc[row_to_show] # use 1 row of data here. Could use multiple rows if desired data_for_prediction_array = data_for_prediction.values.reshape(1, -1) my_model.predict_proba(data_for_prediction_array) The output is array([[0.29, 0.71]]), the team is 70% likely to have a player win the award\nimport shap # package used to calculate Shap values # Create object that can calculate shap values explainer = shap.TreeExplainer(my_model) # Calculate Shap values shap_values = explainer.shap_values(data_for_prediction) shap.initjs() shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction) The shap_values object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don’t win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we’ll pull out SHAP values for positive outcomes (pulling out shap_values[1]).\nOf course, SHAP package also has explainers for every type of model:\nshap.DeepExplainer works with Deep Learning models. shap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values. 4. Advanced Uses of SHAP Value Shap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature).\nConsider the equation:\ny = 4 * x1 + 2 * x2 If x1 takes the value 2, instead of a baseline value of 0, then our SHAP value for x1 would be 8 (from 4 times 2).\nThese are harder to calculate with the sophisticated models we use in practice. But through some algorithmic cleverness, Shap values allow us to decompose any prediction into the sum of effects of each feature value, yielding a graph like this:\nIn addition to this nice breakdown for each prediction, the Shap library offers great visualizations of groups of Shap values. We will focus on two of these visualizations. These visualizations have conceptual similarities to permutation importance and partial dependence plots\nSHAP summary plots give us a birds-eye view of feature importance and what is driving it. We’ll walk through an example plot for the soccer data:\nThis plot is made of many dots. Each dot has three characteristics:\nVertical location shows what feature it is depicting Color shows whether that feature was high or low for that row of the dataset Horizontal location shows whether the effect of that value caused a higher or lower prediction. Some things you should be able to easily pick out:\nThe model ignored the Red and Yellow \u0026 Red features. Usually Yellow Card doesn’t affect the prediction, but there is an extreme case where a high value caused a much lower prediction. High values of Goal scored caused higher predictions, and low values caused low predictions How to Do That in Code Get the data and model ready:\nimport numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv') y = (data['Man of the Match'] == \"Yes\") # Convert from string \"Yes\"/\"No\" to binary feature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]] X = data[feature_names] train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) Let’s get a SHAP summary plot:\nimport shap # package used to calculate Shap values # Create object that can calculate shap values explainer = shap.TreeExplainer(my_model) # calculate shap values. This is what we will plot. # Calculate shap_values for all of val_X rather than a single row, to have more data for plot. shap_values = explainer.shap_values(val_X) # Make plot. Index of [1] is explained in text below. shap.summary_plot(shap_values[1], val_X) The code isn’t too complex. But there are a few caveats.\nWhen plotting, we call shap_values[1]. For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, we index in to get the SHAP values for the prediction of “True”. Calculating SHAP values can be slow. It isn’t a problem here, because this dataset is small. But you’ll want to be careful when running these to plot with reasonably sized datasets. The exception is when using an xgboost model, which SHAP has some optimizations for and which is thus much faster. SHAP Dependence Contributions Plots For SHAP dependence contribution plots provide a similar insight to partial dependence plot’s, but they add a lot more detail.\nEach dot represents a row of the data. The horizontal location is the actual value from the dataset, and the vertical location shows what having that value did to the prediction. The fact this slopes upward says that the more you possess the ball, the higher the model’s prediction is for winning the Man of the Match award.\nThe spread suggests that other features must interact with Ball Possession %. For example, here we have highlighted two points with similar ball possession values. That value caused one prediction to increase, and it caused the other prediction to decrease.\nFor comparison, a simple linear regression would produce plots that are perfect lines, without this spread.\nThis suggests we delve into the interactions, and the plots include color coding to help do that. While the primary trend is upward, you can visually inspect whether that varies by dot color.\nThese two points stand out spatially as being far away from the upward trend. They are both colored purple, indicating the team scored one goal. You can interpret this to say In general, having the ball increases a team’s chance of having their player win the award. But if they only score one goal, that trend reverses and the award judges may penalize them for having the ball so much if they score that little.\nHow to Do That in Code import shap # package used to calculate Shap values # Create object that can calculate shap values explainer = shap.TreeExplainer(my_model) # calculate shap values. This is what we will plot. shap_values = explainer.shap_values(X) # make plot. shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\") If you don’t supply an argument for interaction_index, Shapley uses some logic to pick one that may be interesting!\n",
  "wordCount" : "2627",
  "inLanguage": "en",
  "image": "https://keanteng.github.io/home/papermod-cover.png","datePublished": "2023-08-26T00:00:00Z",
  "dateModified": "2023-08-26T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Kean Teng Blog"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://keanteng.github.io/home/docs/2023/2023-08-26-ml-explainability/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Kean Teng Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://i.postimg.cc/0Qq5g2fX/favicon.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://keanteng.github.io/home/" accesskey="h" title="Kean Teng Blog (Alt + H)">Kean Teng Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://keanteng.github.io/home/ja/" title="日本語"
                            aria-label="日本語">日本語</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://keanteng.github.io/home/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://keanteng.github.io/home/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://keanteng.github.io/home/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://keanteng.github.io/home/faqs/" title="FAQs">
                    <span>FAQs</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://keanteng.github.io/home/">Home</a>&nbsp;»&nbsp;<a href="https://keanteng.github.io/home/docs/">Docs</a></div>
    <h1 class="post-title entry-hint-parent">
      Machine Learning Explainability
    </h1>
    <div class="post-description">
      My Kaggle Learning Note 
    </div>
    <div class="post-meta"><span title='2023-08-26 00:00:00 +0000 UTC'>August 26, 2023</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;Kean Teng Blog&nbsp;|&nbsp;<a href="https://github.com/keanteng/home/tree/main/content/docs/2023/2023-08-26-ml-explainability/index.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#1-permutation-importance" aria-label="1. Permutation Importance">1. Permutation Importance</a><ul>
                            
                    <li>
                        <a href="#11-example" aria-label="1.1 Example">1.1 Example</a></li></ul>
                    </li>
                    <li>
                        <a href="#2-partial-plots" aria-label="2. Partial Plots">2. Partial Plots</a><ul>
                            
                    <li>
                        <a href="#21-example" aria-label="2.1 Example">2.1 Example</a></li>
                    <li>
                        <a href="#22-2d-partial-dependence-plots" aria-label="2.2 2D Partial Dependence Plots">2.2 2D Partial Dependence Plots</a></li></ul>
                    </li>
                    <li>
                        <a href="#3-shap-value" aria-label="3. SHAP Value">3. SHAP Value</a><ul>
                            
                    <li>
                        <a href="#how-to-do-that-in-code" aria-label="How to Do That in Code">How to Do That in Code</a></li></ul>
                    </li>
                    <li>
                        <a href="#4-advanced-uses-of-shap-value" aria-label="4. Advanced Uses of SHAP Value">4. Advanced Uses of SHAP Value</a><ul>
                            
                    <li>
                        <a href="#how-to-do-that-in-code-1" aria-label="How to Do That in Code">How to Do That in Code</a></li>
                    <li>
                        <a href="#shap-dependence-contributions-plots" aria-label="SHAP Dependence Contributions Plots">SHAP Dependence Contributions Plots</a></li>
                    <li>
                        <a href="#how-to-do-that-in-code-2" aria-label="How to Do That in Code">How to Do That in Code</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><center><img src="https://images.unsplash.com/photo-1495592822108-9e6261896da8?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1770&q=80"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i>Images from Unsplash</i></p>
<blockquote>
<p><em>Disclaimer: This article is my learning note from the courses I took from Kaggle.</em></p></blockquote>
<p>Many people tend to say that machine learning models are &ldquo;black boxes&rdquo; because they can make good predictions but cannot understand the logic behind those predictions.</p>
<p>In this course, we will learn on methods to extract insights from model:</p>
<ul>
<li>What features in the data the model think as the most important?</li>
<li>How each feature affect the prediction?</li>
</ul>
<p>So, why do we need model insights? Model insights can be useful in a couple of ways:</p>
<p><strong>Debugging</strong></p>
<p>Given the frequency and potentially disastrous consequences of bugs, debugging is one of the most valuable skills in data science. Understanding the patterns a model is finding will help you identify when those are at odds with your knowledge of the real world, and this is typically the first step in tracking down bugs.</p>
<p><strong>Informing Feature Engineering</strong></p>
<p>Feature engineering is usually the most effective way to improve model accuracy. Feature engineering usually involves repeatedly creating new features using transformations of your raw data or features you have previously created.</p>
<p>Sometimes you can go through this process using nothing but intuition about the underlying topic. But you&rsquo;ll need more direction when you have 100s of raw features or when you lack background knowledge about the topic you are working on.</p>
<p><strong>Directing Future Data Collection</strong></p>
<p>You have no control over datasets you download online. But many businesses and organizations using data science have opportunities to expand what types of data they collect. Collecting new types of data can be expensive or inconvenient, so they only want to do this if they know it will be worthwhile. Model-based insights give you a good understanding of the value of features you currently have, which will help you reason about what new values may be most helpful.</p>
<p><strong>Informing Human Decision-Making</strong></p>
<p>Some decisions are made automatically by models. Amazon doesn&rsquo;t have humans (or elves) scurry to decide what to show you whenever you go to their website. But many important decisions are made by humans. For these decisions, insights can be more valuable than predictions.</p>
<p><strong>Building Trust</strong></p>
<p>Many people won&rsquo;t assume they can trust your model for important decisions without verifying some basic facts. This is a smart precaution given the frequency of data errors. In practice, showing insights that fit their general understanding of the problem will help build trust, even among people with little deep knowledge of data science.</p>
<h2 id="1-permutation-importance">1. Permutation Importance<a hidden class="anchor" aria-hidden="true" href="#1-permutation-importance">#</a></h2>
<p>Permutation importance is calculated after a model has been fitted. Imagine that now, we want to predict a person&rsquo;s height when they become 20 years old using only data that is available at age 10.</p>
<center><img src="wjMAysV.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>Now, if we randomly shuffle a single column of the validation data but all the other columns remain in place, how would the accuracy of the prediction be affected?</p>
<center><img src="h17tMUU.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>Of course, such an approach would reduce the model accuracy, since the data no longer corresponds to what we can observe in the real world. Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions. In this case, shuffling height at age 10 would cause terrible predictions. If we shuffled socks owned instead, the resulting predictions wouldn&rsquo;t suffer nearly as much.</p>
<p>So, here is what we can do:</p>
<ul>
<li>Get a trained model</li>
<li>Shuffle values in a single column and make prediction from it. Calculate the difference of prediction and target value with a loss function. The performance deterioration is the importance of the variable that we shuffled</li>
<li>Return to step 2 until all the importance for each column is calculated</li>
</ul>
<h3 id="11-example">1.1 Example<a hidden class="anchor" aria-hidden="true" href="#11-example">#</a></h3>
<p>Here is how to calculate the importance with <code>eli5</code> library:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">eli5</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">eli5.sklearn</span> <span class="kn">import</span> <span class="n">PermutationImportance</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">perm</span> <span class="o">=</span> <span class="n">PermutationImportance</span><span class="p">(</span><span class="n">my_model</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">val_X</span><span class="p">,</span> <span class="n">val_y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">eli5</span><span class="o">.</span><span class="n">show_weights</span><span class="p">(</span><span class="n">perm</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span> <span class="n">val_X</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</span></span></code></pre></div><center><img src="image.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>From the above figure, the values towards the top is the most important features. The first number in each row shows how much the model performance decreased with a random shuffling. There is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next.</p>
<p>You&rsquo;ll occasionally see negative values for permutation importance. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn&rsquo;t matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance.</p>
<h2 id="2-partial-plots">2. Partial Plots<a hidden class="anchor" aria-hidden="true" href="#2-partial-plots">#</a></h2>
<p>Similar to permutation importance, partial dependence plots are calculated after a model has been fit. To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.</p>
<p>We will use the fitted model to predict our outcome (probability their player won &ldquo;man of the match&rdquo;). But we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).</p>
<h3 id="21-example">2.1 Example<a hidden class="anchor" aria-hidden="true" href="#21-example">#</a></h3>
<p>Let&rsquo;s get a decision tree from the model:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">graphviz</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tree_graph</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">tree_model</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">graphviz</span><span class="o">.</span><span class="n">Source</span><span class="p">(</span><span class="n">tree_graph</span><span class="p">)</span>
</span></span></code></pre></div><center><img src="image-1.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>Produce a partial dependence plot:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">PartialDependenceDisplay</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create and plot the data</span>
</span></span><span class="line"><span class="cl"><span class="n">disp1</span> <span class="o">=</span> <span class="n">PartialDependenceDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">tree_model</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;Goal Scored&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><center><img src="__results___6_0.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>The y-axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.</p>
<p>From this particular graph, we see that scoring a goal substantially increases your chances of winning &ldquo;Man of The Match.&rdquo; But extra goals beyond that appear to have little impact on predictions.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="n">feature_to_plot</span> <span class="o">=</span> <span class="s1">&#39;Distance Covered (Kms)&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">disp2</span> <span class="o">=</span> <span class="n">PartialDependenceDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">tree_model</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="p">[</span><span class="n">feature_to_plot</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><center><img src="__results___8_0.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>This graph seems too simple to represent reality. But that&rsquo;s because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model&rsquo;s structure.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="c1"># Build Random Forest model</span>
</span></span><span class="line"><span class="cl"><span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">disp3</span> <span class="o">=</span> <span class="n">PartialDependenceDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">rf_model</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="p">[</span><span class="n">feature_to_plot</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><center><img src="__results___10_0.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<blockquote>
<p>This model thinks you are more likely to win Man of the Match if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.</p></blockquote>
<p>In general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model. Though this dataset is small enough that we would be careful in how we interpret any model.</p>
<h3 id="22-2d-partial-dependence-plots">2.2 2D Partial Dependence Plots<a hidden class="anchor" aria-hidden="true" href="#22-2d-partial-dependence-plots">#</a></h3>
<p>We will use the same datasets as of above:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">f_names</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;Goal Scored&#39;</span><span class="p">,</span> <span class="s1">&#39;Distance Covered (Kms)&#39;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Similar to previous PDP plot except we use tuple of features instead of single feature</span>
</span></span><span class="line"><span class="cl"><span class="n">disp4</span> <span class="o">=</span> <span class="n">PartialDependenceDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">tree_model</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="n">f_names</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><center><img src="__results___12_0.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>From the plot above, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn&rsquo;t matter. Can you see this by tracing through the decision tree with 0 goals?</p>
<p>But distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot.</p>
<h2 id="3-shap-value">3. SHAP Value<a hidden class="anchor" aria-hidden="true" href="#3-shap-value">#</a></h2>
<p>SHAP or <code>SHapley Additive exPlanations</code> is used to break down a prediction t0 show the impact of each feature. It interprets the impact of having a certain value for a given feature in comparison to the prediction we would make if that feature took some baseline value.</p>
<p>For example, consider the Man of the Match award example for previous section, we could ask questions like how much prediction driven by the fact that the team scored 3 goals?</p>
<p>But for each team, they are many features, so if we answer for the <code>number of goals</code>, we could repeat the process for other features too. SHAP values of all features sum up to explain why my prediction was different from the baseline.</p>
<pre tabindex="0"><code>sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values
</code></pre><center><img src="JVD2U7k.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>To interpret the graph:</p>
<p>We predicted 0.7, whereas the base_value is 0.4979. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature&rsquo;s effect. Feature values decreasing the prediction are in blue. The biggest impact comes from Goal Scored being 2. Though the ball possession value has a meaningful effect decreasing the prediction.</p>
<p>If you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.</p>
<h3 id="how-to-do-that-in-code">How to Do That in Code<a hidden class="anchor" aria-hidden="true" href="#how-to-do-that-in-code">#</a></h3>
<p>Let&rsquo;s get the model ready:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Man of the Match&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&#34;Yes&#34;</span><span class="p">)</span>  <span class="c1"># Convert from string &#34;Yes&#34;/&#34;No&#34; to binary</span>
</span></span><span class="line"><span class="cl"><span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">train_X</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">my_model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
</span></span></code></pre></div><p>We will look for SHAP value for a single row of the dataset. Let&rsquo;s check on the raw prediction first:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="n">row_to_show</span> <span class="o">=</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl"><span class="n">data_for_prediction</span> <span class="o">=</span> <span class="n">val_X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">row_to_show</span><span class="p">]</span>  <span class="c1"># use 1 row of data here. Could use multiple rows if desired</span>
</span></span><span class="line"><span class="cl"><span class="n">data_for_prediction_array</span> <span class="o">=</span> <span class="n">data_for_prediction</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">my_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">data_for_prediction_array</span><span class="p">)</span>
</span></span></code></pre></div><p>The output is <code>array([[0.29, 0.71]])</code>, the team is 70% likely to have a player win the award</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">shap</span>  <span class="c1"># package used to calculate Shap values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create object that can calculate shap values</span>
</span></span><span class="line"><span class="cl"><span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">my_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate Shap values</span>
</span></span><span class="line"><span class="cl"><span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">data_for_prediction</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">shap</span><span class="o">.</span><span class="n">force_plot</span><span class="p">(</span><span class="n">explainer</span><span class="o">.</span><span class="n">expected_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shap_values</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">data_for_prediction</span><span class="p">)</span>
</span></span></code></pre></div><p>The <code>shap_values</code> object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don&rsquo;t win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we&rsquo;ll pull out SHAP values for positive outcomes (pulling out <code>shap_values[1]</code>).</p>
<center><img src="image-2.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>Of course, SHAP package also has explainers for every type of model:</p>
<ul>
<li><code>shap.DeepExplainer</code> works with Deep Learning models.</li>
<li><code>shap.KernelExplainer</code> works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.</li>
</ul>
<h2 id="4-advanced-uses-of-shap-value">4. Advanced Uses of SHAP Value<a hidden class="anchor" aria-hidden="true" href="#4-advanced-uses-of-shap-value">#</a></h2>
<p>Shap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature).</p>
<p>Consider the equation:</p>
<pre tabindex="0"><code>y = 4 * x1 + 2 * x2
</code></pre><p>If x1 takes the value 2, instead of a baseline value of 0, then our SHAP value for  x1 would be 8 (from 4 times 2).</p>
<p>These are harder to calculate with the sophisticated models we use in practice. But through some algorithmic cleverness, Shap values allow us to decompose any prediction into the sum of effects of each feature value, yielding a graph like this:</p>
<center><img src="JVD2U7K-1.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>In addition to this nice breakdown for each prediction, the Shap library offers great visualizations of groups of Shap values. We will focus on two of these visualizations. These visualizations have conceptual similarities to permutation importance and partial dependence plots</p>
<p>SHAP summary plots give us a birds-eye view of feature importance and what is driving it. We&rsquo;ll walk through an example plot for the soccer data:</p>
<center><img src="Ew9X3su.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>This plot is made of many dots. Each dot has three characteristics:</p>
<ul>
<li>Vertical location shows what feature it is depicting</li>
<li>Color shows whether that feature was high or low for that row of the dataset</li>
<li>Horizontal location shows whether the effect of that value caused a higher or lower prediction.</li>
</ul>
<p>Some things you should be able to easily pick out:</p>
<ul>
<li>The model ignored the Red and Yellow &amp; Red features.</li>
<li>Usually Yellow Card doesn&rsquo;t affect the prediction, but there is an extreme case where a high value caused a much lower prediction.</li>
<li>High values of Goal scored caused higher predictions, and low values caused low predictions</li>
</ul>
<h3 id="how-to-do-that-in-code-1">How to Do That in Code<a hidden class="anchor" aria-hidden="true" href="#how-to-do-that-in-code-1">#</a></h3>
<p>Get the data and model ready:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Man of the Match&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&#34;Yes&#34;</span><span class="p">)</span>  <span class="c1"># Convert from string &#34;Yes&#34;/&#34;No&#34; to binary</span>
</span></span><span class="line"><span class="cl"><span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">train_X</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">my_model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
</span></span></code></pre></div><p>Let&rsquo;s get a SHAP summary plot:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">shap</span>  <span class="c1"># package used to calculate Shap values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create object that can calculate shap values</span>
</span></span><span class="line"><span class="cl"><span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">my_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># calculate shap values. This is what we will plot.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate shap_values for all of val_X rather than a single row, to have more data for plot.</span>
</span></span><span class="line"><span class="cl"><span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">val_X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Make plot. Index of [1] is explained in text below.</span>
</span></span><span class="line"><span class="cl"><span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">val_X</span><span class="p">)</span>
</span></span></code></pre></div><center><img src="__results___4_0.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>The code isn&rsquo;t too complex. But there are a few caveats.</p>
<ul>
<li>When plotting, we call <code>shap_values[1]</code>. For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, we index in to get the SHAP values for the prediction of &ldquo;True&rdquo;.</li>
<li>Calculating SHAP values can be slow. It isn&rsquo;t a problem here, because this dataset is small. But you&rsquo;ll want to be careful when running these to plot with reasonably sized datasets. The exception is when using an xgboost model, which SHAP has some optimizations for and which is thus much faster.</li>
</ul>
<h3 id="shap-dependence-contributions-plots">SHAP Dependence Contributions Plots<a hidden class="anchor" aria-hidden="true" href="#shap-dependence-contributions-plots">#</a></h3>
<p>For SHAP dependence contribution plots provide a similar insight to partial dependence plot&rsquo;s, but they add a lot more detail.</p>
<center><img src="uQ2JmBm.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>Each dot represents a row of the data. The horizontal location is the actual value from the dataset, and the vertical location shows what having that value did to the prediction. The fact this slopes upward says that the more you possess the ball, the higher the model&rsquo;s prediction is for winning the Man of the Match award.</p>
<p>The spread suggests that other features must interact with Ball Possession %. For example, here we have highlighted two points with similar ball possession values. That value caused one prediction to increase, and it caused the other prediction to decrease.</p>
<center><img src="tFzp6jc.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>For comparison, a simple linear regression would produce plots that are perfect lines, without this spread.</p>
<p>This suggests we delve into the interactions, and the plots include color coding to help do that. While the primary trend is upward, you can visually inspect whether that varies by dot color.</p>
<center><img src="NVB3eNW.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>These two points stand out spatially as being far away from the upward trend. They are both colored purple, indicating the team scored one goal. You can interpret this to say In general, having the ball increases a team&rsquo;s chance of having their player win the award. But if they only score one goal, that trend reverses and the award judges may penalize them for having the ball so much if they score that little.</p>
<h3 id="how-to-do-that-in-code-2">How to Do That in Code<a hidden class="anchor" aria-hidden="true" href="#how-to-do-that-in-code-2">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">shap</span>  <span class="c1"># package used to calculate Shap values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create object that can calculate shap values</span>
</span></span><span class="line"><span class="cl"><span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">my_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># calculate shap values. This is what we will plot.</span>
</span></span><span class="line"><span class="cl"><span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># make plot.</span>
</span></span><span class="line"><span class="cl"><span class="n">shap</span><span class="o">.</span><span class="n">dependence_plot</span><span class="p">(</span><span class="s1">&#39;Ball Possession %&#39;</span><span class="p">,</span> <span class="n">shap_values</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">interaction_index</span><span class="o">=</span><span class="s2">&#34;Goal Scored&#34;</span><span class="p">)</span>
</span></span></code></pre></div><center><img src="__results___6_0-1.png"  class = "center"/></center>
<p style="text-align: center; color:grey;"><i></i></p>
<p>If you don&rsquo;t supply an argument for interaction_index, Shapley uses some logic to pick one that may be interesting!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://keanteng.github.io/home/tags/python/">Python</a></li>
      <li><a href="https://keanteng.github.io/home/tags/machine-learning/">“Machine Learning&#34;</a></li>
      <li><a href="https://keanteng.github.io/home/tags/shap/">SHAP</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://keanteng.github.io/home/docs/2023/2023-08-26-land-cover-elevation-and-slope/">
    <span class="title">« Prev</span>
    <br>
    <span>Land Cover, Elevation &amp; Slope</span>
  </a>
  <a class="next" href="https://keanteng.github.io/home/docs/2023/2023-08-20-feature-engineering/">
    <span class="title">Next »</span>
    <br>
    <span>Feature Engineering</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Explainability on x"
            href="https://x.com/intent/tweet/?text=Machine%20Learning%20Explainability&amp;url=https%3a%2f%2fkeanteng.github.io%2fhome%2fdocs%2f2023%2f2023-08-26-ml-explainability%2f&amp;hashtags=Python%2c%e2%80%9cMachineLearning%22%2cSHAP">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Explainability on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fkeanteng.github.io%2fhome%2fdocs%2f2023%2f2023-08-26-ml-explainability%2f&amp;title=Machine%20Learning%20Explainability&amp;summary=Machine%20Learning%20Explainability&amp;source=https%3a%2f%2fkeanteng.github.io%2fhome%2fdocs%2f2023%2f2023-08-26-ml-explainability%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Explainability on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fkeanteng.github.io%2fhome%2fdocs%2f2023%2f2023-08-26-ml-explainability%2f&title=Machine%20Learning%20Explainability">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Explainability on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkeanteng.github.io%2fhome%2fdocs%2f2023%2f2023-08-26-ml-explainability%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Explainability on whatsapp"
            href="https://api.whatsapp.com/send?text=Machine%20Learning%20Explainability%20-%20https%3a%2f%2fkeanteng.github.io%2fhome%2fdocs%2f2023%2f2023-08-26-ml-explainability%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Explainability on telegram"
            href="https://telegram.me/share/url?text=Machine%20Learning%20Explainability&amp;url=https%3a%2f%2fkeanteng.github.io%2fhome%2fdocs%2f2023%2f2023-08-26-ml-explainability%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Explainability on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Machine%20Learning%20Explainability&u=https%3a%2f%2fkeanteng.github.io%2fhome%2fdocs%2f2023%2f2023-08-26-ml-explainability%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
        data-repo="keanteng/home"
        data-repo-id="R_kgDOJUcFvg"
        data-category="General"
        data-category-id="DIC_kwDOJUcFvs4CeIVI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://keanteng.github.io/home/">Kean Teng Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
